--- 
title: "MAT8800 - Bayesian Regression Project" 
author: "Duane Stanton" 
output: 
  html_document:
    highlight: haddock 
    theme: lumen 
    toc: TRUE 
    toc_depth: 3 
    toc_float: 
      collapsed: TRUE 
      smooth_scroll: FALSE 
date: "`r format(Sys.time(), '%B %d, %Y')`" 
--- 

<style type="text/css"> body, td {font-size: 18px;} code.r{font-size: 16px;} pre
{font-size: 16px} .list-group-item.active, .list-group-item.active:focus,
.list-group-item.active:hover { background-color: #00205B; color: #13B5EA}
</style>

```{r setup, include=F} 
knitr::opts_chunk$set(echo = TRUE) 
```
```{r, echo = F} 
# Plot setup 
theme_ds1 <- function(base_size = 13.5){
  theme_bw(base_size = base_size) %+replace% 
    theme( #line = element_line(color ="#000000"), 
           #text = element_text(color = "#000000"), 
           axis.title = element_text(color = "#000000", face = "plain"), 
           axis.text = element_text(color="#000000", size = 12), 
           axis.line = element_line(size = 0.5, linetype = 1, color = "#505050"), 
           #axis.line.y = element_line(size = 0.5, linetype = 1, color = "#505050"), 
           #legend.key = element_rect(color="#D2E2F9", fill = "#FFFFFF", linetype = 0), 
           legend.title = element_text(size = 11.5, face ="plain"), 
           legend.title.align = 0.5, 
           legend.text = element_text(size = 12),
           legend.background = element_rect(size = 0.1, color = "#EAEAEA", fill = "#FFFFFF", linetype = 1),
           panel.grid = element_line(color = "#505050", linetype = 0, size = 0.5), 
           #panel.grid.minor.x = element_line(size = 0.35),
           #panel.grid.minor.y = element_line(size = 0.35), 
           panel.border = element_rect(color = "#EAEAEA", fill = NA), 
           panel.background = element_rect(fill = "#FFFFFF"), 
           #plot.title = element_text(color = "#000000"), 
           plot.background = element_rect(color = NA, fill = "#FFFFFF"), 
           #plot.title = element_text(hjust = 0, face = "plain"), 
           plot.subtitle = element_text(hjust = 0, vjust = 1, size = 12, face = "plain"),
           plot.caption=element_text(size=11, hjust=1, face="italic", color="black") 
           #strip.text = element_text(size = 12), 
           #strip.background = element_rect(fill = NA), 
          ) 
        }

color_set8 <- c("#00205B", "#13B5EA", "#079500", "#9ECC14", "#CDB87D",
"#EEB31B", "#E27400", "#894600") 
```

```{r, echo = F} 
# Setting default reporting to 4 significant digits 
options(digits = 4)
# widening code chunk limits
options(width = 90)
```

# Introduction: Project, Resources, and Data

**Project**

The goal of this project is to apply the concepts of Bayesian regression as explored in my independent study on Bayesian regression in R and Stan, in coordination with Dr. Frey, during the Fall 2018 semester.

Given that the Fall term included consequential midterm Congressional elections (where party control of the House of Representatives and possibly the Senate were considered to possibly shift, and which did occur for the House), I chose to analyze House election data for the three most recent House elections - those for 2012, 2014, and 2016. 

I determined House elections were a good candidate for such a regression analysis for several reasons:

* The aforementioned overall topicality given we are in an "election year".

* With 435 Congressional districts per election, there is both a greater quantity of data and more granular data when considering the district level than comparative analysis for Senate elections (where senators serves for staggered 6-year terms such that 1/3 of Senate seats are "up for election" each election cycle - versus whole-House elections for 2-year terms for Representatives - and Senate elections occur at the state level, so there are fewer than 50 observations per election). More data and more granular data allows observed data to "have a greater say" in posterior distribution estimates relative to given prior estimates when fitting the models.

* Because Congressional districts are required to be approximately equal in relative population size based on the most recently completed decennial Census, with a minimum of one Representative per state[^1], Congressional districts are by design less prone to having disproportionately large or small populations relative to other districts.

[^1]: https://www.law.cornell.edu/uscode/text/2/2a

* Related to the previous point, the 2012, 2014, and 2016 elections each concerned districts based on population estimated from the 2010 Census, lending stability to district allocations.

**Resources**

This project was made possible thanks to several resources:

* Richard McElreath's *Statistical Rethinking*[^2], the core text used for the independent study. It's a great introduction to the overall mechanisms and basic theory of Bayesian regression, with most applications in base R and using the author's accompanying `rethinking` R package.

[^2]: McElreath, Richard. *Statistical Rethinking: a Bayesian Course with Examples in R and Stan*. Chapman and Hall/CRC, 2016.

* A Solomon Kurz's *Statistical Rethinking with brms, ggplot2, and the tidyverse*[^3], a bookdown.org project working through the text using "tidyverse" rather than base R methodology, and the `brms` package rather than `rethinking`. This has been an excellent source of coding examples for working through McElreath's text, with accompanying parallel insights and commentary.

[^3]: https://bookdown.org/connect/#/apps/1850/access

* A. Vehtari, A. Gelman, and J. Gabry's article *Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC*[^4], which provided insight into improving Bayesian model evaluation for out-of-sample prediction accuracy.

[^4]: https://arxiv.org/pdf/1507.04544.pdf

* R tools including the R statistical programming language (v3.5.1 for the duration of my work), the R Studio integrated development environment (I used v1.1.456 for all my work, including creating this document), McElreath's `rethinking` (latest version 1.80), Paul Bürkner's `brms` package (*Bayesian Regression Models using Stan*, latest version 2.60), Hadley Wickham et. al.'s "tidyverse" suite of packages including `ggplot2` (v3.1.0) and `dplyr` (v0.7.8), and the `rstan` package (v2.17.4), which interfaces with the Stan statistical modeling and computation platform which makes all the higher-level analysis possible.

* Additional R packages employed include `cowplot` (v0.9.3) for side-by-side plots, `bayesplot` (v1.6.0) and `tidybayes` (v1.0.3) for Bayesian model diagnostics, and `knitr` (v1.2.0) to create this report as well as working with `kableExtra` (v0.9.0) to create summary tables in this report. I wholeheartedly apologize for any omissions of other packages used throughout this report, which should be displayed in `library(packageName)` commands throughout.

* Last and certainly not least, Dr. Jesse Frey, who oversaw the independent study and provided greater insight into Bayesian methodology as well as advice on potential ways to strengthen this report and the overall independent study.

*Please note: Any mistakes in this report are entirely my own!*

**Data**

Data for the analysis comes from three sources:

1) *District-level election data* comes from the Federal Election Commission's (FEC's) Election Results page[^5], for each year considered (2012, 2014, 2016). The data was read in from CSV files and processed to extract variables such as the per-district election winner and the maximum and total number of primary and general election candidates and votes by party (Democratic, Republican, and Other).

[^5]: https://transition.fec.gov/pubrec/electionresults.shtml

2) *Campaign-reported financial data* comes from the FEC's bulk downloads page of candidate financial summary data[^6] for each year considered - specifically the file `candidate_summary_[YYYY].csv`. This data was matched to each campaign using the candidates' FEC-assigned candidate ID, which was present in both the election and financial data sets. 

[^6]: {https://cg-519a459a-0ea3-42c2-b7bc-fa1143481f74.s3-us-gov-west-1.amazonaws.com/bulk-downloads/index.html ; redirected from https://www.fec.gov/files/bulk-downloads/index.html }

Specifically, the variables for `Total_Receipt` (total campaign-reported money received, in USD) and `Total_Disbursement` (total campaign-reported money spent, also in USD) were used - this does not account for third-party spending (e.g. PACs and Super PACs), but provides some measure of reasonably well-defined financial data.

The top 10 entries based on `Total_Disbursement` for each year, and all entries with missing reported receipts and/or spending were investigated and confirmed against the FEC site and against the Center for Responsive Politics' site[^7], which also monitors and records campaign-reported finances. This step turned up several cases of significant campaign activity (receipts/spending in excess of $100,000) as well as many smaller-level entries for each year that was not in the FEC bulk data.

[^7]: http://www.opensecrets.org/

3) *District-level demographics data* comes from U.S. Census Bureau forms `B02001` for ethnicity composition (counts of population identifying as *White alone*, *Black or African American alone*, *American Indian and Alaska Native alone*, *Asian alone*, *Native Hawaiian and Other Pacific Islander alone*, *Some other race alone*, or *Two or more races*), and `S0201` for all other demographics including male/female proportions, education level among the age 25+ population, and age-group proportions (among others). Data comes from American Community Survey (ACS) 1-year estimates for the year of each election considered, and were collected from the Census Bureau's American FactFinder tool[^8].

[^8]: https://factfinder.census.gov/faces/nav/jsf/pages/searchresults.xhtml?refresh=t

*Program Setup*

```{r, echo = F, message = F} 
# Set rstan options for optimal Stan functionality
library(rstan)
rstan_options(auto_write = T) 
options(mc.cores = parallel::detectCores() - 1)
```

```{r, message = F} 
# loading packages used
library(tidyverse)
library(knitr)
library(kableExtra)
library(brms)
library(bayesplot)
library(RColorBrewer)

# load 2012, 2014, 2016 Congressional District House elections and demographic data
houseElectionDat <- 
  read.csv("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/houseElectionDat.csv")
```  

# Considering the outcome variable - which party's candidate was elected

While I also considered analyzing the vote margin between parties (e.g. # of votes for the Democratic candidate vs. # of votes for the Republican candidate, for each district), I settled on analyzing the outcome of a logistic regression model predicting whether or not a Democratic party candidate was elected for each district. This is primarily because a small set of districts had "uncontested" elections where either the Republican or Democratic candidate ran unopposed in the general election (and in a subset of these cases, was even unopposed in the primary).

At some future time, it may be interesting to consider a zero-inflated regression for the vote-margin model, however.

*Winning party for each Congressional district*

```{r, fig.align = "center", fig.width = 7, warning = F, echo = F}
houseElectionDat %>%
  group_by(year) %>%
  summarize(generalWinnerD   = sum(generalWinD),
            generalWinnerR   = sum(generalWinR),
            generalWinnerOth = sum(generalWinOth))

annoText <- 
  tibble(year   = c(2012, 2012, 2014, 2014, 2016, 2016),
         result = factor(rep(c("D", "R"), 3),
                         levels = levels(houseElectionDat$generalWinner)),
         labHt  = c(201, 234, 188, 247, 194, 241),
         labTxt = c(sum(houseElectionDat$year == 2012 & houseElectionDat$generalWinner == "D"),
                    sum(houseElectionDat$year == 2012 & houseElectionDat$generalWinner == "R"),
                    sum(houseElectionDat$year == 2014 & houseElectionDat$generalWinner == "D"),
                    sum(houseElectionDat$year == 2014 & houseElectionDat$generalWinner == "R"),
                    sum(houseElectionDat$year == 2016 & houseElectionDat$generalWinner == "D"),
                    sum(houseElectionDat$year == 2016 & houseElectionDat$generalWinner == "R")) )

houseElectionDat %>%
  ggplot() +
  geom_bar(aes(x = generalWinner, fill = generalWinner)) +
  geom_text(data = annoText, aes(x = result, y = labHt*1.05, label = labTxt)) +
  labs(title = "Winning party counts in general election by year",
       x = "Party of winning candidate",
       y = "Count") +
  facet_wrap(factor(year) ~ .) +
  scale_fill_manual(values = c(brewer.pal(3,"Blues")[3], brewer.pal(3,"Reds")[3]),
                    guide = F) +
  theme_ds1()
```

As the above plot show, Republican candidates won a majority of House seats in each election, with the greatest margin in 2014 (+59 seats). No third-party candidates won election in any race for the three years considered.

Because the Democratic party is the "challenging / minority" party for the 2018 midterm elections, I chose `generalWinD` (general election won by a Democratic candidate) as the outcome variable. Since no third party candidate was elected in the three years considered, the complement of this can be considered "general election won by a Republican candidate" for this analysis.

# Considering predictor variables

## Summary table of pre-standardization values for numeric predictor variables

The regression models below all use standardized continuous variables (with the exception of Boolean predictors, such as whether or not the [party] candidate is the incumbent, and small-count predictors, such as the number of primary candidates for [party]). This allows us to interpret posterior estimates for coefficients in terms of relative predictive association with a Democratic candidate being elected for relative changes in magnitude rather than in natural-scale units.

However, it is important to understand the natural scale of each variable, to put these magnitude changes in proper context.

```{r, echo = F}
predictorsDat12 <- 
  houseElectionDat %>%
  filter(year == 2012) %>%
  select(-state, -district, -district2, -year, 
         -generalTotD, -generalMaxD, 
         -generalTotR, -generalMaxR,
         -generalTotOth, -generalMaxOth,
         -generalWinner,
         -areaSqMi, -contains("_SE") ) %>%
  t()

predDat12Means <- 
  apply(predictorsDat12, 1, mean)

predDat12SDs <- 
  apply(predictorsDat12, 1, sd)

predictorsDat14 <- 
  houseElectionDat %>%
  filter(year == 2014) %>%
  select(-state, -district, -district2, -year, 
         -generalTotD, -generalMaxD, 
         -generalTotR, -generalMaxR,
         -generalTotOth, -generalMaxOth,
         -generalWinner,
         -areaSqMi, -contains("_SE") ) %>%
  t()

predDat14Means <- 
  apply(predictorsDat14, 1, mean)

predDat14SDs <- 
  apply(predictorsDat14, 1, sd)

predictorsDat16 <- 
  houseElectionDat %>%
  filter(year == 2016) %>%
  select(-state, -district, -district2, -year, 
         -generalTotD, -generalMaxD, 
         -generalTotR, -generalMaxR,
         -generalTotOth, -generalMaxOth,
         -generalWinner,
         -areaSqMi, -contains("_SE") ) %>%
  t()

predDat16Means <- 
  apply(predictorsDat16, 1, mean)

predDat16SDs <- 
  apply(predictorsDat16, 1, sd)

varNames <- rownames(predictorsDat12)

predictorSummary <- 
  tibble(predictor = varNames,
         unitDesc  = c(rep("count", 3),
                       rep("proportion", 3),
                       rep("USD", 15), 
                       "count",
                       rep("vote count", 4),
                       "proportion",
                       "count",
                       rep("vote count", 4),
                       "proportion",
                       "vote count",
                       "count",
                       rep("vote count", 4),
                       "proportion",
                       "count",
                       "vote count",
                       rep("count", 3),
                       rep("proportion", 6),
                       "count",
                       "vote count",
                       "count",
                       "count per sq. mi.",
                       rep("percentage", 2),
                       "years",
                       rep("count", 3),
                       rep("percentage", 10),
                       rep("USD", 2),
                       rep("percentage", 6),
                       "log(count per sq. mi.)"),
         mean12    = predDat12Means,
         sd12      = predDat12SDs,
         mean14    = predDat14Means,
         sd14      = predDat14SDs,
         mean16    = predDat16Means,
         sd16      = predDat16SDs )

predictorSummary %>%
  kable(align = "c", booktabs = T, digits = 3, col.names = c("Predictor", "Unit", rep(c("Mean", "SD"), 3)) ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F) %>%
  add_header_above(c(" " = 2, "2012" = 2, "2014" = 2, "2016" = 2)) %>%
  column_spec(c(4,6), border_right = T) 
```

```{r, echo = F, eval = F}
# create a version of houseElectionDat with standardized numeric vars
houseElectionDat12 <- 
  houseElectionDat %>%
  filter(year == 2012)

houseElectionDat14 <- 
  houseElectionDat %>%
  filter(year == 2014)

houseElectionDat16 <- 
  houseElectionDat %>%
  filter(year == 2016)

standardize <- function(x){
  (x - mean(x)) / sd(x)
}

houseElectionDat12_std <- 
  houseElectionDat12 %>%
  mutate(state                       = state,
         district                    = district,
         year                        = year,
         incumbentD                  = incumbentD,
         incumbentR                  = incumbentR,
         incumbentOth                = incumbentOth,
         incumbentWinD               = incumbentWinD,
         incumbentWinR               = incumbentWinR,
         incumbentWinOth             = incumbentWinOth,
         cmpgnTotRecptD              = standardize(houseElectionDat12$cmpgnTotRecptD),
         cmpgnMaxRecptD              = standardize(houseElectionDat12$cmpgnMaxRecptD),
         cmpgnTotRecptR              = standardize(houseElectionDat12$cmpgnTotRecptR),
         cmpgnMaxRecptR              = standardize(houseElectionDat12$cmpgnMaxRecptR),
         cmpgnTotRecptOth            = standardize(houseElectionDat12$cmpgnTotRecptOth),
         cmpgnMaxRecptOth            = standardize(houseElectionDat12$cmpgnMaxRecptOth),
         cmpgnRecptTotal             = standardize(houseElectionDat12$cmpgnRecptTotal),
         cmpgnTotDisbursD            = standardize(houseElectionDat12$cmpgnTotDisbursD),
         cmpgnMaxDisbursD            = standardize(houseElectionDat12$cmpgnMaxDisbursD),
         cmpgnTotDisbursR            = standardize(houseElectionDat12$cmpgnTotDisbursR),
         cmpgnMaxDisbursR            = standardize(houseElectionDat12$cmpgnMaxDisbursR),
         cmpgnMaxDisbursDvsR         = standardize(houseElectionDat12$cmpgnMaxDisbursDvsR),
         cmpgnTotDisbursOth          = standardize(houseElectionDat12$cmpgnTotDisbursOth),
         cmpgnMaxDisbursOth          = standardize(houseElectionDat12$cmpgnMaxDisbursOth),
         cmpgnDisbursTotal           = standardize(houseElectionDat12$cmpgnDisbursTotal),
         primaryNumCandD             = standardize(houseElectionDat12$primaryNumCandD),
         primaryTotD                 = standardize(houseElectionDat12$primaryTotD),
         primaryMaxD                 = standardize(houseElectionDat12$primaryMaxD),
         primRunoffTotD              = standardize(houseElectionDat12$primRunoffTotD),
         primRunoffMaxD              = standardize(houseElectionDat12$primRunoffMaxD),
         primaryUnopD                = primaryUnopD,
         primaryNumCandR             = standardize(houseElectionDat12$primaryNumCandR),
         primaryTotR                 = standardize(houseElectionDat12$primaryTotR),
         primaryMaxR                 = standardize(houseElectionDat12$primaryMaxR),
         primRunoffTotR              = standardize(houseElectionDat12$primRunoffTotR),
         primRunoffMaxR              = standardize(houseElectionDat12$primRunoffMaxR),
         primaryUnopR                = primaryUnopR,
         primaryTotDvsR              = standardize(houseElectionDat12$primaryTotDvsR),
         primaryNumCandOth           = standardize(houseElectionDat12$primaryNumCandOth),
         primaryTotOth               = standardize(houseElectionDat12$primaryTotOth),
         primaryMaxOth               = standardize(houseElectionDat12$primaryMaxOth),
         primRunoffTotOth            = standardize(houseElectionDat12$primRunoffTotOth),
         primRunoffMaxOth            = standardize(houseElectionDat12$primRunoffMaxOth),
         primaryUnopOth              = primaryUnopOth,
         primaryNumCandTotal         = standardize(houseElectionDat12$primaryNumCandTotal),
         primaryTotal                = standardize(houseElectionDat12$primaryTotal),
         generalNumCandD             = standardize(houseElectionDat12$generalNumCandD),
         generalTotD                 = standardize(houseElectionDat12$generalTotD),
         generalNumCandR             = standardize(houseElectionDat12$generalNumCandR),
         generalTotR                 = standardize(houseElectionDat12$generalTotR),
         generalNumCandOth           = standardize(houseElectionDat12$generalNumCandOth),
         generalTotOth               = standardize(houseElectionDat12$generalTotOth),
         generalMaxD                 = standardize(houseElectionDat12$generalMaxD),
         generalUnopD                = generalUnopD,
         generalMaxR                 = standardize(houseElectionDat12$generalMaxR),
         generalUnopR                = generalUnopR,
         generalMaxOth               = standardize(houseElectionDat12$generalMaxOth),
         generalUnopOth              = generalUnopOth,
         generalWinD                 = generalWinD,
         generalWinR                 = generalWinR,
         generalWinOth               = generalWinOth,
         generalWinner               = generalWinner,
         generalNumCandTotal         = standardize(houseElectionDat12$generalNumCandTotal),
         generalTotal                = standardize(houseElectionDat12$generalTotal),
         areaSqMi                    = standardize(houseElectionDat12$areaSqMi),
         totalPop                    = standardize(houseElectionDat12$totalPop),
         # per discussion w/ Dr. Frey, _SE variables are standardized by applying 
         # same underlying transformation as the corresponding measurement var
         totalPop_SE                 = totalPop_SE,
         popDensityPerSqMi           = standardize(houseElectionDat12$popDensityPerSqMi),
         pctMale                     = standardize(houseElectionDat12$pctMale),
         pctMale_SE                  = pctMale_SE / sd(houseElectionDat12$pctMale),
         pctFemale                   = standardize(houseElectionDat12$pctFemale),
         pctFemale_SE                = pctFemale_SE / sd(houseElectionDat12$pctFemale),
         medianAgeYr                 = standardize(houseElectionDat12$medianAgeYr),
         medianAgeYr_SE              = medianAgeYr_SE / sd(houseElectionDat12$medianAgeYr),
         # need to strongly consider using calculated proportions of age groups rather than standarized counts
         age18_34Pop                 = standardize(houseElectionDat12$age18_34Pop),
         age18_34Pop_SE              = age18_34Pop_SE / sd(houseElectionDat12$age18_34Pop),
         age35_64Pop                 = standardize(houseElectionDat12$age35_64Pop),
         age35_64Pop_SE              = age35_64Pop_SE / sd(houseElectionDat12$age35_64Pop),
         age65plusPop                = standardize(houseElectionDat12$age65plusPop),
         age65plusPop_SE             = age65plusPop_SE / sd(houseElectionDat12$age65plusPop),
         pctEduHSorLessAge25plus     = standardize(houseElectionDat12$pctEduHSorLessAge25plus),
         pctEduHSorLessAge25plus_SE  = pctEduHSorLessAge25plus_SE / sd(houseElectionDat12$pctEduHSorLessAge25plus),
         pctEduSomColgAge25plus      = standardize(houseElectionDat12$pctEduSomColgAge25plus),
         pctEduSomColgAge25plus_SE   = pctEduSomColgAge25plus_SE / sd(houseElectionDat12$pctEduSomColgAge25plus),
         pctEduBachAge25plus         = standardize(houseElectionDat12$pctEduBachAge25plus),
         pctEduBachAge25plus_SE      = pctEduBachAge25plus_SE / sd(houseElectionDat12$pctEduBachAge25plus),
         pctEduGradProAge25plus      = standardize(houseElectionDat12$pctEduGradProAge25plus),
         pctEduGradProAge25plus_SE   = pctEduGradProAge25plus_SE / sd(houseElectionDat12$pctEduGradProAge25plus),
         pctEduBachPlusAge25plus     = standardize(houseElectionDat12$pctEduBachPlusAge25plus),
         pctEduBachPlusAge25plus_SE  = pctEduBachPlusAge25plus_SE / sd(houseElectionDat12$pctEduBachPlusAge25plus),
         pctUSBorn                   = standardize(houseElectionDat12$pctUSBorn),
         pctUSBorn_SE                = pctUSBorn_SE / sd(houseElectionDat12$pctUSBorn),
         pctForgnBornUSCtzn          = standardize(houseElectionDat12$pctForgnBornUSCtzn),
         pctForgnBornUSCtzn_SE       = pctForgnBornUSCtzn_SE / sd(houseElectionDat12$pctForgnBornUSCtzn),
         pctAge5PlusNonEnglshHome    = standardize(houseElectionDat12$pctAge5PlusNonEnglshHome),
         pctAge5PlusNonEnglshHome_SE = pctAge5PlusNonEnglshHome_SE / sd(houseElectionDat12$pctAge5PlusNonEnglshHome),
         pctInLaborForceUnemp        = standardize(houseElectionDat12$pctInLaborForceUnemp),
         pctInLaborForceUnemp_SE     = pctInLaborForceUnemp_SE / sd(houseElectionDat12$pctInLaborForceUnemp),
         pctNotInLaborForce          = standardize(houseElectionDat12$pctNotInLaborForce),
         pctNotInLaborForce_SE       = pctNotInLaborForce_SE / sd(houseElectionDat12$pctNotInLaborForce),
         medianHHIncome              = standardize(houseElectionDat12$medianHHIncome),
         medianHHIncome_SE           = medianHHIncome_SE / sd(houseElectionDat12$medianHHIncome),
         perCapitaIncome             = standardize(houseElectionDat12$perCapitaIncome),
         perCapitaIncome_SE          = perCapitaIncome_SE / sd(houseElectionDat12$perCapitaIncome),
         pctNoHealthInsCivNonInst    = standardize(houseElectionDat12$pctNoHealthInsCivNonInst),
         pctNoHealthInsCivNonInst_SE = pctNoHealthInsCivNonInst_SE / sd(houseElectionDat12$pctNoHealthInsCivNonInst),
         pctPovertyAge18_64          = standardize(houseElectionDat12$pctPovertyAge18_64),
         pctPovertyAge18_64_SE       = pctPovertyAge18_64_SE / sd(houseElectionDat12$pctPovertyAge18_64),
         pctWhiteAlone               = standardize(houseElectionDat12$pctWhiteAlone),
         pctWhiteAlone_SE            = pctWhiteAlone_SE / sd(houseElectionDat12$pctWhiteAlone),
         pctAfrAmAlone               = standardize(houseElectionDat12$pctAfrAmAlone),
         pctAfrAmAlone_SE            = pctAfrAmAlone_SE / sd(houseElectionDat12$pctAfrAmAlone),
         pctOtherAlone               = standardize(houseElectionDat12$pctOtherAlone),
         pctOtherAlone_SE            = pctOtherAlone_SE / sd(houseElectionDat12$pctOtherAlone),
         pctMultiRacial              = standardize(houseElectionDat12$pctMultiRacial),
         pctMultiRacial_SE           = pctMultiRacial_SE / sd(houseElectionDat12$pctMultiRacial),
         log_popDensPerSqMi          = standardize(houseElectionDat12$log_popDensPerSqMi) )

houseElectionDat14_std <- 
  houseElectionDat14 %>%
  mutate(state                       = state,
         district                    = district,
         year                        = year,
         incumbentD                  = incumbentD,
         incumbentR                  = incumbentR,
         incumbentOth                = incumbentOth,
         incumbentWinD               = incumbentWinD,
         incumbentWinR               = incumbentWinR,
         incumbentWinOth             = incumbentWinOth,
         cmpgnTotRecptD              = standardize(houseElectionDat14$cmpgnTotRecptD),
         cmpgnMaxRecptD              = standardize(houseElectionDat14$cmpgnMaxRecptD),
         cmpgnTotRecptR              = standardize(houseElectionDat14$cmpgnTotRecptR),
         cmpgnMaxRecptR              = standardize(houseElectionDat14$cmpgnMaxRecptR),
         cmpgnTotRecptOth            = standardize(houseElectionDat14$cmpgnTotRecptOth),
         cmpgnMaxRecptOth            = standardize(houseElectionDat14$cmpgnMaxRecptOth),
         cmpgnRecptTotal             = standardize(houseElectionDat14$cmpgnRecptTotal),
         cmpgnTotDisbursD            = standardize(houseElectionDat14$cmpgnTotDisbursD),
         cmpgnMaxDisbursD            = standardize(houseElectionDat14$cmpgnMaxDisbursD),
         cmpgnTotDisbursR            = standardize(houseElectionDat14$cmpgnTotDisbursR),
         cmpgnMaxDisbursR            = standardize(houseElectionDat14$cmpgnMaxDisbursR),
         cmpgnMaxDisbursDvsR         = standardize(houseElectionDat14$cmpgnMaxDisbursDvsR),
         cmpgnTotDisbursOth          = standardize(houseElectionDat14$cmpgnTotDisbursOth),
         cmpgnMaxDisbursOth          = standardize(houseElectionDat14$cmpgnMaxDisbursOth),
         cmpgnDisbursTotal           = standardize(houseElectionDat14$cmpgnDisbursTotal),
         primaryNumCandD             = standardize(houseElectionDat14$primaryNumCandD),
         primaryTotD                 = standardize(houseElectionDat14$primaryTotD),
         primaryMaxD                 = standardize(houseElectionDat14$primaryMaxD),
         primRunoffTotD              = standardize(houseElectionDat14$primRunoffTotD),
         primRunoffMaxD              = standardize(houseElectionDat14$primRunoffMaxD),
         primaryUnopD                = primaryUnopD,
         primaryNumCandR             = standardize(houseElectionDat14$primaryNumCandR),
         primaryTotR                 = standardize(houseElectionDat14$primaryTotR),
         primaryMaxR                 = standardize(houseElectionDat14$primaryMaxR),
         primRunoffTotR              = standardize(houseElectionDat14$primRunoffTotR),
         primRunoffMaxR              = standardize(houseElectionDat14$primRunoffMaxR),
         primaryUnopR                = primaryUnopR,
         primaryTotDvsR              = standardize(houseElectionDat14$primaryTotDvsR),
         primaryNumCandOth           = standardize(houseElectionDat14$primaryNumCandOth),
         primaryTotOth               = standardize(houseElectionDat14$primaryTotOth),
         primaryMaxOth               = standardize(houseElectionDat14$primaryMaxOth),
         primRunoffTotOth            = standardize(houseElectionDat14$primRunoffTotOth),
         primRunoffMaxOth            = standardize(houseElectionDat14$primRunoffMaxOth),
         primaryUnopOth              = primaryUnopOth,
         primaryNumCandTotal         = standardize(houseElectionDat14$primaryNumCandTotal),
         primaryTotal                = standardize(houseElectionDat14$primaryTotal),
         generalNumCandD             = standardize(houseElectionDat14$generalNumCandD),
         generalTotD                 = standardize(houseElectionDat14$generalTotD),
         generalNumCandR             = standardize(houseElectionDat14$generalNumCandR),
         generalTotR                 = standardize(houseElectionDat14$generalTotR),
         generalNumCandOth           = standardize(houseElectionDat14$generalNumCandOth),
         generalTotOth               = standardize(houseElectionDat14$generalTotOth),
         generalMaxD                 = standardize(houseElectionDat14$generalMaxD),
         generalUnopD                = generalUnopD,
         generalMaxR                 = standardize(houseElectionDat14$generalMaxR),
         generalUnopR                = generalUnopR,
         generalMaxOth               = standardize(houseElectionDat14$generalMaxOth),
         generalUnopOth              = generalUnopOth,
         generalWinD                 = generalWinD,
         generalWinR                 = generalWinR,
         generalWinOth               = generalWinOth,
         generalWinner               = generalWinner,
         generalNumCandTotal         = standardize(houseElectionDat14$generalNumCandTotal),
         generalTotal                = standardize(houseElectionDat14$generalTotal),
         areaSqMi                    = standardize(houseElectionDat14$areaSqMi),
         totalPop                    = standardize(houseElectionDat14$totalPop),
         # per discussion w/ Dr. Frey, _SE variables are standardized by applying 
         # same underlying transformation as the corresponding measurement var
         totalPop_SE                 = totalPop_SE,
         popDensityPerSqMi           = standardize(houseElectionDat14$popDensityPerSqMi),
         pctMale                     = standardize(houseElectionDat14$pctMale),
         pctMale_SE                  = pctMale_SE / sd(houseElectionDat14$pctMale),
         pctFemale                   = standardize(houseElectionDat14$pctFemale),
         pctFemale_SE                = pctFemale_SE / sd(houseElectionDat14$pctFemale),
         medianAgeYr                 = standardize(houseElectionDat14$medianAgeYr),
         medianAgeYr_SE              = medianAgeYr_SE / sd(houseElectionDat14$medianAgeYr),
         # need to strongly consider using calculated proportions of age groups rather than standarized counts
         age18_34Pop                 = standardize(houseElectionDat14$age18_34Pop),
         age18_34Pop_SE              = age18_34Pop_SE / sd(houseElectionDat14$age18_34Pop),
         age35_64Pop                 = standardize(houseElectionDat14$age35_64Pop),
         age35_64Pop_SE              = age35_64Pop_SE / sd(houseElectionDat14$age35_64Pop),
         age65plusPop                = standardize(houseElectionDat14$age65plusPop),
         age65plusPop_SE             = age65plusPop_SE / sd(houseElectionDat14$age65plusPop),
         pctEduHSorLessAge25plus     = standardize(houseElectionDat14$pctEduHSorLessAge25plus),
         pctEduHSorLessAge25plus_SE  = pctEduHSorLessAge25plus_SE / sd(houseElectionDat14$pctEduHSorLessAge25plus),
         pctEduSomColgAge25plus      = standardize(houseElectionDat14$pctEduSomColgAge25plus),
         pctEduSomColgAge25plus_SE   = pctEduSomColgAge25plus_SE / sd(houseElectionDat14$pctEduSomColgAge25plus),
         pctEduBachAge25plus         = standardize(houseElectionDat14$pctEduBachAge25plus),
         pctEduBachAge25plus_SE      = pctEduBachAge25plus_SE / sd(houseElectionDat14$pctEduBachAge25plus),
         pctEduGradProAge25plus      = standardize(houseElectionDat14$pctEduGradProAge25plus),
         pctEduGradProAge25plus_SE   = pctEduGradProAge25plus_SE / sd(houseElectionDat14$pctEduGradProAge25plus),
         pctEduBachPlusAge25plus     = standardize(houseElectionDat14$pctEduBachPlusAge25plus),
         pctEduBachPlusAge25plus_SE  = pctEduBachPlusAge25plus_SE / sd(houseElectionDat14$pctEduBachPlusAge25plus),
         pctUSBorn                   = standardize(houseElectionDat14$pctUSBorn),
         pctUSBorn_SE                = pctUSBorn_SE / sd(houseElectionDat14$pctUSBorn),
         pctForgnBornUSCtzn          = standardize(houseElectionDat14$pctForgnBornUSCtzn),
         pctForgnBornUSCtzn_SE       = pctForgnBornUSCtzn_SE / sd(houseElectionDat14$pctForgnBornUSCtzn),
         pctAge5PlusNonEnglshHome    = standardize(houseElectionDat14$pctAge5PlusNonEnglshHome),
         pctAge5PlusNonEnglshHome_SE = pctAge5PlusNonEnglshHome_SE / sd(houseElectionDat14$pctAge5PlusNonEnglshHome),
         pctInLaborForceUnemp        = standardize(houseElectionDat14$pctInLaborForceUnemp),
         pctInLaborForceUnemp_SE     = pctInLaborForceUnemp_SE / sd(houseElectionDat14$pctInLaborForceUnemp),
         pctNotInLaborForce          = standardize(houseElectionDat14$pctNotInLaborForce),
         pctNotInLaborForce_SE       = pctNotInLaborForce_SE / sd(houseElectionDat14$pctNotInLaborForce),
         medianHHIncome              = standardize(houseElectionDat14$medianHHIncome),
         medianHHIncome_SE           = medianHHIncome_SE / sd(houseElectionDat14$medianHHIncome),
         perCapitaIncome             = standardize(houseElectionDat14$perCapitaIncome),
         perCapitaIncome_SE          = perCapitaIncome_SE / sd(houseElectionDat14$perCapitaIncome),
         pctNoHealthInsCivNonInst    = standardize(houseElectionDat14$pctNoHealthInsCivNonInst),
         pctNoHealthInsCivNonInst_SE = pctNoHealthInsCivNonInst_SE / sd(houseElectionDat14$pctNoHealthInsCivNonInst),
         pctPovertyAge18_64          = standardize(houseElectionDat14$pctPovertyAge18_64),
         pctPovertyAge18_64_SE       = pctPovertyAge18_64_SE / sd(houseElectionDat14$pctPovertyAge18_64),
         pctWhiteAlone               = standardize(houseElectionDat14$pctWhiteAlone),
         pctWhiteAlone_SE            = pctWhiteAlone_SE / sd(houseElectionDat14$pctWhiteAlone),
         pctAfrAmAlone               = standardize(houseElectionDat14$pctAfrAmAlone),
         pctAfrAmAlone_SE            = pctAfrAmAlone_SE / sd(houseElectionDat14$pctAfrAmAlone),
         pctOtherAlone               = standardize(houseElectionDat14$pctOtherAlone),
         pctOtherAlone_SE            = pctOtherAlone_SE / sd(houseElectionDat14$pctOtherAlone),
         pctMultiRacial              = standardize(houseElectionDat14$pctMultiRacial),
         pctMultiRacial_SE           = pctMultiRacial_SE / sd(houseElectionDat14$pctMultiRacial),
         log_popDensPerSqMi          = standardize(houseElectionDat14$log_popDensPerSqMi) )

houseElectionDat16_std <- 
  houseElectionDat16 %>%
  mutate(state                       = state,
         district                    = district,
         year                        = year,
         incumbentD                  = incumbentD,
         incumbentR                  = incumbentR,
         incumbentOth                = incumbentOth,
         incumbentWinD               = incumbentWinD,
         incumbentWinR               = incumbentWinR,
         incumbentWinOth             = incumbentWinOth,
         cmpgnTotRecptD              = standardize(houseElectionDat16$cmpgnTotRecptD),
         cmpgnMaxRecptD              = standardize(houseElectionDat16$cmpgnMaxRecptD),
         cmpgnTotRecptR              = standardize(houseElectionDat16$cmpgnTotRecptR),
         cmpgnMaxRecptR              = standardize(houseElectionDat16$cmpgnMaxRecptR),
         cmpgnTotRecptOth            = standardize(houseElectionDat16$cmpgnTotRecptOth),
         cmpgnMaxRecptOth            = standardize(houseElectionDat16$cmpgnMaxRecptOth),
         cmpgnRecptTotal             = standardize(houseElectionDat16$cmpgnRecptTotal),
         cmpgnTotDisbursD            = standardize(houseElectionDat16$cmpgnTotDisbursD),
         cmpgnMaxDisbursD            = standardize(houseElectionDat16$cmpgnMaxDisbursD),
         cmpgnTotDisbursR            = standardize(houseElectionDat16$cmpgnTotDisbursR),
         cmpgnMaxDisbursR            = standardize(houseElectionDat16$cmpgnMaxDisbursR),
         cmpgnMaxDisbursDvsR         = standardize(houseElectionDat16$cmpgnMaxDisbursDvsR),
         cmpgnTotDisbursOth          = standardize(houseElectionDat16$cmpgnTotDisbursOth),
         cmpgnMaxDisbursOth          = standardize(houseElectionDat16$cmpgnMaxDisbursOth),
         cmpgnDisbursTotal           = standardize(houseElectionDat16$cmpgnDisbursTotal),
         primaryNumCandD             = standardize(houseElectionDat16$primaryNumCandD),
         primaryTotD                 = standardize(houseElectionDat16$primaryTotD),
         primaryMaxD                 = standardize(houseElectionDat16$primaryMaxD),
         primRunoffTotD              = standardize(houseElectionDat16$primRunoffTotD),
         primRunoffMaxD              = standardize(houseElectionDat16$primRunoffMaxD),
         primaryUnopD                = primaryUnopD,
         primaryNumCandR             = standardize(houseElectionDat16$primaryNumCandR),
         primaryTotR                 = standardize(houseElectionDat16$primaryTotR),
         primaryMaxR                 = standardize(houseElectionDat16$primaryMaxR),
         primRunoffTotR              = standardize(houseElectionDat16$primRunoffTotR),
         primRunoffMaxR              = standardize(houseElectionDat16$primRunoffMaxR),
         primaryUnopR                = primaryUnopR,
         primaryTotDvsR              = standardize(houseElectionDat16$primaryTotDvsR),
         primaryNumCandOth           = standardize(houseElectionDat16$primaryNumCandOth),
         primaryTotOth               = standardize(houseElectionDat16$primaryTotOth),
         primaryMaxOth               = standardize(houseElectionDat16$primaryMaxOth),
         primRunoffTotOth            = standardize(houseElectionDat16$primRunoffTotOth),
         primRunoffMaxOth            = standardize(houseElectionDat16$primRunoffMaxOth),
         primaryUnopOth              = primaryUnopOth,
         primaryNumCandTotal         = standardize(houseElectionDat16$primaryNumCandTotal),
         primaryTotal                = standardize(houseElectionDat16$primaryTotal),
         generalNumCandD             = standardize(houseElectionDat16$generalNumCandD),
         generalTotD                 = standardize(houseElectionDat16$generalTotD),
         generalNumCandR             = standardize(houseElectionDat16$generalNumCandR),
         generalTotR                 = standardize(houseElectionDat16$generalTotR),
         generalNumCandOth           = standardize(houseElectionDat16$generalNumCandOth),
         generalTotOth               = standardize(houseElectionDat16$generalTotOth),
         generalMaxD                 = standardize(houseElectionDat16$generalMaxD),
         generalUnopD                = generalUnopD,
         generalMaxR                 = standardize(houseElectionDat16$generalMaxR),
         generalUnopR                = generalUnopR,
         generalMaxOth               = standardize(houseElectionDat16$generalMaxOth),
         generalUnopOth              = generalUnopOth,
         generalWinD                 = generalWinD,
         generalWinR                 = generalWinR,
         generalWinOth               = generalWinOth,
         generalWinner               = generalWinner,
         generalNumCandTotal         = standardize(houseElectionDat16$generalNumCandTotal),
         generalTotal                = standardize(houseElectionDat16$generalTotal),
         areaSqMi                    = standardize(houseElectionDat16$areaSqMi),
         totalPop                    = standardize(houseElectionDat16$totalPop),
         # per discussion w/ Dr. Frey, _SE variables are standardized by applying 
         # same underlying transformation as the corresponding measurement var
         totalPop_SE                 = totalPop_SE,
         popDensityPerSqMi           = standardize(houseElectionDat16$popDensityPerSqMi),
         pctMale                     = standardize(houseElectionDat16$pctMale),
         pctMale_SE                  = pctMale_SE / sd(houseElectionDat16$pctMale),
         pctFemale                   = standardize(houseElectionDat16$pctFemale),
         pctFemale_SE                = pctFemale_SE / sd(houseElectionDat16$pctFemale),
         medianAgeYr                 = standardize(houseElectionDat16$medianAgeYr),
         medianAgeYr_SE              = medianAgeYr_SE / sd(houseElectionDat16$medianAgeYr),
         # need to strongly consider using calculated proportions of age groups rather than standarized counts
         age18_34Pop                 = standardize(houseElectionDat16$age18_34Pop),
         age18_34Pop_SE              = age18_34Pop_SE / sd(houseElectionDat16$age18_34Pop),
         age35_64Pop                 = standardize(houseElectionDat16$age35_64Pop),
         age35_64Pop_SE              = age35_64Pop_SE / sd(houseElectionDat16$age35_64Pop),
         age65plusPop                = standardize(houseElectionDat16$age65plusPop),
         age65plusPop_SE             = age65plusPop_SE / sd(houseElectionDat16$age65plusPop),
         pctEduHSorLessAge25plus     = standardize(houseElectionDat16$pctEduHSorLessAge25plus),
         pctEduHSorLessAge25plus_SE  = pctEduHSorLessAge25plus_SE / sd(houseElectionDat16$pctEduHSorLessAge25plus),
         pctEduSomColgAge25plus      = standardize(houseElectionDat16$pctEduSomColgAge25plus),
         pctEduSomColgAge25plus_SE   = pctEduSomColgAge25plus_SE / sd(houseElectionDat16$pctEduSomColgAge25plus),
         pctEduBachAge25plus         = standardize(houseElectionDat16$pctEduBachAge25plus),
         pctEduBachAge25plus_SE      = pctEduBachAge25plus_SE / sd(houseElectionDat16$pctEduBachAge25plus),
         pctEduGradProAge25plus      = standardize(houseElectionDat16$pctEduGradProAge25plus),
         pctEduGradProAge25plus_SE   = pctEduGradProAge25plus_SE / sd(houseElectionDat16$pctEduGradProAge25plus),
         pctEduBachPlusAge25plus     = standardize(houseElectionDat16$pctEduBachPlusAge25plus),
         pctEduBachPlusAge25plus_SE  = pctEduBachPlusAge25plus_SE / sd(houseElectionDat16$pctEduBachPlusAge25plus),
         pctUSBorn                   = standardize(houseElectionDat16$pctUSBorn),
         pctUSBorn_SE                = pctUSBorn_SE / sd(houseElectionDat16$pctUSBorn),
         pctForgnBornUSCtzn          = standardize(houseElectionDat16$pctForgnBornUSCtzn),
         pctForgnBornUSCtzn_SE       = pctForgnBornUSCtzn_SE / sd(houseElectionDat16$pctForgnBornUSCtzn),
         pctAge5PlusNonEnglshHome    = standardize(houseElectionDat16$pctAge5PlusNonEnglshHome),
         pctAge5PlusNonEnglshHome_SE = pctAge5PlusNonEnglshHome_SE / sd(houseElectionDat16$pctAge5PlusNonEnglshHome),
         pctInLaborForceUnemp        = standardize(houseElectionDat16$pctInLaborForceUnemp),
         pctInLaborForceUnemp_SE     = pctInLaborForceUnemp_SE / sd(houseElectionDat16$pctInLaborForceUnemp),
         pctNotInLaborForce          = standardize(houseElectionDat16$pctNotInLaborForce),
         pctNotInLaborForce_SE       = pctNotInLaborForce_SE / sd(houseElectionDat16$pctNotInLaborForce),
         medianHHIncome              = standardize(houseElectionDat16$medianHHIncome),
         medianHHIncome_SE           = medianHHIncome_SE / sd(houseElectionDat16$medianHHIncome),
         perCapitaIncome             = standardize(houseElectionDat16$perCapitaIncome),
         perCapitaIncome_SE          = perCapitaIncome_SE / sd(houseElectionDat16$perCapitaIncome),
         pctNoHealthInsCivNonInst    = standardize(houseElectionDat16$pctNoHealthInsCivNonInst),
         pctNoHealthInsCivNonInst_SE = pctNoHealthInsCivNonInst_SE / sd(houseElectionDat16$pctNoHealthInsCivNonInst),
         pctPovertyAge18_64          = standardize(houseElectionDat16$pctPovertyAge18_64),
         pctPovertyAge18_64_SE       = pctPovertyAge18_64_SE / sd(houseElectionDat16$pctPovertyAge18_64),
         pctWhiteAlone               = standardize(houseElectionDat16$pctWhiteAlone),
         pctWhiteAlone_SE            = pctWhiteAlone_SE / sd(houseElectionDat16$pctWhiteAlone),
         pctAfrAmAlone               = standardize(houseElectionDat16$pctAfrAmAlone),
         pctAfrAmAlone_SE            = pctAfrAmAlone_SE / sd(houseElectionDat16$pctAfrAmAlone),
         pctOtherAlone               = standardize(houseElectionDat16$pctOtherAlone),
         pctOtherAlone_SE            = pctOtherAlone_SE / sd(houseElectionDat16$pctOtherAlone),
         pctMultiRacial              = standardize(houseElectionDat16$pctMultiRacial),
         pctMultiRacial_SE           = pctMultiRacial_SE / sd(houseElectionDat16$pctMultiRacial),
         log_popDensPerSqMi          = standardize(houseElectionDat16$log_popDensPerSqMi) )

houseElectionDat_std <- 
  houseElectionDat12_std %>%
  bind_rows(houseElectionDat14_std,
            houseElectionDat16_std) %>%
    # the next two variables are entirely 0, so standardization doesn't serve any use
  mutate(primRunoffTotOth = 0,
         primRunoffMaxOth = 0)

setwd("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData")
write.csv(houseElectionDat_std, file = "houseElectionDat_std.csv", row.names = FALSE)
```

```{r, echo = F}
rm(annoText, predictorsDat12, predictorsDat14, predictorsDat16, predictorSummary, predDat12Means, predDat12SDs, predDat14Means, predDat14SDs, predDat16Means, predDat16SDs, varNames)
```

# Evaluating possible predictor variables - EDA and thoughts on potential posteriors

Given the abundance of predictors which could be included in the model, I decided to focus on those which I felt were most likely to have some meaningful (very plausibly non-zero) association with the predictor variable.

Additionally, I wanted predictor variables to provide good breadth of coverage for what I feel are certain key factors relevant to understanding election outcomes:

* Incumbency - whether a given district has the previously-elected candidate in the running, and - if so - which party they are a member of

* Campaign finances - some measure of the resources available (or utilized) for election contestants

* Primary election results - some measure of political activity in the primary elections to choose general-election candidates

* Demographic factors - things such as population density (to get an idea of relative urbanicity/rurality), age or educational mix, and economic status, among others

## Summary of district-level predictors considered

* `incumbentD` and `incumbentR` - district has an incumbent for D/R party contesting the election (count)

* `cmpgnMaxDisbursDvsR` - difference in maximum reported campaign spend by D/R candidate (*I considered using TOTAL spend by party here, but correlation between max and total spend is very high here - $\approx 0.95$ for D and $\approx 0.97$ for R across the full dataset*)

* `primaryTotDvsR` - difference in total vote count for each party in primary elections (*may consider primary winner's share of total party primary votes in future iteration*)

* `primaryUnopD` and `primaryUnopR` - district has a D/R candidate running unopposed in the primary (Boolean)

* `popDensityPerSqMi` - district population density per square mile

* `medianAgeYr` - district median age

* `pctEduHSorLessAge25plus` - district % of age 25+ population with HS or less as highest completed education level

* `pctEduGradProAge25plus` - district % of age 25+ population with Graduate/Professional degree as highest completed education level

* `pctInLaborForceUnemp` - % of district labor force (age 16+, either in civilian labor force (employed or seeking work, not in an institution e.g. school or prison) OR armed forces) unemployed

* `perCapitaIncome` - total district income divided by total district population

* `pctWhiteAlone` - % of total district population identifying as 'white' only

*Potential alternative/additional predictors OR for another model - for future consideration*:  

`cmpgnTotDisbursD` and `cmpgnTotDisbursR`, `primaryMax[D/R]` / `primaryTot[D/R]`, `generalNumCandTotal`, `pctFemale`, `age18_34Pop` and `age65plusPop`, `pctEduGradProAge25plus`, `pctUSBorn` AND/OR `pctForgnBornUSCtzn`, `pctNotInLaborForce`, `pctNoHealthInsCivNonInst`, `pctPovertyAge18_64`, `pctAfrAmAlone`, `pctMultiRacial`

## Exploratory Data Analysis - predictors and outcome variables

While these plots use non-standardized data for the sake of exploring the data in each variable's natural scale, the fit models employ standardized numerical variables to allow greater ease of interpreting predictor coefficients in terms of relative magnitudes.

*Note: Each predictor will be assigned a moderately regularizing normal prior, centered at 0.*

**Incumbency and outcome**

```{r, echo = F, fig.align = "center", fig.width = 11}
# outcome by incumbency - this was previously created for MAT 8452 project work
# how many times did the incumbent win, by party and year?
incumbentDat <- 
  tibble(district2 = houseElectionDat$district2,
         year      = houseElectionDat$year,
         incumbent = if_else(houseElectionDat$incumbentD >= 1 & 
                             houseElectionDat$incumbentR == 0 &
                             houseElectionDat$incumbentOth == 0, "D",
                        if_else(houseElectionDat$incumbentD == 0 & 
                                houseElectionDat$incumbentR >= 1 &
                                houseElectionDat$incumbentOth == 0, "R",
                        if_else(houseElectionDat$incumbentD == 0 & 
                                houseElectionDat$incumbentR == 0 &
                                houseElectionDat$incumbentOth >= 1, "Oth", 
                           if_else(houseElectionDat$incumbentD == 0 & 
                                   houseElectionDat$incumbentR == 0 &
                                   houseElectionDat$incumbentOth == 0, "None", 
                                   "Check")))),
         generalWinner = houseElectionDat$generalWinner,
         result    = if_else(houseElectionDat$incumbentWinD == 1, "D incumbent won",
                        if_else(houseElectionDat$incumbentWinR == 1, "R incumbent won",
                           if_else(houseElectionDat$incumbentWinOth == 1, "Oth incumbent won",
                              if_else(houseElectionDat$incumbentD >= 1 &
                                      houseElectionDat$incumbentWinD != 1 &
                                      houseElectionDat$generalWinner == "D", "D incumbent lost, other D won",
                              if_else(houseElectionDat$incumbentD >= 1 &
                                      houseElectionDat$incumbentWinD != 1 &
                                      houseElectionDat$generalWinner == "R", "D incumbent lost, R won",
                              if_else(houseElectionDat$incumbentD >= 1 &
                                      houseElectionDat$incumbentWinD != 1 &
                                      houseElectionDat$generalWinner == "Oth", "D incumbent lost, Oth won",
                                  if_else(houseElectionDat$incumbentR >= 1 &
                                          houseElectionDat$incumbentWinR != 1 &
                                          houseElectionDat$generalWinner == "R", "R incumbent lost, other R won",
                                  if_else(houseElectionDat$incumbentR >= 1 &
                                          houseElectionDat$incumbentWinR != 1 &
                                          houseElectionDat$generalWinner == "D", "R incumbent lost, D won",
                                  if_else(houseElectionDat$incumbentR >= 1 &
                                          houseElectionDat$incumbentWinR != 1 &
                                          houseElectionDat$generalWinner == "Oth", "R incumbent lost, Oth won",
                                     if_else(incumbent == "None" & houseElectionDat$generalWinner == "D",
                                             "No incumbent, D won",
                                     if_else(incumbent == "None" & houseElectionDat$generalWinner == "R",
                                             "No incumbent, R won",
                                     if_else(incumbent == "None" & houseElectionDat$generalWinner == "Oth",
                                             "No incumbent, Oth won", "Check")))))))))))) ) %>%
  mutate(result = factor(result, 
                         levels = c("D incumbent won",
                                    "D incumbent lost, other D won",
                                    "R incumbent lost, D won",
                                    "No incumbent, D won",
                                    "D incumbent lost, Oth won",
                                    "Oth incumbent won",
                                    "No incumbent, Oth won",
                                    "R incumbent lost, Oth won",
                                    "No incumbent, R won",
                                    "D incumbent lost, R won",
                                    "R incumbent lost, other R won",
                                    "R incumbent won")) ) %>%
  # two districts in 2012 had both R & D incumbents
  # due to post-2010 redistricting - these were removed from the plot
  filter(incumbent != "Check")

# separate dataset for annotation
annoText <- 
  tibble(year   = c(2012, 2012, 2014, 2014, 2016, 2016),
         result = factor(rep(c("R incumbent lost, D won", 
                               "D incumbent lost, R won"), 3),
                         levels = levels(incumbentDat$result)),
         labHt  = c(132, 189, 154, 195, 144, 203),
         labTxt = c(paste("D seats:","\n",sum(incumbentDat$year == 2012 & 
                                              incumbentDat$generalWinner == "D")),
                    paste("R seats:","\n",sum(incumbentDat$year == 2012 & 
                                              incumbentDat$generalWinner == "R")),
                    paste("D seats:","\n",sum(incumbentDat$year == 2014 & 
                                              incumbentDat$generalWinner == "D")),
                    paste("R seats:","\n",sum(incumbentDat$year == 2014 & 
                                              incumbentDat$generalWinner == "R")),
                    paste("D seats:","\n",sum(incumbentDat$year == 2016 & 
                                              incumbentDat$generalWinner == "D")),
                    paste("R seats:","\n",sum(incumbentDat$year == 2016 & 
                                              incumbentDat$generalWinner == "R"))) )

incumbentDat %>%
  ggplot() +
  geom_bar(aes(x = result, fill = result)) +
  geom_text(data = annoText, 
            aes(x = result, y = labHt-10, label = labTxt, color = result), 
            size = 4) +
  facet_wrap(. ~ factor(year)) +
  labs(title = "Summary of incumbency and outcome by year",
       x = NULL, y = "# of House seats", fill = NULL,
       caption = "Note: post-2010 redistricting meant 2 2012 districts
       had both a D and an R incumbent; those are removed") +
  scale_color_manual(values = c("blue", "red"), guide = F) +
  scale_fill_brewer(type = "div", palette = "RdBu", direction = -1) +
  theme_ds1() + 
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        legend.position = "top")

rm(annoText, incumbentDat)
```

The above shows that, in the vast majority of cases where there is an incumbent (standing for re-election) from a given party, that incumbent wins. I anticipate a relatively strongly positive posterior distribution for `incumbentD` and an equally negative posterior for `incumbentR`.

**Reported campaign spending - maximum vs. total by party (per district)**

```{r, echo = F, fig.align = "center", fig.width = 8.5, fig.height = 7}
spendDatD <- 
  houseElectionDat %>%
  select(district2, year, cmpgnTotDisbursD, cmpgnMaxDisbursD) %>%
  mutate(cmpgnTotDisburs = cmpgnTotDisbursD,
         cmpgnMaxDisburs = cmpgnMaxDisbursD,
         party           = "D") %>%
  select(-cmpgnTotDisbursD, -cmpgnMaxDisbursD)

spendDatR <- 
  houseElectionDat %>%
  select(district2, year, cmpgnTotDisbursR, cmpgnMaxDisbursR) %>%
  mutate(cmpgnTotDisburs = cmpgnTotDisbursR,
         cmpgnMaxDisburs = cmpgnMaxDisbursR,
         party           = "R") %>%
  select(-cmpgnTotDisbursR, -cmpgnMaxDisbursR)

spendDatD %>% 
  bind_rows(spendDatR) %>%
  ggplot(aes(x = cmpgnMaxDisburs, y = cmpgnTotDisburs, color = party)) +
  geom_abline(intercept = 0, slope = 1, color = "darkgrey") +
  geom_point(alpha = 0.3) +
  facet_wrap(party ~ year) +
  labs(title = "Total vs. Maximum campaign spending by party and year",
       subtitle = "The two are generally quite closely aligned",
       x = "Maximum reported campaign spend, USD",
       y = "Total reported campaing spend, USD") +
  scale_color_manual(values = c("blue", "red"), guide = F) +
  theme_ds1() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5))

rm(spendDatD, spendDatR)
```

Maximum and Total spend by party / year / district are generally quite closely related - for now I'll consider maximum campaign spend levels.

**Reported campaign spending - maximum D minus R vs. outcome by party (per district)**

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
bins <- 100
cols <- c("darkred", "red",
          "lightgrey",
          "blue", "darkblue")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(47)
cuts <- cut(houseElectionDat$cmpgnMaxDisbursDvsR,bins)
names(cuts) <- sapply(cuts,function(t)
                           cut.cols[which(as.character(t) == levels(cuts))]) 

houseElectionDat %>%
  ggplot(aes(x = cmpgnMaxDisbursDvsR, fill = cut(cmpgnMaxDisbursDvsR, bins))) +
  geom_histogram(bins = 100) +
  geom_vline(xintercept = 0, color = "darkgrey") +
  facet_wrap(generalWinner ~ year) +
  labs(title = "Election outcome and Difference in maximum campaign spend",
       x = "Year (cols) \n Difference in maximum campaign spend (USD), D minus R",
       y = "Election winner (rows) \n and Bin count (axis labels)") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()
```

It appears that districts where one party has greater reported spend relative to the other is more likely to win that election. It seems reasonable to expect a weakly positive posterior in this case.

**Primary votes - maximum vs. total by party (per district)**

```{r, echo = F, fig.align = "center", fig.width = 8.5, fig.height = 7}
primaryDatD <- 
  houseElectionDat %>%
  select(district2, year, primaryTotD, primaryMaxD) %>%
  mutate(primaryTot = primaryTotD,
         primaryMax = primaryMaxD,
         party           = "D") %>%
  select(-primaryTotD, -primaryMaxD)

primaryDatR <- 
  houseElectionDat %>%
  select(district2, year, primaryTotR, primaryMaxR) %>%
  mutate(primaryTot = primaryTotR,
         primaryMax = primaryMaxR,
         party           = "R") %>%
  select(-primaryTotR, -primaryMaxR)

primaryDatD %>% 
  bind_rows(primaryDatR) %>%
  ggplot(aes(x = primaryMax, y = primaryTot, color = party)) +
  geom_abline(intercept = 0, slope = 1, color = "darkgrey") +
  geom_point(alpha = 0.3) +
  facet_wrap(party ~ year) +
  labs(title = "Total vs. Maximum primary votes by party and year",
       subtitle = "The two are less closely aligned than for campaign spend",
       x = "Maximum primary votes",
       y = "Total primary votes") +
  scale_color_manual(values = c("blue", "red"), guide = F) +
  theme_ds1() + theme(axis.text.x = element_text(angle = 45, vjust = 0.5))

rm(primaryDatD, primaryDatR)
```

While this is somewhat similar to the comparison of maximum / total reported campaign spend, the greater fan-shaped distribution of points indicates there is somewhat less of a correlation between maximum / total primary votes (by party) than in the case of maximum / total campaign spend.

I may be introducing inconsistency in my approach here, but I feel that maximum campaign spend is a more relevant metric than total spend, while the reverse is true in terms of primary votes. This is because, I believe, that total primary vote count for a given party is indicative of the enthusiasm/interest for a given party in a district. While maximum primary vote count may reflect the winning candidate's strength relative to their competition, I anticipate that voters are more likely to back their preferred party and to some degree disregard the primary winner's relative strength than they are to put greater weight on primary win margins. 

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
bins <- 175
cols <- c("darkred", "red",
          "lightgrey",
          "blue", "darkblue")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(138)
cuts <- cut(houseElectionDat$primaryTotDvsR,bins)
names(cuts) <- sapply(cuts,function(t)
                           cut.cols[which(as.character(t) == levels(cuts))]) 

houseElectionDat %>%
  ggplot(aes(x = primaryTotDvsR, fill = cut(primaryTotDvsR, bins))) +
  geom_histogram(bins = 100) +
  facet_wrap(generalWinner ~ year) +
  labs(title = "Election outcome and Difference in total primary votes",
       x = "Year (cols) \n Difference in total primary votes, D minus R",
       y = "Election winner (rows) \n and Bin count (axis labels)") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  scale_y_continuous(breaks = seq(0, 55, 10)) +
  coord_cartesian(ylim = c(0, 55)) +
  theme_ds1()

rm(bins, cols, cuts)
```

As with difference in maximum campaign spend, difference in total primary vote by party seems to provide a reasonably good indicator of the ultimate likelihood a Democratic candidate wins election in the House.

It seems prudent to compare the two predictors against each other, in case there is excessive correlation between the two variables:  

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
houseElectionDat %>%
  ggplot(aes(x = cmpgnMaxDisbursDvsR, y = primaryTotDvsR)) +
  geom_vline(xintercept = 0, color = "darkgrey") +
  geom_hline(yintercept = 0, color = "darkgrey") +
  geom_point(alpha = 0.4, color = color_set8[6]) +
  facet_wrap(. ~ year) +
  labs(title = "Difference in total primary votes vs. \n Difference in campaign spend",
       x = "Difference in maximum campaign spend (USD), D minus R",
       y = "Difference in total primary votes, \n D minus R") +
  theme_ds1()
```

While there is certainly evidence of a positive correlation between the party difference in maximum campaign spend and the party difference in total primary votes, it's not worryingly high:

```{r}
houseElectionDat %>% 
  group_by(year) %>% 
  summarize(corVal = round(cor((cmpgnMaxDisbursD - cmpgnMaxDisbursR),
                       (primaryTotD - primaryTotR)), 3) ) %>%
  kable() %>%
  kable_styling(full_width = F)
```

I'm not sure, but the widening discrepancy over time may be reflective of increasing partisan lean within districts over time. It should be noted that the 2016 presidential campaign was cast as especially partisan, however, so it may be related to that.

**Primary unopposed D/R candidate**  

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
x <- houseElectionDat %>%
  group_by(primaryUnopD, primaryUnopR, year) %>%
  summarize(winD = sum(generalWinner == "D"),
            winR = sum(generalWinner == "R") ) %>%
  as_tibble()

x %>%
  mutate(primaryUnopD = if_else(primaryUnopD == 0, "D primary contested",
                                                   "D primary unopposed"),
         primaryUnopR = if_else(primaryUnopR == 0, "R primary contested",
                                                   "R primary unopposed")) %>%
  ggplot() +
  geom_col(aes(x = year - 0.25, y = winD), fill = "blue", width = 0.5) +
  geom_col(aes(x = year + 0.25, y = winR), fill = "red", width = 0.5) +
  geom_text(aes(x = year - 0.25, y = winD + 17, label = winD)) +
  geom_text(aes(x = year + 0.25, y = winR + 17, label = winR)) +
  facet_wrap(primaryUnopD ~ primaryUnopR) +
  labs(title = "Primary unopposed for [party] and election outcome",
       x = "Year",
       y = "Count",
       caption = "Blue: D win, Red: R win") +
  scale_x_continuous(breaks = seq(2012, 2016, 2)) +
  theme_ds1()

rm(x)
```

The plot above suggests a candidates' running unopposed in their party's primary election doesn't seems to provide a great deal of insight into the likelihood they (or at least their party) win the general election. Also, in the majority of elections, both the Republican and Democratic primaries have multiple challengers.

**Population density**

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
houseElectionDat %>%
  ggplot(aes(x = popDensityPerSqMi / 1000, y = generalWinD)) +
  geom_jitter(alpha = 0.5, color = color_set8[6], width = 0) +
  labs(title = "Population density and Democratic winner",
       x = "Population density (pop. in 1,000s per sq. mi.)",
       y = "D election winner") +
  facet_wrap(. ~ year) +
  scale_y_continuous(breaks = 0:1) +
  theme_ds1()

houseElectionDat %>%
  group_by(year, generalWinner) %>%
  mutate(med = median(popDensityPerSqMi) / 1000) %>%
  ggplot(aes(x = popDensityPerSqMi / 1000)) +
  #geom_density(adjust = 10, fill = color_set8[6]) +
  geom_histogram(bins = 20, fill = color_set8[6]) +
  facet_wrap(generalWinner ~ year) +
  geom_vline(aes(xintercept = med)) +
  labs(title = "Election outcome and Population density",
       x = "Year (cols) \n Population density (pop. in 1,000s per sq. mi.)",
       y = "Election winner (rows) \n and Bin count (axis labels)",
       caption = "vertical line at median value of each section") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

houseElectionDat %>%
  group_by(year, generalWinner) %>%
  mutate(med = median(log_popDensPerSqMi)) %>%
  ggplot(aes(x = log_popDensPerSqMi)) +
  #geom_density(adjust = 10, fill = color_set8[6]) +
  geom_histogram(bins = 20, fill = color_set8[6]) +
  facet_wrap(generalWinner ~ year) +
  geom_vline(aes(xintercept = med)) +
  labs(title = "Election outcome and log(Population density)",
       subtitle = "vertical line at median value of each section",
       x = "Year (cols) \n log(Population density) (pop. per sq. mi.)",
       y = "Election winner (rows) \n and Bin count (axis labels)",
       caption = expression(e^3 %~~% 20*","*~e^{6} %~~% 400*","*~e^{9} %~~% 8100)) +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()
```

Most districts have relatively low density, but Democratic candidates fare disproportionately well in high-density districts. A moderately weakly positive posterior seems a reasonable expectation.

**Because log-population density has a much more symmetrical distribution, I'll use that.** *[Updated 12/22/18]*

**Median age**

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
houseElectionDat %>%
  group_by(year, generalWinner) %>%
  mutate(med = median(medianAgeYr)) %>%
  ggplot(aes(x = medianAgeYr)) +
  geom_histogram(bins = 50, fill = color_set8[6]) +
  facet_wrap(generalWinner ~ year) +
  geom_vline(aes(xintercept = med)) +
  labs(title = "Election outcome and Median age",
       x = "Year (cols) \n Median age (years)",
       y = "Election winner (rows) \n and Bin count (axis labels)",
       caption = "vertical line at median value of each section") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()
```

It seems that Democratic candidates might fare relatively better in districts with lower median ages, and Republican candidates fare better in districts with higher median ages, but there's nothing especially impressive here.

Perhaps looking at district composition by proportions in younger / older age groups would be useful?

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
houseElectionDat %>%
  group_by(year, generalWinner) %>%
  mutate(med = median(age18_34Pop / totalPop)) %>%
  ggplot(aes(x = age18_34Pop*100 / totalPop)) +
  geom_histogram(bins = 40, fill = color_set8[6]) +
  facet_wrap(generalWinner ~ year) +
  geom_vline(aes(xintercept = med*100)) +
  labs(title = "Election outcome and % of population age 18-34",
       x = "Year (cols) \n % of population age 18-34",
       y = "Election winner (rows) \n and Bin count (axis labels)",
       caption = "vertical line at median value of each section") +
  scale_x_continuous(breaks = seq(15,40, 5)) +
  theme_ds1()

houseElectionDat %>%
  group_by(year, generalWinner) %>%
  mutate(med = median(age65plusPop / totalPop)) %>%
  ggplot(aes(x = age65plusPop*100 / totalPop)) +
  geom_histogram(bins = 40, fill = color_set8[6]) +
  facet_wrap(generalWinner ~ year) +
  geom_vline(aes(xintercept = med*100)) +
  labs(title = "Election outcome and % of population age 65+",
       x = "Year (cols) \n % of population age 65+",
       y = "Election winner (rows) \n and Bin count (axis labels)",
       caption = "vertical line at median value of each section") +
  scale_x_continuous(breaks = seq(15,40, 5)) +
  theme_ds1()
```

As with the median age plot, there's mildly suggestive evidence here, but nothing conclusive. I'll stick with `medianAgeYr` for now, and expect a weak, mildly negative posterior.

**Education level among the age 25+ population**

*I originally also considered using "undergraduate or greater" education here, but there was no meaningful pattern there, so I revised to "graduate/professional degree".*

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
houseElectionDat %>%
  group_by(year, generalWinner) %>%
  mutate(med = median(pctEduHSorLessAge25plus)) %>%
  ggplot(aes(x = pctEduHSorLessAge25plus)) +
  geom_histogram(bins = 40, fill = color_set8[6]) +
  facet_wrap(generalWinner ~ year) +
  geom_vline(aes(xintercept = med)) +
  labs(title = "Election outcome and % of age 25+ population HS or Less education",
       x = "Year (cols) \n % of age 25+ population HS or Less education",
       y = "Election winner (rows) \n and Bin count (axis labels)",
       caption = "vertical line at median value of each section") +
  theme_ds1()

houseElectionDat %>%
  group_by(year, generalWinner) %>%
  mutate(med = median(pctEduGradProAge25plus)) %>%
  ggplot(aes(x = pctEduGradProAge25plus)) +
  geom_histogram(bins = 40, fill = color_set8[6]) +
  facet_wrap(generalWinner ~ year) +
  geom_vline(aes(xintercept = med)) +
  labs(title = "Election outcome and % of age 25+ population Graduate/Professional degree",
       x = "Year (cols) \n % of age 25+ population Graduate/Professional degree",
       y = "Election winner (rows) \n and Bin count (axis labels)",
       caption = "vertical line at median value of each section") +
  theme_ds1()

houseElectionDat %>%
  ggplot(aes(x = pctEduHSorLessAge25plus, y = pctEduGradProAge25plus)) +
  geom_point(alpha = 0.5, color = color_set8[6]) +
  labs(title = "Distribution of lower and higher education among age 25+ population",
       subtitle = "",
       x = "% of age 25+ population HS or Less education",
       y = "% of age 25+ population \n Graduate/Professional degree") +
  facet_wrap(. ~ year) +
  theme_ds1()
```

There is marginal at best evidence of HS or less education having greater association with Democratic or Republican electoral success, but there is reasonably good evidence of districts with greater concentration of graduate/professional degree holders favoring Democratic candidates.

I'll drop the HS or less predictor, and anticipate a weakly positive posterior for the graduate/professional degree predictor.

**Labor force unemployment rate**

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
houseElectionDat %>%
  group_by(year, generalWinner) %>%
  mutate(med = median(pctInLaborForceUnemp)) %>%
  ggplot(aes(x = pctInLaborForceUnemp)) +
  geom_histogram(bins = 40, fill = color_set8[6]) +
  facet_wrap(generalWinner ~ year) +
  geom_vline(aes(xintercept = med)) +
  labs(title = "Election outcome and Labor force unemployment rate",
       x = "Year (cols) \n Labor force unemployment rate (%)",
       y = "Election winner (rows) \n and Bin count (axis labels)",
       caption = "vertical line at median value of each section") +
  theme_ds1()
```

There is some evidence of districts with higher unemployment rates having a greater association with Democratic candidate election winners. Here it seems that a weakly positive posterior could arise.

Let's check to what extent unemployment rate appears to be correlated with graduate/professional education:

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
corVals <- 
  houseElectionDat %>%
  group_by(year) %>%
  summarize(corVal = cor(pctEduGradProAge25plus, pctInLaborForceUnemp))

corDat <- 
  tibble(year   = c(2012, 2014, 2016),
         corExp = c(paste("r ==", round(corVals[1,2], 2)),
                    paste("r ==", round(corVals[2,2], 2)),
                    paste("r ==", round(corVals[3,2], 2)) ),
         xVal   = rep(27, 3),
         yVal   = rep(11, 3) )

houseElectionDat %>%
  ggplot(aes(x = pctEduGradProAge25plus, y = pctInLaborForceUnemp)) +
  geom_point(alpha = 0.5, color = color_set8[6]) +
  geom_text(data = corDat, aes(x = xVal, y = yVal, label = corExp), parse = T) +
  labs(title = "% of age 25+ population Graduate/Professional degree and \n Unemployment rate",
       x = "% of age 25+ population Graduate/Professional degree",
       y = "Labor force unemployment rate (%)") +
  facet_wrap(. ~ year) +
  theme_ds1()

rm(corVals, corDat)
```

While the districts with the greatest relative proportions of graduate/professional degree-holders have consistently low unemployment rates, there is increasingly great variability in unemployment rates as the district proportion of graduate/professional degree-holders decreases. This results in a weak negative correlation between the two variables.

**Per capita income**

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
houseElectionDat %>%
  group_by(year, generalWinner) %>%
  mutate(med = median(perCapitaIncome) / 1000) %>%
  ggplot(aes(x = perCapitaIncome / 1000)) +
  geom_histogram(bins = 40, fill = color_set8[6]) +
  facet_wrap(generalWinner ~ year) +
  geom_vline(aes(xintercept = med)) +
  labs(title = "Election outcome and Per capita income",
       x = "Year (cols) \n Per capita income (1,000 USD per person)",
       y = "Election winner (rows) \n and Bin count (axis labels)",
       caption = "vertical line at median value of each section") +
  theme_ds1()
```

There also appears to be a positive association between greater per capita income for a district and that district's electing a Democratic candidate. Again, a weakly positive posterior seems plausible.

As with unemployment rate, it seems prudent to consider the extent to which per capita income is correlated with the proportion of highly credentialed inhabitants, and also with the unemployment rate.

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
corVals <- 
  houseElectionDat %>%
  group_by(year) %>%
  summarize(corVal = cor(pctEduGradProAge25plus, perCapitaIncome))

corDat <- 
  tibble(year   = c(2012, 2014, 2016),
         corExp = c(paste("r ==", round(corVals[1,2], 3)),
                    paste("r ==", round(corVals[2,2], 3)),
                    paste("r ==", round(corVals[3,2], 3)) ),
         xVal   = rep(25, 3),
         yVal   = rep(20, 3) )

houseElectionDat %>%
  ggplot(aes(x = pctEduGradProAge25plus, y = perCapitaIncome / 1000)) +
  geom_point(alpha = 0.5, color = color_set8[6]) +
  geom_text(data = corDat, aes(x = xVal, y = yVal, label = corExp), parse = T) +
  labs(title = "% of age 25+ population Graduate/Professional degree and \n Per capita income",
       x = "% of age 25+ population Graduate/Professional degree",
       y = "District per capita income (1,000 USD)") +
  facet_wrap(. ~ year) +
  theme_ds1()

corVals <- 
  houseElectionDat %>%
  group_by(year) %>%
  summarize(corVal = cor(pctInLaborForceUnemp, perCapitaIncome))

corDat <- 
  tibble(year   = c(2012, 2014, 2016),
         corExp = c(paste("r ==", round(corVals[1,2], 2)),
                    paste("r ==", round(corVals[2,2], 2)),
                    paste("r ==", round(corVals[3,2], 2)) ),
         xVal   = rep(10, 3),
         yVal   = rep(50, 3) )

houseElectionDat %>%
  ggplot(aes(x = pctInLaborForceUnemp, y = perCapitaIncome / 1000)) +
  geom_point(alpha = 0.5, color = color_set8[6]) +
  geom_text(data = corDat, aes(x = xVal, y = yVal, label = corExp), parse = T) +
  labs(title = "Labor force unemployment rate and \n Per capita income",
       x = "Labor force unemployment rate (%)",
       y = "District per capita income (1,000 USD)") +
  facet_wrap(. ~ year) +
  theme_ds1()

rm(corVals, corDat)
```

There is a strong, consistent positive correlation between graduate/professional degree and per capita income. If the inclusion of both predictors becomes problematic for fitting the model, I'm inclined to drop the per capita income predictor, as this would seem to be the more difficult of the two to accurately measure in a "real world" setting faced by a non-governmental entity interested in estimating House election outcomes.

There is also a weaker, consistent negative correlation between unemployment rate and per capita income.

**Proportion of population identifying as white**

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 5}
houseElectionDat %>%
  ggplot(aes(x = pctWhiteAlone, y = generalWinD)) +
  geom_vline(xintercept = 50, color = "grey") +
  geom_jitter(alpha = 0.5, color = color_set8[6], width = 0) +
  labs(title = "Proportion of population identifying as white and Democratic winner",
       x = "% of population identifying as white",
       y = "D election winner") +
  facet_wrap(. ~ year) +
  scale_y_continuous(breaks = 0:1) +
  theme_ds1()

houseElectionDat %>%
  group_by(year, generalWinner) %>%
  mutate(med = median(pctWhiteAlone)) %>%
  ggplot(aes(x = pctWhiteAlone)) +
  geom_histogram(bins = 40, fill = color_set8[6]) +
  facet_wrap(generalWinner ~ year) +
  geom_vline(aes(xintercept = med)) +
  geom_vline(xintercept = 50, color = "lightgrey", linetype = 2) +
  labs(title = "Election outcome and Proportion of population identifying as white",
       x = "Year (cols) \n % of population identifying as white",
       y = "Election winner (rows) \n and Bin count (axis labels)",
       caption = "vertical line at median value of each section") +
  theme_ds1()
```

Districts where 50% or less of the population identifies as white only appear to only elect Democratic candidates in the three years considered (with the exception of a single district in 2016). For districts where more than half of the population identifies as white only, Democratic candidates *are* elected, but not nearly as often as are Republican candidates.

I anticipate a negative posterior will emerge here.

# Model fitting and analysis

My original model-fitting approach was the following:

1) Fit models for standardized 2012 data only, using varying subsets of predictors, and compare models by information criterion values and by information criterion-based relative model weights. Also confirm the models were fit well using trace plots and related diagnostics.

2) Among the "top-performing" model(s) selected, evaluate in-sample prediction performance, and consider whether trimming the "full" model (with all considered predictors) leads to improved IC metrics.

3) For the same model(s) chosen from step 2, make predictions using 2014 data and evaluate.

4) Fit the "full" model to 2014 standardized data and assess model fit as well as whether a "reduced" version has superior IC metrics, and any major differences compared to the 2012 "full" model. Repeat assessment of performance in predicting "in sample" (2014) and then "out of sample" (2016) data.

5) Repeat step 4 for 2016 data, without the "out of sample" prediction.

6) Fit "all years full" models, this time considering fixed-effects (intercept and slopes) vs. varying-effects models since now there are 3 observations for each district (one for each year's election). Analyze the fit and in-sample performance of these models and conclude that the two models are not meaningfully different, and relegate the analysis to the Appendix.

7) Fit "all years full" models with binomial-outcome variables (aggregating predictors across the three years considered for each district). Analyze the fit and in-sample performance of these models and go forward with sensitivity analysis (considering the effect of looser priors on posterior estimates) and counterfactual analysis (holding other predictors constant at defined levels, assessing the impact on predicted outcomes when manipulating the level of a single predictor) for the "top" model.

**In order to "get to the good stuff in this report, step 7 from above is shown in the next section, and all work from steps 1 through 5 is in Appendix 1, while all work from step 6 is in Appendix 2.**

# Fitting and Analyzing "all years" models

```{r}
# load standardized predictors set
houseElectionDat_std <- 
  read.csv("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/houseElectionDat_std.csv")

# break out 2012, 2014, 2016 data
houseElectionDat12_std <- 
  houseElectionDat_std %>%
  filter(year == 2012)

houseElectionDat14_std <- 
  houseElectionDat_std %>%
  filter(year == 2014)

houseElectionDat16_std <- 
  houseElectionDat_std %>%
  filter(year == 2016)
```

## Model 2 - Binomial model, using 2012 / 2014 / 2016 data

This section treats the outcome of a given districts *set of elections* as a Binomial random variable (with the outcome being a Democratic candidate's election 0, 1, 2, or 3 times out of 3 elections).

Rather than treating each year's district-level election as unique events, let's fit fixed-effects and varying-effects models evaluating each district's propensity to elect a Democratic or Republican candidate - in other words, aggregating data at the district level. This should also hopefully provide a proper setting for partial pooling to improve the model fit.

To aggregate predictor data, I'll take the mean of the three years' observations for each predictor. *Note: While I also tried fitting a model incorporating measurement error (as the standard error of each district's mean predictor value), it is apparently more computationally demanding than I'd thought, so I'm sticking with the two models shown below.*

```{r}
# first need to aggregate predictor estimates and response outcomes at district level
# here I take the mean of each district-level predictor
# and include district-level measurement error as sd(predictor) / sqrt(nObs = 3)

houseElectionDat_std_agg <- 
  houseElectionDat_std %>%
  group_by(district2) %>%
  # only going to retain predictors previously used in full models thus far
  # note: aside from the outcome var, predictors are MEAN values
  # note: adding a very small positive constant to _SE vars to resolve issue
  summarize(nObs = n(),
            generalWinD   = sum(generalWinD),
            incumbentD_mn    = mean(incumbentD),
            incumbentD_SE = sd(incumbentD) / sqrt(nObs),
            incumbentR_mn    = mean(incumbentR),
            incumbentR_SE = sd(incumbentR) / sqrt(nObs),
            primaryUnopD_mn  = mean(primaryUnopD),
            primaryUnopD_SE = sd(primaryUnopD) / sqrt(nObs),
            primaryUnopR_mn  = mean(primaryUnopR),
            primaryUnopR_SE = sd(primaryUnopR) / sqrt(nObs),
            cmpgnMaxDisbursDvsR_mn = mean(cmpgnMaxDisbursDvsR),
            cmpgnMaxDisbursDvsR_SE = sd(cmpgnMaxDisbursDvsR) / sqrt(nObs),
            primaryTotDvsR_mn = mean(primaryTotDvsR),
            primaryTotDvsR_SE = sd(primaryTotDvsR) / sqrt(nObs),
            popDensityPerSqMi_mn = mean(popDensityPerSqMi),
            popDensityPerSqMi_SE = sd(popDensityPerSqMi) / sqrt(nObs),
            medianAgeYr_mn = mean(medianAgeYr),
            medianAgeYr_SE = sd(medianAgeYr) / sqrt(nObs),
            pctEduGradProAge25plus_mn = mean(pctEduGradProAge25plus),
            pctEduGradProAge25plus_SE = sd(pctEduGradProAge25plus) / sqrt(nObs),
            pctInLaborForceUnemp_mn = mean(pctInLaborForceUnemp),
            pctInLaborForceUnemp_SE = sd(pctInLaborForceUnemp) / sqrt(nObs),
            perCapitaIncome_mn = mean(perCapitaIncome),
            perCapitaIncome_SE = sd(perCapitaIncome) / sqrt(nObs),
            pctWhiteAlone_mn = mean(pctWhiteAlone),
            pctWhiteAlone_SE = sd(pctWhiteAlone) / sqrt(nObs),
            log_popDensPerSqMi_mn = mean(log_popDensPerSqMi),
            log_popDensPerSqMi_SE = sd(log_popDensPerSqMi) / sqrt(nObs) )
```

### Fitting the models, and preliminary fit summaries

```{r, eval = F}
m2.allFull_fIfS <- 
  brm(generalWinD | trials(nObs) ~ 1 + incumbentD_mn + incumbentR_mn +
                                   primaryUnopD_mn + primaryUnopR_mn + 
                                   cmpgnMaxDisbursDvsR_mn + primaryTotDvsR_mn + 
                                   log_popDensPerSqMi_mn + medianAgeYr_mn + 
                                   pctEduGradProAge25plus_mn + 
                                   pctInLaborForceUnemp_mn + perCapitaIncome_mn + 
                                   pctWhiteAlone_mn, 
      data = houseElectionDat_std_agg, family = binomial(), 
      prior = c(set_prior("normal(0, 3)", class = "Intercept"),
                set_prior("normal(0, 2)", class = "b")), 
      iter = 7000, warmup = 3000, chains = 3, 
      control = list(adapt_delta = 0.99),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_fIfS")

m2.allFull_vIvS <- 
  brm(generalWinD | trials(nObs) ~ 1 + incumbentD_mn + incumbentR_mn + 
                        primaryUnopD_mn + primaryUnopR_mn + cmpgnMaxDisbursDvsR_mn + 
                        primaryTotDvsR_mn + log_popDensPerSqMi_mn + medianAgeYr_mn + 
                        pctEduGradProAge25plus_mn + pctInLaborForceUnemp_mn + 
                        perCapitaIncome_mn + pctWhiteAlone_mn + 
                   (1 + incumbentD_mn + incumbentR_mn + 
                        primaryUnopD_mn + primaryUnopR_mn + cmpgnMaxDisbursDvsR_mn + 
                        primaryTotDvsR_mn + log_popDensPerSqMi_mn + medianAgeYr_mn + 
                        pctEduGradProAge25plus_mn + pctInLaborForceUnemp_mn + 
                        perCapitaIncome_mn + pctWhiteAlone_mn |district2), 
      data = houseElectionDat_std_agg, family = binomial(), 
      prior = c(set_prior("normal(0, 3)", class = "Intercept"),
                set_prior("normal(0, 2)", class = "b"), 
                set_prior("cauchy(0, 2)", class = "sd"),
                set_prior("lkj(2)", class = "cor")), 
      iter = 7000, warmup = 3000, chains = 3, 
      control = list(adapt_delta = 0.99),
      thin = 2,
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS")
```

```{r, echo = F}
m2.allFull_fIfS <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_fIfS.rds")

m2.allFull_vIvS <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS.rds")
```

```{r}
summary(m2.allFull_fIfS)
```

The fixed-effect model has good effective sample size (maximum possible: *n* = 12,000) and convergence diagnostics (R-hat = exactly 1). There is certainly variety in the predictor posterior distribution estimates, with some having 95% credible intervals not including 0 and others essentially centered over 0.

```{r}
summary(m2.allFull_vIvS)
```

The varying-effects model is decidedly more complex, with `sd` parameters estimating the variability of partially pooled predictor posteriors (these seem to indicate there is appreciable variability for each), `cor` parameters estimating the correlation of posterior slopes (I think that, due to the `brms` default of using non-centered parametrization, it's unsurprising these are all very plausibly 0), and population-level posteriors which appear to be relatively different from that of the fixed-effects model.

To get a better idea of this last set, let's compare the population-level predictor posteriors between the fixed- and varying-effects models:

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 6}
post_b_2.f <- 
  posterior_summary(m2.allFull_fIfS, pars = "b_", probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(modFX = "fixed",
         par   = rownames(posterior_summary(m2.allFull_fIfS, pars = "b_")))

post_b_2.v <- 
  posterior_summary(m2.allFull_vIvS, pars = "b_", probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(modFX = "varying",
         par   = rownames(posterior_summary(m2.allFull_vIvS, pars = "b_")))

# using this next part to set plot par order
# based on varying-effects model mean estimates
parOrder <- 
  post_b_2.v %>%
  mutate(par = str_remove(par, "b_"),
         par = str_remove(par, "_mn") ) %>%
  arrange(Estimate) %>%
  select(par)

post_b_2 <- 
  post_b_2.f %>%
  bind_rows(post_b_2.v) %>%
  # tidy parameter labels by removing common "b_" and "_mn"
  mutate(par = str_remove(par, "b_"),
         par = str_remove(par, "_mn"),
         par = factor(par,
                      levels = parOrder$par) )

post_b_2 %>%
  ggplot(aes(color = modFX, group = modFX)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_pointrange(aes(x = par, y = Estimate, ymin = Q5, ymax = Q95),
                  position = position_dodge(width = 0.5)) +
  labs(title = "Comparison of m2.allFull models",
       subtitle = "Fixed vs. Varying effects",
       color = NULL,
       x = "Population-level parameter",
       y = "Posterior estimate",
       caption = "Mean point estimate and 90% credible intervals") +
  coord_flip() +
  scale_color_manual(values = color_set8[c(2, 4)]) +
  theme_ds1() + theme(legend.position = "top")
```

While the varying-effects model has wider 90% credible intervals (indicating greater uncertainty in the posterior estimates) for each population-level parameter, it does not have all point estimates shrinking towards zero as I might have expected. Indeed, for parameters whose fixed-effects estimates are further from zero, the corresponding varying-effects estimates are further still from zero - though the two 90% credible intervals largely overlap in all cases.

### Model comparisons - relative IC values/weights, and in-sample prediction performance

**Relative Information Criterion values**

I anticipate that the greater ability of the varying-effects model to "learn from the data" (in the sense of potentially partially pool intercept/slope estimates across multiple, similar districts) will result in relatively superior IC values and, correspondingly, a greater share of IC-allocated model weight.

```{r, eval = F}
(m2.allFull_kFold <- kfold(m2.allFull_fIfS, m2.allFull_vIvS,
                      K = 10, compare = T) )
```

```{r, echo = F, eval = F}
saveRDS(m2.allFull_kFold,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_kFold.rds")
```

```{r, echo = F}
readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_kFold.rds")
```

```{r, eval = F}
(m2.allFull_modWts <- 
   model_weights(m2.allFull_fIfS, m2.allFull_vIvS, weights = "kfold", K = 10) )
```

```{r, echo = F, eval = F}
saveRDS(m2.allFull_modWts,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_modWts.rds")
```

```{r, echo = F}
readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_modWts.rds")
```

*K*-fold cross-validation IC-based weighting assigns a roughly 70/30 split between the fixed-effects and varying-effects models.

### In-sample prediction performance

Let's assess how well `m2.allFull_fIfS` and `m2.allFull_vIvS` predict the number of Democratic candidates elected across the three elections, and how the two models compare (for in-sample data).

```{r, fig.align = "center", fig.width = 8}
# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m2.allFull_fIfS <- posterior_predict(m2.allFull_fIfS)
# mean for each per-year district (column of postPred_m2.allFull_fIfS)
meanPred_m2.allFull_fIfS <- apply(postPred_m2.allFull_fIfS, 2, mean)

predVsObs.2f <- 
  tibble(district2        = houseElectionDat_std_agg$district2,
         generalWinD      = houseElectionDat_std_agg$generalWinD,
         meanPredWinD     = meanPred_m2.allFull_fIfS )

nBins <- 100
cols <- c("darkred", "red",
          "lightgrey",
          "blue", "darkblue")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(50)
cuts <- cut(predVsObs.2f$meanPredWinD, nBins)
names(cuts) <- sapply(cuts,function(t)
                           cut.cols[which(as.character(t) == levels(cuts))]) 

predVsObs.2f %>%
  ggplot(aes(x = meanPredWinD, fill = cut(meanPredWinD, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m2.allFull_fIfS",
       subtitle = "Faceted by # Democratic candidates elected (out of 3 elections: 2012/14/16)",
       x = "Mean count of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m2.allFull_vIvS <- posterior_predict(m2.allFull_vIvS)
# mean for each per-year district (column of postPred_m2.allFull_vIvS)
meanPred_m2.allFull_vIvS <- apply(postPred_m2.allFull_vIvS, 2, mean)

predVsObs.2v <- 
  tibble(district2        = houseElectionDat_std_agg$district2,
         generalWinD      = houseElectionDat_std_agg$generalWinD,
         meanPredWinD     = meanPred_m2.allFull_vIvS )

nBins <- 100
cols <- c("darkred", "red",
          "lightgrey",
          "blue", "darkblue")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(50)
cuts <- cut(predVsObs.2v$meanPredWinD, nBins)
names(cuts) <- sapply(cuts,function(t)
                           cut.cols[which(as.character(t) == levels(cuts))]) 

predVsObs.2v %>%
  ggplot(aes(x = meanPredWinD, fill = cut(meanPredWinD, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m2.allFull_vIvS",
       subtitle = "Faceted by # Democratic candidates elected (out of 3 elections: 2012/14/16)",
       x = "Mean count of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

# comparison of fixed- and varying-effects model predictions
ggplot() +
  geom_density(data = predVsObs.2f,
               aes(x = meanPredWinD), fill = color_set8[5], alpha = 0.7,
               adjust = 0.1) +
  geom_density(data = predVsObs.2v,
               aes(x = meanPredWinD), fill = color_set8[7], alpha = 0.7,
               adjust = 0.1) +
  labs(title = "Comparing posterior prediction densities, 2016 models",
       subtitle = "Fixed (tan) vs Varying effects (orange)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Density") +
  theme_ds1()
```

While there are relatively few observations with Democratic candidates being elected in 1 or 2 out of the three elections (in most cases the districts are either "fully Republican" or "fully Democrat"), the varying-effects model seems to outperform the fixed-effects model in these cases, as well as in the "all or nothing" outcomes which make up the majority.

In order to compare the two models, let's define a "miss" as a model predicting a mean outcome (# of Democratic candidates elected among the 3 elections for each district) that is 0.5 or more away from the observed outcome.

```{r}
predVsObs.2 <- 
  tibble(district2        = houseElectionDat_std_agg$district2,
         generalWinD      = houseElectionDat_std_agg$generalWinD,
         meanPredWinD.f   = meanPred_m2.allFull_fIfS,
         meanPredWinD.v   = meanPred_m2.allFull_vIvS) %>%
  mutate(hitOrMiss.f = if_else((generalWinD == 0 & meanPredWinD.f < 0.5) |
                               (generalWinD == 1 & meanPredWinD.f > 0.5 & 
                                                   meanPredWinD.f < 1.5) |
                               (generalWinD == 2 & meanPredWinD.f > 1.5 & 
                                                   meanPredWinD.f < 2.5) |
                               (generalWinD == 3 & meanPredWinD.f > 2.5 & 
                                                   meanPredWinD.f < 3.5) |
                               (generalWinD == 4 & meanPredWinD.f > 3.5),
                               "hit", "miss"),
         hitOrMiss.v = if_else((generalWinD == 0 & meanPredWinD.v < 0.5) |
                               (generalWinD == 1 & meanPredWinD.v > 0.5 & 
                                                   meanPredWinD.v < 1.5) |
                               (generalWinD == 2 & meanPredWinD.v > 1.5 & 
                                                   meanPredWinD.v < 2.5) |
                               (generalWinD == 3 & meanPredWinD.v > 2.5 & 
                                                   meanPredWinD.v < 3.5) |
                               (generalWinD == 4 & meanPredWinD.v > 3.5),
                               "hit", "miss") )

predVsObs.2 %>%
  group_by(generalWinD) %>%
  summarize(hit.f  = sum(hitOrMiss.f == "hit"),
            miss.f = sum(hitOrMiss.f == "miss"),
            hit.v  = sum(hitOrMiss.v == "hit"),
            miss.v = sum(hitOrMiss.v == "miss") ) %>%
  kable(align = "c") %>%
  kable_styling(full_width = F, bootstrap_options = "striped")
         
```

As expected from the above plots, the varying-effects model "misses" less frequently for each observed outcome count. The fixed-effects model "misses" in 21 cases, while the varying-effects model "misses" in 11 cases - and for each level of `generalWinD`, the varying-effects model has at most half as many "misses" as seen in the fixed-effects case. 

This is counter to what I would have expected based on *K*-fold IC weights.

For either model, the "misses" are disproportionately more likely in cases where a Democratic candidate was actually elected once out of the three possible elections. For three of the four potential outcomes (where a Democratic candidate is elected 0, 1, or 2 times across the three elections), the varying-effects model is less likely to "miss" than the fixed-effects model.

**Therefore, I will go forward with the varying-effects model for further analysis, including considering possible interactions.**

### Sensitivity analysis

As discussed with Dr. Frey, here I consider the extent to which relaxing prior distribution estimates impacts the posterior distribution of the model. The following pair of models use weaker priors which impart less regularization of parameter estimates compared to the prior pair of fixed- and varying-effects models.

```{r, eval = F}
m2.allFull_fIfS2 <- 
  brm(generalWinD | trials(nObs) ~ 1 + incumbentD_mn + incumbentR_mn +
                                   primaryUnopD_mn + primaryUnopR_mn + 
                                   cmpgnMaxDisbursDvsR_mn + primaryTotDvsR_mn + 
                                   log_popDensPerSqMi_mn + medianAgeYr_mn + 
                                   pctEduGradProAge25plus_mn + 
                                   pctInLaborForceUnemp_mn + perCapitaIncome_mn + 
                                   pctWhiteAlone_mn, 
      data = houseElectionDat_std_agg, family = binomial(), 
      prior = c(set_prior("normal(0, 10)", class = "Intercept"),
                set_prior("normal(0, 10)", class = "b")), 
      iter = 7000, warmup = 3000, chains = 3, 
      control = list(adapt_delta = 0.99),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_fIfS2")

m2.allFull_vIvS2 <- 
  brm(generalWinD | trials(nObs) ~ 1 + incumbentD_mn + incumbentR_mn + 
                    primaryUnopD_mn + primaryUnopR_mn + cmpgnMaxDisbursDvsR_mn + 
                    primaryTotDvsR_mn + log_popDensPerSqMi_mn + medianAgeYr_mn + 
                    pctEduGradProAge25plus_mn + pctInLaborForceUnemp_mn + 
                    perCapitaIncome_mn + pctWhiteAlone_mn + 
                (1 + incumbentD_mn + incumbentR_mn + 
                     primaryUnopD_mn + primaryUnopR_mn + cmpgnMaxDisbursDvsR_mn + 
                     primaryTotDvsR_mn + log_popDensPerSqMi_mn + medianAgeYr_mn + 
                     pctEduGradProAge25plus_mn + pctInLaborForceUnemp_mn + 
                     perCapitaIncome_mn + pctWhiteAlone_mn |district2), 
      data = houseElectionDat_std_agg, family = binomial(), 
      prior = c(set_prior("normal(0, 10)", class = "Intercept"),
                set_prior("normal(0, 10)", class = "b"), 
                set_prior("cauchy(0, 10)", class = "sd"),
                set_prior("lkj(1)", class = "cor")), 
      iter = 7000, warmup = 3000, chains = 3, 
      control = list(adapt_delta = 0.99),
      thin = 2,
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2")
```

```{r, echo = F}
m2.allFull_fIfS2 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_fIfS2.rds")

m2.allFull_vIvS2 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2.rds")
```

Let's check the model summaries to review posterior estimates and the R-hat values:

```{r}
summary(m2.allFull_fIfS2)
```

The posterior estimates seem reasonable (no wildly extended ranges), effective sample sizes are fairly large, and the R-hat values suggest convergence has been achieved. Overlaid density plots and trace plots also confirmed good model gits (posterior estimate density plots aligned across each MCMC chain's estimates, and each chain's trace plot line indicated good stationarity over a relatively narrow range of values, and good mixing of chains).

Let's compare posterior distribution estimates for the "more regularized" versus "less regularized" fixed-effects models:

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 6}
post_b_2.f <- 
  posterior_summary(m2.allFull_fIfS, pars = "b_", probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(priors = "more regularized",
         par   = rownames(posterior_summary(m2.allFull_fIfS, pars = "b_")))

post_b_2.f2 <- 
  posterior_summary(m2.allFull_fIfS2, pars = "b_", probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(priors = "less regularized",
         par   = rownames(posterior_summary(m2.allFull_fIfS2, pars = "b_")))

# using this next part to set plot par order
# based on varying-effects model mean estimates
parOrder <- 
  post_b_2.f %>%
  mutate(par = str_remove(par, "b_"),
         par = str_remove(par, "_mn") ) %>%
  arrange(Estimate) %>%
  select(par)

post_b_2 <- 
  post_b_2.f %>%
  bind_rows(post_b_2.f2) %>%
  # tidy parameter labels by removing common "b_" and "_mn"
  mutate(par = str_remove(par, "b_"),
         par = str_remove(par, "_mn"),
         par = factor(par,
                      levels = parOrder$par) )

post_b_2 %>%
  ggplot(aes(color = priors, group = priors)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_pointrange(aes(x = par, y = Estimate, ymin = Q5, ymax = Q95),
                  position = position_dodge(width = 0.5)) +
  labs(title = "Comparison of m2.allFull.f models",
       subtitle = "Fixed effects, more vs. less regularized priors",
       color = NULL,
       x = "Population-level parameter",
       y = "Posterior estimate",
       caption = "Mean point estimate and 90% credible intervals") +
  coord_flip() +
  scale_color_manual(values = color_set8[1:2]) +
  theme_ds1() + theme(legend.position = "top")
```

The only parameter which appears to be much different between the fixed-effects models is that the posterior for `incumbentD` appears greater for the "less regularized" model (though a majority of the 90% credible intervals overlap). Credible intervals are also a bit wider for the "less regularized" model.

Now let's consider the "less regularized" varying-effects model:

```{r}
summary(m2.allFull_vIvS2)
```

The "less regularized" varying-effects model appears to have very similar effective sample sizes, `sd`, and `cor` parameter estimates compared to the "more regularized" model, and again the R-hat values (and posterior density overlay / MCMC chain trace plots) indicate a good model fit.

Let's compare the population-level parameter posterior estimates between the two varying-effects models:

```{r, echo = F, fig.align = "center", fig.width = 8, fig.height = 6}
post_b_2.v <- 
  posterior_summary(m2.allFull_vIvS, pars = "b_", probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(priors = "more regularized",
         par   = rownames(posterior_summary(m2.allFull_vIvS, pars = "b_")))

post_b_2.v2 <- 
  posterior_summary(m2.allFull_vIvS2, pars = "b_", probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(priors = "less regularized",
         par   = rownames(posterior_summary(m2.allFull_vIvS2, pars = "b_")))

# using this next part to set plot par order
# based on varying-effects model mean estimates
parOrder <- 
  post_b_2.v %>%
  mutate(par = str_remove(par, "b_"),
         par = str_remove(par, "_mn") ) %>%
  arrange(Estimate) %>%
  select(par)

post_b_2 <- 
  post_b_2.v %>%
  bind_rows(post_b_2.v2) %>%
  # tidy parameter labels by removing common "b_" and "_mn"
  mutate(par = str_remove(par, "b_"),
         par = str_remove(par, "_mn"),
         par = factor(par,
                      levels = parOrder$par) )

post_b_2 %>%
  ggplot(aes(color = priors, group = priors)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_pointrange(aes(x = par, y = Estimate, ymin = Q5, ymax = Q95),
                  position = position_dodge(width = 0.5)) +
  labs(title = "Comparison of m2.allFull.v models",
       subtitle = "Fixed effects, more vs. less regularized priors",
       color = NULL,
       x = "Population-level parameter",
       y = "Posterior estimate",
       caption = "Mean point estimate and 90% credible intervals") +
  coord_flip() +
  scale_color_manual(values = color_set8[3:4]) +
  theme_ds1() + theme(legend.position = "top")
```

While the fixed-effects models were quite similar between the greater and lesser regularization priors, the varying-effects model with less regularization shows a much greater polarization of posterior estimates (negative estimates and positive estimates extended further in those respective directions), and also much wider uncertainty (with 90% credible intervals that mostly entirely encompass those of the "more regularization" model).

In short, the fixed-effects model does not seem nearly as influenced by choices in prior regularization as the varying-effects model.

### More comparisons among No-interactions models

The next line of code evaluates the effective number of parameters for each model. We'd expect the greatest number for the varying-effects models, since there are varying intercepts and slopes.

While I'd like to use PSIS-LOO for evaluating the effective number of parameters for each model, the varying-effects models take a considerably long time to run, so I'll fall back on WAIC instead.

The plot below summarizes estimates of $p_{WAIC}$, the estimated effective number of parameters, for each model. 

For reference, there are 13 population parameters (Intercept + 13 predictors), and a theoretical maximum of 5,655 group-level effects parameters (13 * 435 [the number of districts]), without considering the between-groups/-effects correlation and standard deviation parameters. In other words, the varying-effects models could be massively overfit.

```{r, fig.align = "center", fig.width = 8, fig.height = 4}
pWAIC_est <- 
  tibble(model = c("m2.allFull_fIfS - more regularized",
                   "m2.allFull_fIfS2 - less regularized",
                   "m2.allFull_vIvS - more regularized",
                   "m2.allFull_vIvS2 - less regularized"),
         pWAIC = c(waic(m2.allFull_fIfS)$estimates["p_waic","Estimate"],
                   waic(m2.allFull_fIfS2)$estimates["p_waic","Estimate"],
                   waic(m2.allFull_vIvS)$estimates["p_waic","Estimate"],
                   waic(m2.allFull_vIvS2)$estimates["p_waic","Estimate"]),
         pW_SE = c(waic(m2.allFull_fIfS)$estimates["p_waic","SE"],
                   waic(m2.allFull_fIfS2)$estimates["p_waic","SE"],
                   waic(m2.allFull_vIvS)$estimates["p_waic","SE"],
                   waic(m2.allFull_vIvS2)$estimates["p_waic","SE"]) )

pWAIC_est %>%
  ggplot(aes(x = model, y = pWAIC, 
             ymin = pWAIC - 2 * pW_SE, ymax = pWAIC + 2 * pW_SE,
             color = model)) +
  geom_pointrange() +
  coord_flip() +
  labs(title = "Estimated effective # parameters \n for no-interaction models",
       subtitle = expression("Point estimate \u00B1 2 standard errors"),
       x = "Model", y = expression("Estimated effective # parameters - "*p[WAIC])) +
  scale_color_manual(values = color_set8[c(2, 1, 4, 3)], guide = F) +
  theme_ds1()
```

This plot seems truly impressive - while the less-regularized varying-effects model `m2.allFull_vIvS2` plausibly has more effective parameters than either of the fixed-effects models, the (approximately) 95% credible intervals largely overlap, and the varying-effects models do not have so many more estimated effective parameters that it is immediately evident there is major overfitting.

Let's compare relative model weights among these four models to better understand the relative estimated deviance in out-of-sample predictions.

```{r, eval = F}
(m2.allFull2_waic <- waic(m2.allFull_fIfS, m2.allFull_fIfS2,
                          m2.allFull_vIvS, m2.allFull_vIvS2,
                          compare = T) )
```


```{r, echo = F, eval = F}
saveRDS(m2.allFull2_waic,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull2_waic.rds")
```

```{r, echo = F}
readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull2_waic.rds")
```

While the more-regularized (original) fixed-effects model, `m2.allFull_vIvS`, has a lower ("better") WAIC value than its less-regularized alternative, the opposite is true for the varying-effects model. There, the less-regularized (revised) model, `m2.allFull_vIvS2` has a much lower WAIC value, indicating it is estimated to have less prediction deviance for out-of-sample data. 

**After discussion with Dr. Frey, seems very, very plausible the "less regularized" model may actually just be overfit - however, because the estimated effective number of parameters are not excessive in any case,  I'll move forward with the model(s) assigned the most WAIC value-based weighting in the next section.**

This newer varying-effects model also outperforms the two fixed-effects models by WAIC value - I suspect this means it will be allocated a majority of WAIC value-based weighting.

```{r, eval = F}
(m2.allFull_modWts2 <- model_weights(m2.allFull_fIfS, m2.allFull_fIfS2,
                                m2.allFull_vIvS, m2.allFull_vIvS2,
                                weights = "waic") )
```


```{r, echo = F, eval = F}
saveRDS(m2.allFull_modWts2,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_modWts2.rds")
```

```{r, echo = F}
readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_modWts2.rds")
```

The "less regularized" varying-effects model is essentially assigned *all* the model weight when compared with the two fixed-effects models and the original varying-effects model. **Combined with the effective number of parameters estimates above, I'll go forward with the less-regularized varying-effects model.**

**Quick comparison between varying-effects models**

It seems clear that the less-strict priors of the revised `m2.allFull_vIvS2` is estimated to result in improved out-of-sample estimates, but how does this relate to in-sample inference?

```{r, fig.align = "center", fig.width = 8}
# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m2.allFull_vIvS <- posterior_predict(m2.allFull_vIvS)
# mean for each per-year district (column of postPred_m2.allFull_vIvS)
meanPred_m2.allFull_vIvS <- apply(postPred_m2.allFull_vIvS, 2, mean)

predVsObs.2v <- 
  tibble(district2        = houseElectionDat_std_agg$district2,
         generalWinD      = houseElectionDat_std_agg$generalWinD,
         meanPredWinD     = meanPred_m2.allFull_vIvS )

nBins <- 100
cols <- c("darkred", "red",
          "lightgrey",
          "blue", "darkblue")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(50)
cuts <- cut(predVsObs.2v$meanPredWinD, nBins)
names(cuts) <- sapply(cuts,function(t)
                           cut.cols[which(as.character(t) == levels(cuts))]) 

predVsObs.2v %>%
  ggplot(aes(x = meanPredWinD, fill = cut(meanPredWinD, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m2.allFull_vIvS",
       subtitle = "Faceted by # Democratic candidates elected (out of 3 elections: 2012/14/16)",
       x = "Mean count of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m2.allFull_vIvS2 <- posterior_predict(m2.allFull_vIvS2)
# mean for each per-year district (column of postPred_m2.allFull_vIvS2)
meanPred_m2.allFull_vIvS2 <- apply(postPred_m2.allFull_vIvS2, 2, mean)

predVsObs.2v2 <- 
  tibble(district2        = houseElectionDat_std_agg$district2,
         generalWinD      = houseElectionDat_std_agg$generalWinD,
         meanPredWinD     = meanPred_m2.allFull_vIvS2 )

nBins <- 100
cols <- c("darkred", "red",
          "lightgrey",
          "blue", "darkblue")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(50)
cuts <- cut(predVsObs.2v2$meanPredWinD, nBins)
names(cuts) <- sapply(cuts,function(t)
                           cut.cols[which(as.character(t) == levels(cuts))]) 

predVsObs.2v2 %>%
  ggplot(aes(x = meanPredWinD, fill = cut(meanPredWinD, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m2.allFull_vIvS2",
       subtitle = "Faceted by # Democratic candidates elected (out of 3 elections: 2012/14/16)",
       x = "Mean count of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

# comparison of fixed- and varying-effects model predictions
ggplot() +
  geom_density(data = predVsObs.2v,
               aes(x = meanPredWinD), fill = color_set8[3], alpha = 0.5,
               adjust = 0.1) +
  geom_density(data = predVsObs.2v2,
               aes(x = meanPredWinD), fill = color_set8[4], alpha = 0.5,
               adjust = 0.1) +
  labs(title = "Comparing posterior prediction densities, varying-effects models",
       subtitle = "More (Lt green) vs Less regularized priors (Dk green)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Density") +
  theme_ds1()
```

The less-regularized priors model `m2.allFullvIvS2` tightens predictions very considerably around the observed count of Democratic candidates elected across the three elections, in each of the four plot panels. 

Just for quantification's sake, let's again compare the two varying-effects models, again using the "miss" metric of a predicted count being 0.5 or more away from the observed count:

```{r, echo = F}
predVsObs.2 <- 
  tibble(district2        = houseElectionDat_std_agg$district2,
         generalWinD      = houseElectionDat_std_agg$generalWinD,
         meanPredWinD.v   = meanPred_m2.allFull_vIvS,
         meanPredWinD.v2  = meanPred_m2.allFull_vIvS2) %>%
  mutate(hitOrMiss.v = if_else((generalWinD == 0 & meanPredWinD.v < 0.5) |
                               (generalWinD == 1 & meanPredWinD.v > 0.5 & 
                                                   meanPredWinD.v < 1.5) |
                               (generalWinD == 2 & meanPredWinD.v > 1.5 & 
                                                   meanPredWinD.v < 2.5) |
                               (generalWinD == 3 & meanPredWinD.v > 2.5 & 
                                                   meanPredWinD.v < 3.5) |
                               (generalWinD == 4 & meanPredWinD.v > 3.5),
                               "hit", "miss"),
         hitOrMiss.v2 = if_else((generalWinD == 0 & meanPredWinD.v2 < 0.5) |
                                (generalWinD == 1 & meanPredWinD.v2 > 0.5 & 
                                                    meanPredWinD.v2 < 1.5) |
                                (generalWinD == 2 & meanPredWinD.v2 > 1.5 & 
                                                    meanPredWinD.v2 < 2.5) |
                                (generalWinD == 3 & meanPredWinD.v2 > 2.5 & 
                                                    meanPredWinD.v2 < 3.5) |
                                (generalWinD == 4 & meanPredWinD.v2 > 3.5),
                                "hit", "miss") )

predVsObs.2 %>%
  group_by(generalWinD) %>%
  summarize(hit.v   = sum(hitOrMiss.v == "hit"),
            miss.v  = sum(hitOrMiss.v == "miss"),
            hit.v2  = sum(hitOrMiss.v2 == "hit"),
            miss.v2 = sum(hitOrMiss.v2 == "miss") ) %>%
  kable(align = "c") %>%
  kable_styling(full_width = F, bootstrap_options = "striped")
```

There isn't a single observation with a model-prediction "miss" of 0.5 or more for the less-regularized varying-effects model. There is every possibility the model is simply overfit, but I'll proceed with the less-regularized varying-effects model on the off chance the model is well fit to the data.

### Considering potential interactions

Moving forward with the "less-regularized priors, varying-effects" model (`m2.allFull_vIvS2`), let's consider potentially incorporating interactions into the model. I will add them one at a time, evaluate their posterior distribution estimates, and consider adding each predictor with preference to those with posterior credible intervals not concentrated about 0.

The following predictors seem to me to be of interest:

1) `cmpgnMaxDisbursDvsR` $\times$ `primaryTotDvsR` - evaluating the interaction between maximum campaign-reported spending differential and primary vote totals differential

2) `perCapitaIncome` $\times$ `pctEduGradProAge25plus` - evaluating the interaction between per capita income and proportion of the age 25+ population with a graduate/professional degree

3) `perCapitaIncome` $\times$ `pctInLaborForceUnemp` - evaluating the interaction between per capita income and proportion of the labor-force population that is unemployed

4) `incumbentD` $\times$ `cmpgnMaxDisbursDvsR`, `incumbentR` $\times$ `cmpgnMaxDisbursDvsR` - evaluating the interaction between party incumbency and maximum reported spending differential

5) `incumbentD` $\times$ `primaryTotDvsR`, `incumbentR` $\times$ `primaryTotDvsR` - evaluating the interaction between party incumbency and total primary votes differential


```{r, eval = F}
m2.allFull_vIvS2_int1 <- 
  update(m2.allFull_vIvS2, . ~ . + cmpgnMaxDisbursDvsR_mn:primaryTotDvsR_mn,
         file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_int1")

m2.allFull_vIvS2_int2 <- 
  update(m2.allFull_vIvS2, . ~ . + perCapitaIncome_mn:pctEduGradProAge25plus_mn,
         file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_int2")

m2.allFull_vIvS2_int3 <- 
  update(m2.allFull_vIvS2, . ~ . + perCapitaIncome_mn:pctInLaborForceUnemp_mn,
         file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_int3")

m2.allFull_vIvS2_int4 <- 
  update(m2.allFull_vIvS2, . ~ . + incumbentD_mn:cmpgnMaxDisbursDvsR_mn +
                                  incumbentR_mn:cmpgnMaxDisbursDvsR_mn,
         file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_int4")

m2.allFull_vIvS2_int5 <- 
  update(m2.allFull_vIvS2, . ~ . + incumbentD_mn:primaryTotDvsR_mn +
                                  incumbentR_mn:primaryTotDvsR_mn,
         file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_int5")
```

```{r, echo = F}
m2.allFull_vIvS2_int1 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_int1.rds")

m2.allFull_vIvS2_int2 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_int2.rds")

m2.allFull_vIvS2_int3 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_int3.rds")

m2.allFull_vIvS2_int4 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_int4.rds")

m2.allFull_vIvS2_int5 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_int5.rds")
```

```{r, echo = F, fig.align = "center", fig.width = 9, fig.height = 4.5}
post_b_2.v_int1 <- 
  posterior_summary(m2.allFull_vIvS2_int1, 
                    pars = "b_cmpgnMaxDisbursDvsR_mn:primaryTotDvsR_mn", 
                    probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(par   = rownames(posterior_summary(m2.allFull_vIvS2_int1, 
                                            pars = "b_cmpgnMaxDisbursDvsR_mn:primaryTotDvsR_mn")))

post_b_2.v_int2 <- 
  posterior_summary(m2.allFull_vIvS2_int2, 
                    pars = "b_pctEduGradProAge25plus_mn:perCapitaIncome_mn", 
                    probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(par   = rownames(posterior_summary(m2.allFull_vIvS2_int2, 
                                            pars = "b_pctEduGradProAge25plus_mn:perCapitaIncome_mn")))

post_b_2.v_int3 <- 
  posterior_summary(m2.allFull_vIvS2_int3, 
                    pars = "b_pctInLaborForceUnemp_mn:perCapitaIncome_mn", 
                    probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(par   = rownames(posterior_summary(m2.allFull_vIvS2_int3, 
                                            pars = "b_pctInLaborForceUnemp_mn:perCapitaIncome_mn")))

post_b_2.v_int4 <- 
  posterior_summary(m2.allFull_vIvS2_int4, 
                    pars = c("b_incumbentD_mn:cmpgnMaxDisbursDvsR_mn",
                             "b_incumbentR_mn:cmpgnMaxDisbursDvsR_mn"), 
                    probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(par   = rownames(posterior_summary(m2.allFull_vIvS2_int4, 
                                            pars = c("b_incumbentD_mn:cmpgnMaxDisbursDvsR_mn",
                                                     "b_incumbentR_mn:cmpgnMaxDisbursDvsR_mn"))))

post_b_2.v_int5 <- 
  posterior_summary(m2.allFull_vIvS2_int5, 
                    pars = c("b_incumbentD_mn:primaryTotDvsR_mn",
                             "b_incumbentR_mn:primaryTotDvsR_mn"), 
                    probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(par   = rownames(posterior_summary(m2.allFull_vIvS2_int5, 
                                            pars = c("b_incumbentD_mn:primaryTotDvsR_mn",
                                                     "b_incumbentR_mn:primaryTotDvsR_mn"))))

post_b_2.v_int <- 
  post_b_2.v_int1 %>%
  bind_rows(post_b_2.v_int2, post_b_2.v_int3, post_b_2.v_int4, post_b_2.v_int5) %>%
  # tidy parameter labels by removing common "b_" and "_mn"
  mutate(par = str_remove_all(par, "b_"),
         par = str_remove_all(par, "_mn") )

post_b_2.v_int %>%
  ggplot() +
  geom_hline(yintercept = 0, color = "gray") +
  geom_pointrange(aes(x = par, y = Estimate, ymin = Q5, ymax = Q95),
                  color = color_set8[3]) +
  labs(title = "Comparison of m2.allFull.v2_int models' interactions",
       subtitle = "Note: Only the interaction term for each model is considered",
       x = "Population-level (interaction) parameter",
       y = "Posterior estimate",
       caption = "Mean point estimate and 90% credible intervals") +
  coord_flip() +
  theme_ds1()
```

While most of the interactions considered appear to very plausibly have no marginal predicted association with the election outcome, the top two interactions *do* have relatively narrow credible intervals, and the first seems to quite plausibly have a nonzero marginal effect.

Therefore, I will retain the first interaction model, and then compare this interaction model against `m2.allFull_vIvS2`.

```{r, echo = F}
rm(m2.allFull_fIfS, m2.allFull_fIfS2, m2.allFull_vIvS2_int2,
   m2.allFull_vIvS2_int3, m2.allFull_vIvS2_int4, m2.allFull_vIvS2_int5,
   post_b_2, post_b_2.f, post_b_2.f2, post_b_2.v, post_b_2.v_int, post_b_2.v_int1,
   post_b_2.v_int2, post_b_2.v_int3, post_b_2.v_int4, post_b_2.v_int5)
```

```{r, eval = F}
(m2.allFull_vIvS2_modWts <- model_weights(m2.allFull_vIvS2, 
                                          m2.allFull_vIvS2_int1,
                                weights = "waic") )
```


```{r, echo = F, eval = F}
saveRDS(m2.allFull_vIvS2_modWts,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_modWts.rds")
```

```{r, echo = F}
m2.allFull_vIvS2_modWts <- 
  readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_modWts.rds")

m2.allFull_vIvS2_modWts
```

Comparing this latest model against the less-regularized varying-effects model, the newer model featuring an interaction is "assigned" roughly 73% of the relative model weight based on WAIC values.

I take the relative weights being shared to such an extent between the two models as evidence that relying entirely on the model containing the interaction effectively places too much weight on the marginal contribution of the interaction. 

To avoid this overconfidence, I will generate the "final" model using model-weighted averaging between these two models, with relative weights equal to what we see above (about a 25/75 split). This will pull posterior estimates to approximately the middle of the posterior distribution of the non-interaction model and that of the interaction-including model for each parameter.

Let's first check the summary of the new interaction model.

```{r}
summary(m2.allFull_vIvS2_int1)
```

While model fits (based on effective sample sizes and R-hat values) look good, parameter estimates don't seem to have changed much with the interaction model compared to the non-interaction `m2.allFull_vIvS2`, especially for the `sd` and `b_` parameters.

For example, here are the "population effect" `b_` parameters for each model. The interaction effect is fixed at zero for the non-interaction model:

```{r, echo = F, warning = F, fig.align = "center", fig.width = 8.5, fig.height = 6}
post_b_2.v2 <- 
  posterior_summary(m2.allFull_vIvS2, pars = "b_",
                    probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(par   = rownames(posterior_summary(m2.allFull_vIvS2, 
                                            pars = "b_")),
         model = "m2.allFull_vIvS2") %>%
  # add "empty" row for interaction parameter so it'll show in the plot
  bind_rows(tibble(Estimate  = 0,
                   Est.Error = 0,
                   Q5        = 0,
                   Q95       = 0,
                   par       = "b_cmpgnMaxDisbursDvsR_mn:primaryTotDvsR_mn",
                   model     = "m2.allFull_vIvS2"))

post_b_2.v2_int1 <- 
  posterior_summary(m2.allFull_vIvS2_int1, pars = "b_",
                    probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(par   = rownames(posterior_summary(m2.allFull_vIvS2_int1, 
                                            pars = "b_")),
         model = "m2.allFull_vIvS2_int1")

post_b_2.v2 <- 
  post_b_2.v2 %>%
  bind_rows(post_b_2.v2_int1) %>%
  # tidy parameter labels by removing common "b_" and "_mn"
  mutate(par = str_remove_all(par, "b_"),
         par = str_remove_all(par, "_mn") )

post_b_2.v2 %>%
  ggplot() +
  geom_hline(yintercept = 0, color = "gray") +
  geom_pointrange(aes(x = par, y = Estimate, ymin = Q5, ymax = Q95,
                  color = model, group = model),
                  position = position_dodge(width = 0.8)) +
  labs(title = "Comparison of m2.allFull.v2 & m2.allFull.v2_int1 \n population parameters",
       x = "Population-level parameter",
       y = "Posterior estimate",
       caption = "Mean point estimate and 90% credible intervals") +
  coord_flip() +
  scale_color_manual(values = color_set8[c(3, 6)]) +
  theme_ds1() + theme(legend.position = "top")

```

If the interaction effect really does have a non-zero association with the outcome `generalWinD`, it appears to be very dependent on the particular district(s) involved, as the population-level effect is very plausibly zero. The other population-level effects do not appear greatly changed.

Let's check the estimated number effective number of parameters for each model, to ensure adding the interaction effect doesn't induce any worrying increase in overfitting:

```{r, fig.align = "center", fig.width = 8, fig.height = 3}
pWAIC_est <- 
  tibble(model = c("m2.allFull_vIvS2",
                   "m2.allFull_vIvS2_int1"),
         pWAIC = c(waic(m2.allFull_vIvS2)$estimates["p_waic","Estimate"],
                   waic(m2.allFull_vIvS2_int1)$estimates["p_waic","Estimate"]),
         pW_SE = c(waic(m2.allFull_vIvS2)$estimates["p_waic","SE"],
                   waic(m2.allFull_vIvS2_int1)$estimates["p_waic","SE"]) )

pWAIC_est %>%
  ggplot(aes(x = model, y = pWAIC, 
             ymin = pWAIC - 2 * pW_SE, ymax = pWAIC + 2 * pW_SE,
             color = model)) +
  geom_pointrange() +
  coord_flip() +
  labs(title = "Estimated effective # parameters",
       subtitle = expression("Point estimate \u00B1 2 standard errors"),
       x = "Model", y = expression("Estimated effective # parameters - "*p[WAIC])) +
  scale_color_manual(values = color_set8[c(3, 6)], guide = F) +
  theme_ds1()
```

Looks good! There isn't an appreciable increase in the estimated effective number of parameters after adding the interaction.

Now, let's see how model-weighted averaging influences (population) posterior estimates (applicable only to parameters shared between the two models). 

**I believe model-weighted averaging is removing the interaction effect due to the non-interaction model not including that term, rather than proportionally down-weighting it towards zero (which I would have hoped for) - the original version of the plot below (and related data checking) indicates it isn't present. In order to include this parameter, I refit the "no interaction" model with the interaction term having an extremely regularizing prior (Normal(0, 0.00001)) in the hopes of including the term in model-weighted averaging while still minimalizing the term in the "no-interaction" model - probably *not* the wisest course of action here, but c'est la vie.**

```{r, message = F, warning = F, fig.align = "center", fig.width = 9.5, fig.height = 7}
# fitting a version of the "no interaction" model with an extremely skeptical
# prior on the interaction effect, to include it in mod-wtd-averaging
m2.allFull_vIvS2_v2 <- 
  update(m2.allFull_vIvS2, . ~ . + cmpgnMaxDisbursDvsR_mn:primaryTotDvsR_mn,
         # set very strict prior to effectively assign 0 to the interaction
         prior = c(set_prior("normal(0, 0.00001)", class = "b", 
                           coef = "cmpgnMaxDisbursDvsR_mn:primaryTotDvsR_mn") ),
         file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m2.allFull_vIvS2_v2")
# note: WAIC-based model weights comes down to roughly 
# 25% "no-interaction model" / 75% "interaction model", which is very very close
# to the original 23% "no-interaction" / 77% "interaction"
# so just sticking with the previously-run weighting

m2.final_vIvS2 <- 
  posterior_average(m2.allFull_vIvS2_v2, m2.allFull_vIvS2_int1,
                    weights = m2.allFull_vIvS2_modWts)

post_b_modWtAvg <- 
posterior_summary(m2.final_vIvS2,
                  probs = c(0.05, 0.95))[grepl("b_", 
                              rownames(posterior_summary(m2.final_vIvS2))),] %>%
  as_tibble() %>%
  mutate(par = rownames(posterior_summary(m2.final_vIvS2)[grepl("b_", 
                             rownames(posterior_summary(m2.final_vIvS2))),]),
         model = "model-weighted average" ) %>%
  mutate(par = str_remove_all(par, "b_"),
         par = str_remove_all(par, "_mn") )

post_b_2.v2 %>%
  bind_rows(post_b_modWtAvg) %>%
  ggplot() +
  geom_hline(yintercept = 0, color = "gray") +
  geom_pointrange(aes(x = par, y = Estimate, ymin = Q5, ymax = Q95,
                  color = model, group = model),
                  position = position_dodge(width = 0.8)) +
  labs(title = "Comparison of population parameters, \n individual models and model-weighted averages",
       x = "Population-level parameter",
       y = "Posterior estimate", color = NULL,
       caption = "Mean point estimate and 90% credible intervals") +
  coord_flip() +
  scale_color_manual(values = color_set8[c(3, 6, 7)]) +
  theme_ds1() + theme(legend.position = "top")
```

As expected, since the "with interaction" model doesn't drastically change the population-level effects, the model-weighted average estimates have point estimates between that of the two models, and credible intervals are quite comparable to that of the other two models.

Now, let's consider counterfactual analysis (model-implied prediction for theoretical values of one predictor, holding other values constant at some level), and then look at some district-level predictors for the model-weight average "model".

### Counterfactual analysis 1 - incumbency

In order to better understand how the model behaves for changes to a given predictor, it seems reasonable to choose a few predictors over which we'll observe model-predicted counts of democratic candidates elected (out of three elections), while holding all predictors constant except for one.

**I'm not certain , but it appears that Bayesian Model Averaging is not directly possible in this case - I'm using only the interaction-including model here.**

This first counterfactual plot imagines each district somehow had an incumbent from the Democratic party (left column), the Republican party (right column), or neither party (middle column) for each of the three elections, and then predicts how many elections of the three would elect a Democratic candidate - the alternative is electing a Republican candidate (since no observed data involved an Other-party election). The mean and 2 standard deviations of each district's predictions are plotted, with lower and upper bounds at 0 and 3, respectively.

This plot is further organized into how many Democratic candidates actually *were* elected as the rows, and observations are color-coded by the true number of incumbents from each party across the three elections.

*Note: Because post-2010 redistricting moved some incumbents into new districts, there were some cases of multi-incumbent districts.*

```{r, fig.align = "center", fig.width = 10, fig.height = 10}
# create new data frames for each counterfactual plot
# note that incumbency is set to D / None / R for plot
# all other predictors (save one) set to the mean value
houseElectionDat_std_agg <- 
  houseElectionDat_std_agg %>%
  mutate(cmpgnMaxDisbursXprimaryTotDvsR = 
           cmpgnMaxDisbursDvsR_mn * primaryTotDvsR_mn)

# imagining all districts somehow had a Democratic party incumbent each time
newDat_incumbentD <- 
  tibble(district2     = houseElectionDat_std_agg$district2,
         nObs          = rep(3, 435),
         incumbentD_mn = rep(1, 435),
         incumbentR_mn = rep(0, 435),
         primaryUnopD_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopD_mn), 435),
         primaryUnopR_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopR_mn), 435),
         cmpgnMaxDisbursDvsR_mn =
           rep(mean(houseElectionDat_std_agg$cmpgnMaxDisbursDvsR_mn), 435),
         primaryTotDvsR_mn  =
           rep(mean(houseElectionDat_std_agg$primaryTotDvsR_mn), 435),
         log_popDensPerSqMi_mn =
           rep(mean(houseElectionDat_std_agg$log_popDensPerSqMi_mn), 435),
         medianAgeYr_mn     =
           rep(mean(houseElectionDat_std_agg$medianAgeYr_mn), 435),
         pctEduGradProAge25plus_mn =
           rep(mean(houseElectionDat_std_agg$pctEduGradProAge25plus_mn), 435),
         pctInLaborForceUnemp_mn =
           rep(mean(houseElectionDat_std_agg$pctInLaborForceUnemp_mn), 435),
         perCapitaIncome_mn =
           rep(mean(houseElectionDat_std_agg$perCapitaIncome_mn), 435),
         pctWhiteAlone_mn   =
           rep(mean(houseElectionDat_std_agg$pctWhiteAlone_mn), 435) )

# imagining all districts somehow had a Republican party incumbent each time 
newDat_incumbentR <- 
  tibble(district2     = houseElectionDat_std_agg$district2,
         nObs          = rep(3, 435),
         incumbentD_mn = rep(0, 435),
         incumbentR_mn = rep(1, 435),
         primaryUnopD_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopD_mn), 435),
         primaryUnopR_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopR_mn), 435),
         cmpgnMaxDisbursDvsR_mn =
           rep(mean(houseElectionDat_std_agg$cmpgnMaxDisbursDvsR_mn), 435),
         primaryTotDvsR_mn  =
           rep(mean(houseElectionDat_std_agg$primaryTotDvsR_mn), 435),
         log_popDensPerSqMi_mn =
           rep(mean(houseElectionDat_std_agg$log_popDensPerSqMi_mn), 435),
         medianAgeYr_mn     =
           rep(mean(houseElectionDat_std_agg$medianAgeYr_mn), 435),
         pctEduGradProAge25plus_mn =
           rep(mean(houseElectionDat_std_agg$pctEduGradProAge25plus_mn), 435),
         pctInLaborForceUnemp_mn =
           rep(mean(houseElectionDat_std_agg$pctInLaborForceUnemp_mn), 435),
         perCapitaIncome_mn =
           rep(mean(houseElectionDat_std_agg$perCapitaIncome_mn), 435),
         pctWhiteAlone_mn   =
           rep(mean(houseElectionDat_std_agg$pctWhiteAlone_mn), 435) )

# imagining all districts somehow had neither a Democratic party incumbent 
# nor a Republican party incumbent each time 
newDat_incumbentN <- 
  tibble(district2     = houseElectionDat_std_agg$district2,
         nObs          = rep(3, 435),
         incumbentD_mn = rep(0, 435),
         incumbentR_mn = rep(0, 435),
         primaryUnopD_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopD_mn), 435),
         primaryUnopR_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopR_mn), 435),
         cmpgnMaxDisbursDvsR_mn =
           rep(mean(houseElectionDat_std_agg$cmpgnMaxDisbursDvsR_mn), 435),
         primaryTotDvsR_mn  =
           rep(mean(houseElectionDat_std_agg$primaryTotDvsR_mn), 435),
         log_popDensPerSqMi_mn =
           rep(mean(houseElectionDat_std_agg$log_popDensPerSqMi_mn), 435),
         medianAgeYr_mn     =
           rep(mean(houseElectionDat_std_agg$medianAgeYr_mn), 435),
         pctEduGradProAge25plus_mn =
           rep(mean(houseElectionDat_std_agg$pctEduGradProAge25plus_mn), 435),
         pctInLaborForceUnemp_mn =
           rep(mean(houseElectionDat_std_agg$pctInLaborForceUnemp_mn), 435),
         perCapitaIncome_mn =
           rep(mean(houseElectionDat_std_agg$perCapitaIncome_mn), 435),
         pctWhiteAlone_mn   =
           rep(mean(houseElectionDat_std_agg$pctWhiteAlone_mn), 435) )

predCount_incD <- posterior_predict(m2.allFull_vIvS2_int1,
                  newdata = newDat_incumbentD)

predCount_incR <- posterior_predict(m2.allFull_vIvS2_int1,
                  newdata = newDat_incumbentR)

predCount_incN <- posterior_predict(m2.allFull_vIvS2_int1,
                  newdata = newDat_incumbentN)

# need to take the mean of each column for estimate, then SD
predCount_Dat <- 
  tibble(district2       = houseElectionDat_std_agg$district2,
         generalWinD     = houseElectionDat_std_agg$generalWinD,
         incumbentD      = houseElectionDat_std_agg$incumbentD_mn,
         incumbentR      = houseElectionDat_std_agg$incumbentR_mn,
         predCt_incD     = apply(predCount_incD, 2, mean),
         predSD_incD     = apply(predCount_incD, 2, sd),
         predCt_incR     = apply(predCount_incR, 2, mean),
         predSD_incR     = apply(predCount_incR, 2, sd),
         predCt_incN     = apply(predCount_incN, 2, mean),
         predSD_incN     = apply(predCount_incN, 2, sd) )

# see SO user hadley's (accepted) answer
# https://stackoverflow.com/questions/25925556/gather-multiple-sets-of-columns
predCount_Dat <- 
  predCount_Dat %>%
  gather(key, value, -district2, -generalWinD, -incumbentR, -incumbentD) %>%
  extract(key, into = c("metric", "incumbentParty"),
          # ^ = start of string, 
          # ([:alpha:]*) = first group; match upper/lower alphabetic characters
          #                match to end of group
          # \\_ = literal underscore; separates first and second group here
          # ([:alpha:]*) = second group; match upper/lower alphabetic characters
          #                match to end of group
          regex = "^([:alpha:]*)\\_([:alpha:]*)") %>%
  spread(metric, value) %>%
  mutate(incumbentParty = str_remove(incumbentParty, "inc"),
         # maximum of predCt - 2 SDs and 0
         predMin        = if_else(predCt - 2*predSD < 0, 0, predCt - 2*predSD),
         # minimum of predCt + 2 SDs and 3
         predMax        = if_else(predCt + 2*predSD > 3, 3, predCt + 2*predSD),
         incFactor      = factor(
                          if_else(incumbentR*3 == 4, "4R",
                          if_else(incumbentR*3 == 3 & incumbentD*3 == 0, "3R",
                          if_else(incumbentR*3 == 3 & incumbentD*3 == 1, "3R 1D",
                          if_else(incumbentR*3 == 2 & incumbentD*3 == 0, "2R",
                          if_else(incumbentR*3 == 2 & incumbentD*3 == 1, "2R 1D",
                          if_else(incumbentR*3 == 1 & incumbentD*3 == 0, "1R",
                          if_else(incumbentR*3 == 0 & incumbentD*3 == 0, "None",
                          if_else(incumbentR*3 == 1 & incumbentD*3 == 1, "1D 1R",
                          if_else(incumbentR*3 == 2 & incumbentD*3 == 2, "2R 2D",
                          if_else(incumbentR*3 == 0 & incumbentD*3 == 1, "1D",
                          if_else(incumbentR*3 == 1 & incumbentD*3 == 2, "2D 1R",
                          if_else(incumbentR*3 == 0 & incumbentD*3 == 2, "2D",
                          if_else(incumbentR*3 == 1 & incumbentD*3 == 3, "3D 1R",
                          if_else(incumbentR*3 == 0 & incumbentD*3 == 3, "3D",
                          if_else(incumbentD*3 == 4, "4D", "CHECK"))))))))))))))),
                          levels = c("4R", "3R", "3R 1D", "2R", "2R 1D", "1R",
                                     "None", "1D 1R", "2R 2D", "1D", "2D 1R",
                                    "2D", "3D 1R", "3D", "4D", "CHECK"),
                          ordered = T) )

library(RColorBrewer)
# take the first -6- reds; last two are too light to be distinguishing 
plotReds  <- rev(brewer.pal(8, name = "Reds")) # original order is light to dark
# take the last two values, and use a grey for "none"
plotPurps <- brewer.pal(4, name = "Purples") 
# opposite of plotReds, take the last -6- blues
plotBlues <- brewer.pal(8, name = "Blues")

# note: plot will use all 6 Reds, no Grey (for "None"), 
#       2 Purples (for equal # D/R), and all 5 of 6 Blues (no 4th / #6)
plotCols <- c(plotReds[1:6], plotPurps[3:4], plotBlues[c(3:5,7,8)])

predCount_Dat %>%
  ggplot(aes(x = district2, y = predCt, ymin = predMin, ymax = predMax,
             color = incFactor)) +
  geom_pointrange() +
  facet_wrap(generalWinD ~ incumbentParty, ncol = 3) +
  scale_color_manual(values = plotCols) +
  #scale_color_viridis_d() +
  labs(title = "Counterfactual plot considering 'all-[party] incumbents'; Pt Est \u00B1 2 SDs ",
       subtitle = "Numbers: Actual # of Democratic candidates elected across the three elections \n Letters: Dem / No / Rep incumbent assigned for all 3 elections \n Colors: Actual # incumbents by party across the three elections",
       x = NULL, y = "Predicted # Democratic candidates elected", color = NULL,
       caption = "Due to redistricting in 2012, some districts had >3 incumbents") +
  theme_ds1() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```

This is a very potentially-confusing plot, and I'll do my best to discuss what I *think* it is showing.

**Across each row** the same districts are plotted with different counterfactual data assigned - the sample-wide mean of all predictors except for incumbency, where each district was manually assigned to have either a Democratic incumbent (left column), no incumbent (middle), or a Republican incumbent (right) heading into each election.

**Within each column**, districts are divided based on how many times in the three elections a Democratic candidate was actually elected (0 - 3).

This plot seems to show that the model considers incumbency for one party or the other is a very strongly associated predictor with the actual count of that party's candidates being elected, which seems sensible. There appears to be greater relative weight toward Democratic candidate incumbency, as the "$\pm 2 ~\text{SD}$" prediction ranges are wider for the "assigned all-R incumbents" cells per row than for their "assigned all-D incumbents" counterparts. There is considerable uncertainty when *no* incumbents are assigned across the elections, and most districts seem to have a point estimate of 1.8 or so Democratic candidates being elected.

Also, there is evidence that districts are inferred to have different underlying proclivities for electing a given party's candidates - in the middle column it is apparent that point estimates differ somewhat, while in the left and right columns one can see the ranges differ.

### Counterfactual analysis 2 - difference in maximum campaign-reported spend

*Note:* In the "Summary table of pre-standardization values shown earlier, each year's election's mean difference in Democratic-minus-Republican maximum spend has Republicans with a roughly \$210-240,000 spending advantage, with a standard deviation of \$1.7-1.9 million.

This counterfactual plot also assigns non-incumbency predictors to sample-wide mean values, and considers predicted # of Democratic candidates elected (again, the alternative is the Republican candidate being elected) with the manipulated variable being the number of standard deviations from the population mean for difference in campaign-reported spend for the Democratic party candidate vs. the Republican party candidate. **Incumbency counts are allowed to remain at the actual district-level values**. And once more, rows indicate the observed number of Democratic candidates elected across the three elections.

```{r, fig.align = "center", fig.width = 10, fig.height = 10}
# create new data frames for each counterfactual plot
# note that cmpgnMaxDisbursDvsR_mn is set to -1SD / Mean / +1SD for plot
# all other predictors (save one) set to the mean value

# imagining all districts somehow had a Democratic party incumbent each time
newDat_disbMinus1 <- 
  tibble(district2     = houseElectionDat_std_agg$district2,
         nObs          = rep(3, 435),
         incumbentD_mn = houseElectionDat_std_agg$incumbentD_mn,
         incumbentR_mn = houseElectionDat_std_agg$incumbentR_mn,
         primaryUnopD_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopD_mn), 435),
         primaryUnopR_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopR_mn), 435),
         cmpgnMaxDisbursDvsR_mn =
           rep(-1, 435),
         primaryTotDvsR_mn  =
           rep(mean(houseElectionDat_std_agg$primaryTotDvsR_mn), 435),
         log_popDensPerSqMi_mn =
           rep(mean(houseElectionDat_std_agg$log_popDensPerSqMi_mn), 435),
         medianAgeYr_mn     =
           rep(mean(houseElectionDat_std_agg$medianAgeYr_mn), 435),
         pctEduGradProAge25plus_mn =
           rep(mean(houseElectionDat_std_agg$pctEduGradProAge25plus_mn), 435),
         pctInLaborForceUnemp_mn =
           rep(mean(houseElectionDat_std_agg$pctInLaborForceUnemp_mn), 435),
         perCapitaIncome_mn =
           rep(mean(houseElectionDat_std_agg$perCapitaIncome_mn), 435),
         pctWhiteAlone_mn   =
           rep(mean(houseElectionDat_std_agg$pctWhiteAlone_mn), 435) )

# imagining all districts somehow had a Republican party incumbent each time 
newDat_disbMean <- 
  tibble(district2     = houseElectionDat_std_agg$district2,
         nObs          = rep(3, 435),
         incumbentD_mn = houseElectionDat_std_agg$incumbentD_mn,
         incumbentR_mn = houseElectionDat_std_agg$incumbentR_mn,
         primaryUnopD_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopD_mn), 435),
         primaryUnopR_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopR_mn), 435),
         cmpgnMaxDisbursDvsR_mn =
           rep(mean(houseElectionDat_std_agg$cmpgnMaxDisbursDvsR_mn), 435),
         primaryTotDvsR_mn  =
           rep(mean(houseElectionDat_std_agg$primaryTotDvsR_mn), 435),
         log_popDensPerSqMi_mn =
           rep(mean(houseElectionDat_std_agg$log_popDensPerSqMi_mn), 435),
         medianAgeYr_mn     =
           rep(mean(houseElectionDat_std_agg$medianAgeYr_mn), 435),
         pctEduGradProAge25plus_mn =
           rep(mean(houseElectionDat_std_agg$pctEduGradProAge25plus_mn), 435),
         pctInLaborForceUnemp_mn =
           rep(mean(houseElectionDat_std_agg$pctInLaborForceUnemp_mn), 435),
         perCapitaIncome_mn =
           rep(mean(houseElectionDat_std_agg$perCapitaIncome_mn), 435),
         pctWhiteAlone_mn   =
           rep(mean(houseElectionDat_std_agg$pctWhiteAlone_mn), 435) )

# imagining all districts somehow had neither a Democratic party incumbent 
# nor a Republican party incumbent each time 
newDat_disbPlus1 <- 
  tibble(district2     = houseElectionDat_std_agg$district2,
         nObs          = rep(3, 435),
         incumbentD_mn = houseElectionDat_std_agg$incumbentD_mn,
         incumbentR_mn = houseElectionDat_std_agg$incumbentR_mn,
         primaryUnopD_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopD_mn), 435),
         primaryUnopR_mn    = 
           rep(mean(houseElectionDat_std_agg$primaryUnopR_mn), 435),
         cmpgnMaxDisbursDvsR_mn =
           rep(1, 435),
         primaryTotDvsR_mn  =
           rep(mean(houseElectionDat_std_agg$primaryTotDvsR_mn), 435),
         log_popDensPerSqMi_mn =
           rep(mean(houseElectionDat_std_agg$log_popDensPerSqMi_mn), 435),
         medianAgeYr_mn     =
           rep(mean(houseElectionDat_std_agg$medianAgeYr_mn), 435),
         pctEduGradProAge25plus_mn =
           rep(mean(houseElectionDat_std_agg$pctEduGradProAge25plus_mn), 435),
         pctInLaborForceUnemp_mn =
           rep(mean(houseElectionDat_std_agg$pctInLaborForceUnemp_mn), 435),
         perCapitaIncome_mn =
           rep(mean(houseElectionDat_std_agg$perCapitaIncome_mn), 435),
         pctWhiteAlone_mn   =
           rep(mean(houseElectionDat_std_agg$pctWhiteAlone_mn), 435) )

predCount_disbMinus1 <- posterior_predict(m2.allFull_vIvS2_int1,
                  newdata = newDat_disbMinus1)

predCount_disbMean <- posterior_predict(m2.allFull_vIvS2_int1,
                  newdata = newDat_disbMean)

predCount_disbPlus1 <- posterior_predict(m2.allFull_vIvS2_int1,
                  newdata = newDat_disbPlus1)

# need to take the mean of each column for estimate, then SD
predCount_Dat <- 
  tibble(district2           = houseElectionDat_std_agg$district2,
         generalWinD         = houseElectionDat_std_agg$generalWinD,
         incumbentD          = houseElectionDat_std_agg$incumbentD_mn,
         incumbentR          = houseElectionDat_std_agg$incumbentR_mn,
         predCt_disbMinus1SD = apply(predCount_disbMinus1, 2, mean),
         predSD_disbMinus1SD = apply(predCount_disbMinus1, 2, sd),
         predCt_disbMean     = apply(predCount_disbMean, 2, mean),
         predSD_disbMean     = apply(predCount_disbMean, 2, sd),
         predCt_disbPlus1SD  = apply(predCount_disbPlus1, 2, mean),
         predSD_disbPlus1SD  = apply(predCount_disbPlus1, 2, sd) )

# see SO user hadley's (accepted) answer
# https://stackoverflow.com/questions/25925556/gather-multiple-sets-of-columns
predCount_Dat <- 
  predCount_Dat %>%
  gather(key, value, -district2, -generalWinD, -incumbentR, -incumbentD) %>%
  extract(key, into = c("metric", "cmpgnMaxDisbursDvsR"),
          # ^ = start of string, 
          # ([:alpha:]*) = first group; match upper/lower alphabetic characters
          #                match to end of group
          # \\_ = literal underscore; separates first and second group here
          # ([a-zA-Z0-9]*) = second group; match upper/lower alphanumeric characters
          #                  match to end of group
          regex = "^([:alpha:]*)\\_([a-zA-Z0-9]*)") %>%
  spread(metric, value) %>%
  mutate(cmpgnMaxDisbursDvsR = factor(cmpgnMaxDisbursDvsR, 
                                      levels = c("disbMinus1SD",
                                                 "disbMean",
                                                 "disbPlus1SD")),
    # maximum of predCt - 2 SDs and 0
         predMin        = if_else(predCt - 2*predSD < 0, 0, predCt - 2*predSD),
         # minimum of predCt + 2 SDs and 3
         predMax        = if_else(predCt + 2*predSD > 3, 3, predCt + 2*predSD),
         incFactor      = factor(
                          if_else(incumbentR*3 == 4, "4R",
                          if_else(incumbentR*3 == 3 & incumbentD*3 == 0, "3R",
                          if_else(incumbentR*3 == 3 & incumbentD*3 == 1, "3R 1D",
                          if_else(incumbentR*3 == 2 & incumbentD*3 == 0, "2R",
                          if_else(incumbentR*3 == 2 & incumbentD*3 == 1, "2R 1D",
                          if_else(incumbentR*3 == 1 & incumbentD*3 == 0, "1R",
                          if_else(incumbentR*3 == 0 & incumbentD*3 == 0, "None",
                          if_else(incumbentR*3 == 1 & incumbentD*3 == 1, "1D 1R",
                          if_else(incumbentR*3 == 2 & incumbentD*3 == 2, "2R 2D",
                          if_else(incumbentR*3 == 0 & incumbentD*3 == 1, "1D",
                          if_else(incumbentR*3 == 1 & incumbentD*3 == 2, "2D 1R",
                          if_else(incumbentR*3 == 0 & incumbentD*3 == 2, "2D",
                          if_else(incumbentR*3 == 1 & incumbentD*3 == 3, "3D 1R",
                          if_else(incumbentR*3 == 0 & incumbentD*3 == 3, "3D",
                          if_else(incumbentD*3 == 4, "4D", "CHECK"))))))))))))))),
                          levels = c("4R", "3R", "3R 1D", "2R", "2R 1D", "1R",
                                     "None", "1D 1R", "2R 2D", "1D", "2D 1R",
                                    "2D", "3D 1R", "3D", "4D", "CHECK"),
                          ordered = T) )

library(RColorBrewer)
# take the first -6- reds; last two are too light to be distinguishing 
plotReds  <- rev(brewer.pal(8, name = "Reds")) # original order is light to dark
# take the last two values, and use a grey for "none"
plotPurps <- brewer.pal(4, name = "Purples") 
# opposite of plotReds, take the last -6- blues
plotBlues <- brewer.pal(8, name = "Blues")

# note: plot will use all 6 Reds, no Grey (for "None"), 
#       2 Purples (for equal # D/R), and all 5 of 6 Blues (no 4th / #6)
plotCols <- c(plotReds[1:6], plotPurps[3:4], plotBlues[c(3:5,7,8)])

predCount_Dat %>%
  ggplot(aes(x = district2, y = predCt, ymin = predMin, ymax = predMax,
             color = incFactor)) +
  geom_pointrange() +
  facet_wrap(generalWinD ~ cmpgnMaxDisbursDvsR, ncol = 3) +
  scale_color_manual(values = plotCols) +
  #scale_color_viridis_d() +
  labs(title = "Counterfactual plot considering 'campaignMaxDisbursDvsR'; Pt Est \u00B1 2 SDs ",
       subtitle = "Numbers: Actual # of Democratic candidates elected across the three elections \n Groups: -1SD DvsR spend level / Predictor mean / +1SD assigned for all 3 elections \n Colors: # incumbents by party across the three elections",
       x = NULL, y = "Predicted # Democratic candidates elected", color = NULL,
       caption = "Due to redistricting in 2012, some districts had >3 incumbents") +
  theme_ds1() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
```

This counterfactual plot suggests that, based on the model, spend differential (maximum reported $ spent for a Democratic campaign minus the maximum spent for a Republican campaign) makes a sizable difference in the predicted number of candidates elected for a party. This is especially pronounced/fluid for districts with less partisan lean in incumbents. 

At least two things should be noted here:

1) Because the predictor, `cmpgnMaxDisbursDvsR_mn`, is *not* centered at 0, the -1 SD (left) column implies a greater spend differential in favor of the Republican party candidate than the +1 SD (right) column does for a Democratic party candidate. If working at the sample-side level for the disaggregated "natural scale" data (`houseElectionDat` instead of `houseElectionDat_std_agg`) is correct here, the -1SD column implies the Republican party candidate maximum outspends the Democratic party maximum by roughly \$2 million (\$2,027,666). Likewise, the +1SD column implies the Democratic party candidate maximum outspends the Republican party maximum by roughly $1.6 million (\$1,584,997).

2) "Massively outspend all the other party's candidates" is not a tenable strategy, not only due to presumable reduced marginal return but also because on the Republican side this would be an aggregate differential of \$882 million per House election period, and on the Democratic side this would be a spend differential of about \$689.5 million. According to the `houseElectionDat` data set, the most spent by any single campaign in the three years considered was FL-18, where $230.5 million dollars were spent by campaigns alone (excluding PACs / Super PACs / etc.) in 2012. In other words, the *differential* in spending per two-year election cycle would need to be at least 3 times the *total* of the most profligate single campaign in the data set, in order to align with this counterfactual analysis and model assumptions.


### Considering some district-level posteriors for a few districts

Let's consider posterior distribution estimates for some district-level intercepts and slopes, and population-level standard deviations, and correlations.

For this I will consider the following districts:

* PA-7, the district (just barely) containing Villanova University and whose contorted profile for this period has been described as "Goofy kicking Donald Duck"[^10]. There was a Republican party incumbent each election, and the incumbent won each time.

[^10]:https://www.washingtonpost.com/blogs/the-fix/post/name-that-district-contest-winner-goofy-kicking-donald-duck/2011/12/29/gIQA2Fa2OP_blog.html?noredirect=on

* MA-7, the district containing approximately half of Boston and much of its surrounding suburbs. Somewhat the obverse of PA-7, Massachusetts' 7th had a Democratic party incumbent each election, and this incumbent won each time.

* NH-1, the district containing the southeastern portion of New Hampshire. This district went back-and-forth between parties, with the same Democratic candidate elected in 2012 and 2016, and a Republican candidate elected in 2014. There was a Republican incumbent in 2012 and (the same candidate) in 2016, while there was a Democratic incumbent in 2014.

A few population-level posteriors are also considered.

This first code chunk extracts some population level parameters and the district-level deviances from the population estimates for each district considered. It then computes the district-level value estimates for each parameter.

```{r}
post_m2.allFull_v2_int1 <- 
  m2.allFull_vIvS2_int1 %>%
  posterior_samples() %>%
  # selecting only a subset of parameters
  select(b_Intercept, b_incumbentD_mn, b_incumbentR_mn, 
         b_cmpgnMaxDisbursDvsR_mn,
         sd_district2__Intercept, 
         sd_district2__incumbentD_mn, sd_district2__incumbentR_mn,
         sd_district2__cmpgnMaxDisbursDvsR_mn,
         cor_district2__Intercept__incumbentD_mn,
         cor_district2__Intercept__incumbentR_mn,
         cor_district2__Intercept__cmpgnMaxDisbursDvsR_mn,
         cor_district2__incumbentD_mn__cmpgnMaxDisbursDvsR_mn,
         cor_district2__incumbentR_mn__cmpgnMaxDisbursDvsR_mn,
         "r_district2[MA-7,Intercept]", "r_district2[NH-1,Intercept]",
         "r_district2[PA-7,Intercept]",
         "r_district2[MA-7,incumbentD_mn]", "r_district2[NH-1,incumbentD_mn]",
         "r_district2[PA-7,incumbentD_mn]",
         "r_district2[MA-7,incumbentR_mn]", "r_district2[NH-1,incumbentR_mn]",
         "r_district2[PA-7,incumbentR_mn]",
         "r_district2[MA-7,cmpgnMaxDisbursDvsR_mn]", 
         "r_district2[NH-1,cmpgnMaxDisbursDvsR_mn]",
         "r_district2[PA-7,cmpgnMaxDisbursDvsR_mn]" )

post_m2_v2_int1_sub <- 
  post_m2.allFull_v2_int1 %>%
  transmute(`intercept_MA-7`  = b_Intercept + `r_district2[MA-7,Intercept]`,
            `incumbentD_MA-7` = b_incumbentD_mn + 
                                `r_district2[MA-7,incumbentD_mn]`,
            `incumbentR_MA-7` = b_incumbentR_mn + 
                                `r_district2[MA-7,incumbentR_mn]`,
            `cmpgnMaxDisbursDvsR_MA-7` = 
                                b_cmpgnMaxDisbursDvsR_mn +
                                `r_district2[MA-7,cmpgnMaxDisbursDvsR_mn]`,
            `intercept_NH-1`  = b_Intercept + `r_district2[NH-1,Intercept]`,
            `incumbentD_NH-1` = b_incumbentD_mn + 
                                `r_district2[NH-1,incumbentD_mn]`,
            `incumbentR_NH-1` = b_incumbentR_mn + 
                                `r_district2[NH-1,incumbentR_mn]`,
            `cmpgnMaxDisbursDvsR_NH-1` = 
                                b_cmpgnMaxDisbursDvsR_mn +
                                `r_district2[NH-1,cmpgnMaxDisbursDvsR_mn]`,
            `intercept_PA-7`  = b_Intercept + `r_district2[PA-7,Intercept]`,
            `incumbentD_PA-7` = b_incumbentD_mn + 
                                `r_district2[PA-7,incumbentD_mn]`,
            `incumbentR_PA-7` = b_incumbentR_mn + 
                                `r_district2[PA-7,incumbentR_mn]`,
            `cmpgnMaxDisbursDvsR_PA-7` = 
                                b_cmpgnMaxDisbursDvsR_mn +
                                `r_district2[PA-7,cmpgnMaxDisbursDvsR_mn]`) %>%
  # gather to narrow data format
  gather() %>%
  # extract variables based on district2
  extract(key, into = c("par", "district2"),
          # ^ = start of string, 
          # ([:alpha:]*) = first group; match upper/lower alphabetic characters
          #                match to end of group
          # \\_ = literal underscore; separates first and second group here
          # ([a-zA-Z0-9\\-]*) = second group; match upper/lower alphanumeric chr
          #                     AND (escaped) dash symbol; match to end of group
          regex = "^([:alpha:]*)\\_([a-zA-Z0-9\\-]*)") %>%
  # there are 5,250 post-warmup MCMC iterations per parameter (12 pars here)
  mutate(obs = rep(1:5250, 12) ) %>%
  # spread the parameters into their own columns, 
  # now linked to which iter and district2; value is the stored value per entry
  spread(par, value)
```

Next we'll plot each district's estimates for `b_Intercept` (the baseline tendency to elect Democratic candidates) and each of `b_incumbentD` and `b_incumbentR` (the estimated marginal effect a given party having an incumbent in the district  associated with electing a Democratic candidate in that district).

```{r, message = F, fig.align = "center", fig.width = 9, fig.height = 9}
plot1 <- 
  post_m2_v2_int1_sub %>%
  ggplot(aes(x = intercept, y = incumbentD, color = district2)) +
  geom_point(alpha = 0.4) +
  #geom_text(aes(label = district2, x = 0, y = 0)) +
  labs(title = "Posterior estimates for 3 districts",
       x = "Intercept", y = "incumbentD") +
  scale_color_manual(values = c(color_set8[1:2], "orangered")) +
  theme_ds1() + theme(legend.position = "top")

plot2 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = cor_district2__Intercept__incumbentD_mn)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for cor parameter",
       subtitle = "District-level correlation?",
       x = "cor_Intercept_incumbentD", y = "Density") +
  theme_ds1()

plot3 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = sd_district2__Intercept)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for SD parameter",
       subtitle = "Inter-district SD?",
       x = "sd_Intercept", y = "Density") +
  theme_ds1()

plot4 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = sd_district2__incumbentD_mn)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for SD parameter",
       subtitle = "Inter-district SD?",
       x = "sd_incumbentD", y = "Density") +
  theme_ds1()

library(cowplot)
plot_grid(plot1, plot2, plot3, plot4, nrow = 2)
```

The quarter of plots above show three district-level bivariate posterior estimates (upper left) as well as three population-level posterior estimates for correlation between the two parameters (upper right) and standard deviations which I believe summarize district-level estimates of variance from the population-level estimate for each parameter (bottom).

Each of the three districts appear to have considerable uncertainty for the `b_Intercept` and `b_incumbentD` estimates, with the former centered over zero and the latter centered over 12 or so.

I think the correlation estimate's being centered over zero may be related to `brms`'s using non-centered parametrization, but I don't understand the concept well enough to be very confident in that.

For comparison, here is the same setup with `incumbentR`:

```{r, message = F, fig.align = "center", fig.width = 9, fig.height = 9}
plot1 <- 
  post_m2_v2_int1_sub %>%
  ggplot(aes(x = intercept, y = incumbentR, color = district2)) +
  geom_point(alpha = 0.4) +
  #geom_text(aes(label = district2, x = 0, y = 0)) +
  labs(title = "Posterior estimates for 3 districts",
       x = "Intercept", y = "incumbentR") +
  scale_color_manual(values = c(color_set8[1:2], "orangered")) +
  theme_ds1() + theme(legend.position = "top")

plot2 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = cor_district2__Intercept__incumbentR_mn)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for cor parameter",
       subtitle = "District-level correlation?",
       x = "cor_Intercept_incumbentR", y = "Density") +
  theme_ds1()

plot3 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = sd_district2__Intercept)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for SD parameter",
       subtitle = "Inter-district SD?",
       x = "sd_Intercept", y = "Density") +
  theme_ds1()

plot4 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = sd_district2__incumbentR_mn)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for SD parameter",
       subtitle = "Inter-district SD?",
       x = "sd_incumbentR", y = "Density") +
  theme_ds1()

plot_grid(plot1, plot2, plot3, plot4, nrow = 2)
```

There appears to be roughly equal uncertainty for each district's estimate of `b_incumbentR` in the opposite direction from `b_incumbentD`. The population-level `sd_incumbentR` estimate has more proportional density over larger values compared to `sd_incumbentD`, suggesting perhaps there is wider variability for the former among districts.

Finally, here are estimates for each of `b_incumbentD` and `b_incumbentR` with `cmpgnMaxDisbursDvsR`.

```{r, message = F, fig.align = "center", fig.width = 9, fig.height = 9}
plot1 <- 
  post_m2_v2_int1_sub %>%
  ggplot(aes(x = incumbentD, y = cmpgnMaxDisbursDvsR, color = district2)) +
  geom_point(alpha = 0.4) +
  #geom_text(aes(label = district2, x = 0, y = 0)) +
  labs(title = "Posterior estimates for 3 districts",
       x = "incumbentD", y = "cmpgnMaxDisbursDvsR") +
  scale_color_manual(values = c(color_set8[1:2], "orangered")) +
  theme_ds1() + theme(legend.position = "top")

plot2 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = cor_district2__incumbentD_mn__cmpgnMaxDisbursDvsR_mn)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for cor parameter",
       subtitle = "District-level correlation?",
       x = "cor_incumbentD_cmpgnMaxDisbursDvsR", y = "Density") +
  theme_ds1()

plot3 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = sd_district2__incumbentD_mn)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for SD parameter",
       subtitle = "Inter-district SD?",
       x = "sd_incumbentD", y = "Density") +
  theme_ds1()

plot4 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = sd_district2__cmpgnMaxDisbursDvsR_mn)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for SD parameter",
       subtitle = "Inter-district SD?",
       x = "sd_cmpgnMaxDisbursDvsR", y = "Density") +
  theme_ds1()

plot_grid(plot1, plot2, plot3, plot4, nrow = 2)
```

```{r, message = F, fig.align = "center", fig.width = 9, fig.height = 9}
plot1 <- 
  post_m2_v2_int1_sub %>%
  ggplot(aes(x = incumbentR, y = cmpgnMaxDisbursDvsR, color = district2)) +
  geom_point(alpha = 0.4) +
  #geom_text(aes(label = district2, x = 0, y = 0)) +
  labs(title = "Posterior estimates for 3 districts",
       x = "incumbentR", y = "cmpgnMaxDisbursDvsR") +
  scale_color_manual(values = c(color_set8[1:2], "orangered")) +
  theme_ds1() + theme(legend.position = "top")

plot2 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = cor_district2__incumbentR_mn__cmpgnMaxDisbursDvsR_mn)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for cor parameter",
       subtitle = "District-level correlation?",
       x = "cor_incumbentR_cmpgnMaxDisbursDvsR", y = "Density") +
  theme_ds1()

plot3 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = sd_district2__incumbentR_mn)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for SD parameter",
       subtitle = "Inter-district SD?",
       x = "sd_incumbentR", y = "Density") +
  theme_ds1()

plot4 <- 
  post_m2.allFull_v2_int1 %>%
  ggplot(aes(x = sd_district2__cmpgnMaxDisbursDvsR_mn)) +
  geom_density(fill = "gray", alpha = 0.7) +
  labs(title = "Posterior estimate for SD parameter",
       subtitle = "Inter-district SD?",
       x = "sd_cmpgnMaxDisbursDvsR", y = "Density") +
  theme_ds1()

plot_grid(plot1, plot2, plot3, plot4, nrow = 2)
```

### Example of Stan code for the interaction-including model

This is the unwieldy code that `brms` converts from the `brms::brm()` design model into Stan code. Makes me very, very glad that `brms` exists...

```{r}
stancode(m2.allFull_vIvS2_int1)
```


```{r, eval = F, echo = F, message = F, fig.align = "center", fig.width = 8.5, fig.height = 11.5}
set.seed(8800)
# betas prior
prior_b <- rnorm(1000, mean = 0, sd = 10) %>%
  as_tibble()

# plots
post_m2.allFull_vIvS2_XX <- posterior_samples(m2.allFull_vIvS2_XX) %>%
  select(-lp__)

plotInt <- 
  post_m2.allFull_vIvS2_XX %>%
  ggplot(aes(x = b_Intercept)) +
  geom_density(data = prior_b, aes(x = value), 
               fill = color_set8[3], alpha = 0.6) +
  geom_density(fill = color_set8[4], alpha = 0.4) +
  labs(subtitle = "Intercept", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotincumbentD <- 
  post_m2.allFull_vIvS2_XX %>%
  ggplot(aes(x = b_incumbentD_mn)) +
  geom_density(data = prior_b, aes(x = value), 
               fill = color_set8[3], alpha = 0.6) +
  geom_density(fill = color_set8[4], alpha = 0.4) +
  labs(subtitle = "incumbentD", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotincumbentR <- 
  post_m2.allFull_vIvS2_XX %>%
  ggplot(aes(x = b_incumbentR_mn)) +
  geom_density(data = prior_b, aes(x = value), 
               fill = color_set8[3], alpha = 0.6) +
  geom_density(fill = color_set8[4], alpha = 0.4) +
  labs(subtitle = "incumbentR", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotprimaryUnopD <- 
  post_m2.allFull_vIvS2_XX %>%
  ggplot(aes(x = b_primaryUnopD_mn)) +
  geom_density(data = prior_b, aes(x = value), 
               fill = color_set8[3], alpha = 0.6) +
  geom_density(fill = color_set8[4], alpha = 0.4) +
  labs(subtitle = "primaryUnopD", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotprimaryUnopR <- 
  post_m2.allFull_vIvS2_XX %>%
  ggplot(aes(x = b_primaryUnopR_mn)) +
  geom_density(data = prior_b, aes(x = value), 
               fill = color_set8[3], alpha = 0.6) +
  geom_density(fill = color_set8[4], alpha = 0.4) +
  labs(subtitle = "primaryUnopR", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotcmpgnMaxDisbursDvsR <- 
  post_m2.allFull_vIvS2_XX %>%
  ggplot(aes(x = b_cmpgnMaxDisbursDvsR_mn)) +
  geom_density(data = prior_b, aes(x = value), 
               fill = color_set8[3], alpha = 0.6) +
  geom_density(fill = color_set8[4], alpha = 0.4) +
  labs(subtitle = "cmpgnMaxDisbursDvsR", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotprimaryTotDvsR <- 
  post_m2.allFull_vIvS2_XX %>%
  ggplot(aes(x = b_primaryTotDvsR_mn)) +
  geom_density(data = prior_b, aes(x = value), 
               fill = color_set8[3], alpha = 0.6) +
  geom_density(fill = color_set8[4], alpha = 0.4) +
  labs(subtitle = "primaryTotDvsR", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotlogPopDensPerSqMi <- 
  post_m2.allFull_vIvS2_XX %>%
  ggplot(aes(x = b_log_popDensPerSqMi_mn)) +
  geom_density(data = prior_b, aes(x = value), 
               fill = color_set8[3], alpha = 0.6) +
  geom_density(fill = color_set8[4], alpha = 0.4) +
  labs(subtitle = "log_popDensPerSqMi", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotpctEduGradProAge25plus <- 
  post_m2.allFull_vIvS2_XX %>%
  ggplot(aes(x = b_pctEduGradProAge25plus_mn)) +
  geom_density(data = prior_b, aes(x = value), 
               fill = color_set8[3], alpha = 0.6) +
  geom_density(fill = color_set8[4], alpha = 0.4) +
  labs(subtitle = "pctEduGradProAge25plus", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotpctInLaborForceUnemp <- 
  post_m2.allFull_vIvS2_XX %>%
  ggplot(aes(x = b_pctInLaborForceUnemp_mn)) +
  geom_density(data = prior_b, aes(x = value), 
               fill = color_set8[3], alpha = 0.6) +
  geom_density(fill = color_set8[4], alpha = 0.4) +
  labs(subtitle = "pctInLaborForceUnemp", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

library(cowplot)

p <- plot_grid(plotInt, plotincumbentD, plotincumbentR, plotprimaryUnopD, plotprimaryUnopR,
          plotcmpgnMaxDisbursDvsR, plotprimaryTotDvsR, plotlogPopDensPerSqMi,
          plotpctEduGradProAge25plus, plotpctInLaborForceUnemp, ncol = 2, nrow = 5)

title <- ggdraw() + draw_label("Prior (Dk green) and Posterior (Lt green) estimates")

plot_grid(title, p, ncol = 2, rel_heights=c(0.1, 1)) # rel_heights values control title margins
```




# Appendix 1: - Single-year model fitting and analysis

## Model 1 - Bernoulli model, using 2012 data

As a first approximation / candidate model, I will use the following setup:

**Outcome variable:** `generalWinD` (D winner in general election for a given House district)

**Model form:** Bernoulli

*Justification:* Each district's outcome is binary; either a Democratic party candidate won the seat, or they did not (in which case, for each of the three years considered, a Republican party candidate won).

Where possible, I will account for measurement uncertainty in the predictor variables - this applies to variables from the U.S. Census Bureau.

I will also make use of partial pooling with varying intercepts by district.

*Note: For each of the following models, I made sure the population-level parameters all had Rhat values of no more than 1.02 (all but one had values of 1.01 or less, with a strong majority having Rhat values of 1.00 or less), and I also made sure there was good stationarity and mixing of the MCMC chains.*

*These reviews were conducted in the console using `summary()` and `plot()`; more comprehensive analysis follows the model-fitting code chunk for selecting the best-performing model(s) to proceed with.*

*Planned next steps: evaluate model fits by an Information Criterion, and relative model weights by the same, then evaluate model predictions on the fit data, and finally create and assess predictions for the next election's data; lather, rinse, repeat*

### Fitting Model 1 models

The next code chunk is pretty lengthy, but it details each model's components and their priors therein.

```{r, eval = F}
# intercept only model
m1.12_int <- 
  brm(generalWinD ~ 1,
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)",         class = "Intercept") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.95),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_int")

# model including incumbency predictors ONLY
m1.12_incmbt <- 
  brm(generalWinD ~ 1 + incumbentD + incumbentR,
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)", class = "Intercept"),
                set_prior("normal(0,3)", class = "b", coef = "incumbentD"),
                set_prior("normal(0,3)", class = "b", coef = "incumbentR") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_incmbt")

# model including primary unopposed predictors ONLY
m1.12_prmryUnop <- 
  brm(generalWinD ~ 1 + primaryUnopD + primaryUnopR,
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)", class = "Intercept"),
                set_prior("normal(0,2)", class = "b") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_prmryUnop")

# model including difference in max campaign spend ONLY
m1.12_spnd <- 
  brm(generalWinD ~ 1 + cmpgnMaxDisbursDvsR,
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)", class = "Intercept"),
                set_prior("normal(0,2)", class = "b") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.95),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_spnd")

# model including difference in primary vote totals ONLY
m1.12_prmryTot <- 
  brm(generalWinD ~ 1 + primaryTotDvsR,
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)", class = "Intercept"),
                set_prior("normal(0,2)", class = "b") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.95),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_prmryTot")

# model including population density ONLY
m1.12_logPopDens <- 
  brm(generalWinD ~ 1 + log_popDensPerSqMi,
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)", class = "Intercept"),
                set_prior("normal(0,2)", class = "b") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.95),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_logPopDens")

# model including median age ONLY
m1.12_mdnAge <- 
  brm(generalWinD ~ 1 + me(medianAgeYr, medianAgeYr_SE),
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)",         class = "Intercept"),
                set_prior("normal(0,2)",         class = "b"),
                set_prior("normal(0,10)",        class = "meanme"),
                set_prior("student_t(3, 0, 10)", class = "sdme")),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99),
      thin = 2,
      save_mevars = TRUE,
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_mdnAge")

# model including education level ONLY
m1.12_hiEdu <- 
  brm(generalWinD ~ 1 + me(pctEduGradProAge25plus, pctEduGradProAge25plus_SE),
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)",         class = "Intercept"),
                set_prior("normal(0,2)",         class = "b"),
                set_prior("normal(0,10)",        class = "meanme"),
                set_prior("student_t(3, 0, 10)", class = "sdme") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99),
      thin = 2,
      save_mevars = TRUE,
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_hiEdu")

# model including unemployment and per capita income ONLY
m1.12_unempInc <- 
  brm(generalWinD ~ 1 + me(pctInLaborForceUnemp, pctInLaborForceUnemp_SE) + 
                        me(perCapitaIncome, perCapitaIncome_SE),
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)",         class = "Intercept"),
                set_prior("normal(0,2)",         class = "b"),
                set_prior("normal(0,10)",        class = "meanme"),
                set_prior("student_t(3, 0, 10)", class = "sdme") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99, max_treedepth = 12),
      thin = 2,
      save_mevars = TRUE,
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_unempInc")

# model including % white alone ONLY
m1.12_wht <- 
  brm(generalWinD ~ 1 + me(pctWhiteAlone, pctWhiteAlone_SE),
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)",         class = "Intercept"),
                set_prior("normal(0,2)",         class = "b"),
                set_prior("normal(0,10)",        class = "meanme"),
                set_prior("student_t(3, 0, 10)", class = "sdme")),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99),
      save_mevars = TRUE,
      thin = 2,
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_wht")

# model including election/campaign predictors ONLY
m1.12_elecCmpgn <- 
  brm(generalWinD ~ 1 + incumbentD + incumbentR + primaryUnopD + primaryUnopR + 
                        cmpgnMaxDisbursDvsR + primaryTotDvsR,
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)",         class = "Intercept"),
                set_prior("normal(0,2)",         class = "b") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_elecCmpgn")

# model including demographic predictors ONLY
# no measurement error inclusion due to issue with fitting information criterion
m1.12_demo_noME <- 
  brm(generalWinD ~ 1 + log_popDensPerSqMi + medianAgeYr + pctEduGradProAge25plus + 
                    pctInLaborForceUnemp + perCapitaIncome + pctWhiteAlone,
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)",         class = "Intercept"),
                set_prior("normal(0,2)",         class = "b") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99, max_treedepth = 12),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_demo_noME")

# model including ALL predictors
# no measurement error inclusion due to issue with fitting information criterion
m1.12_fullHouse_noME <- 
  brm(generalWinD ~ 1 + incumbentD + incumbentR + primaryUnopD + primaryUnopR + 
                        cmpgnMaxDisbursDvsR + primaryTotDvsR + log_popDensPerSqMi 
                      + medianAgeYr + pctEduGradProAge25plus + 
                        pctInLaborForceUnemp + perCapitaIncome + pctWhiteAlone,
      data = houseElectionDat12_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)",         class = "Intercept"),
                set_prior("normal(0,2)",         class = "b") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99, max_treedepth = 12),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_fullHouse_noME")
```

### Evaluating Model 1 fits

Here, I'll consider each model's Information Criterion estimate, and conduct a more in-depth analysis on the model(s) that emerge as the most promising. This analysis will include residuals plots (district-level error in prediction), counterfactual plots (model-implied prediction for theoretical values of one predictor, holding other values constant at some level), and combination plots (to confirm the model(s)'s Markov chains indicate a good model fit). 

Provided these steps turn out well, I will then investigate using the model(s) to predict 2014 data, and then move on to investigating whether fitting the same set of models to 2014 data exhibits similar behavior and also performance on predicting 2016 data.

```{r, echo = F}
m1.12_int <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_int.rds")

m1.12_incmbt <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_incmbt.rds")

m1.12_prmryUnop <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_prmryUnop.rds")

m1.12_spnd <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_spnd.rds")

m1.12_prmryTot <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_prmryTot.rds")

m1.12_logPopDens <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_logPopDens.rds")

m1.12_mdnAge <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_mdnAge.rds")

m1.12_hiEdu <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_hiEdu.rds")

m1.12_unempInc <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_unempInc.rds")

m1.12_wht <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_wht.rds")

m1.12_elecCmpgn <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_elecCmpgn.rds")

m1.12_demo_noME <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_demo_noME.rds")

m1.12_fullHouse_noME <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_fullHouse_noME.rds")
```

**Looking into fitting `m1.12_unempInc`**

```{r, echo = F, fig.align = "center", fig.width = 10, fig.height = 7}
# troubleshooting m1.12_unempInc - currently not fitting well - with bayesplot
# http://mc-stan.org/bayesplot/articles/visual-mcmc-diagnostics.html

# extract posterior draws
post_m1.12_unempInc   <- as.array(m1.12_unempInc)

post_m1.12_demo_noME  <- as.array(m1.12_demo_noME)

# extract No U-Turn Sampler parameters
np_m1.12_unempInc   <- nuts_params(m1.12_unempInc)

# isolate the params for population-level rather than group-level effects
# same parameters for original and V2 models with unempInc
popParams_m1.12_unempInc <- 
  parnames(m1.12_unempInc)[!grepl("r_district2", parnames(m1.12_unempInc)) &
                           !grepl("Xme_", parnames(m1.12_unempInc)) & 
                           !grepl("lp__", parnames(m1.12_unempInc))]

popParams_m1.12_demo_noME <- 
  parnames(m1.12_demo_noME)[!grepl("lp__", parnames(m1.12_demo_noME))]

color_scheme_set(scheme = color_set8[c(1:4, 6:7)])

# a plot to evaluate divergence
mcmc_parcoord(post_m1.12_unempInc, 
              np = np_m1.12_unempInc, pars = popParams_m1.12_unempInc) +
  labs(title = "unempInc") +
  theme_ds1() +
  theme(axis.text.x = element_text(angle = 90, size = 10, vjust = 0.5))

# evaluating Rhat
mcmc_rhat(rhat(m1.12_unempInc, pars = popParams_m1.12_unempInc)) +
  labs(title = "unempInc") +
  theme_ds1() + theme(legend.position = "top")

# evaluating effective sample size, n_eff
mcmc_neff(neff_ratio(m1.12_unempInc, pars = popParams_m1.12_unempInc)) +
  labs(title = "unempInc") +
  theme_ds1() + theme(legend.position = "top")

# evaluating autocorrelation
mcmc_acf(post_m1.12_unempInc, 
         pars = popParams_m1.12_unempInc, lags = 50) +
  labs(title = "unempInc") +
  theme_ds1() +
  theme(strip.text.x = element_text(size = 9, hjust = 0))

mcmc_acf(post_m1.12_demo_noME, 
         pars = popParams_m1.12_demo_noME, lags = 50) +
  labs(title = "demo_noME") +
  theme_ds1() +
  theme(strip.text.x = element_text(size = 9, hjust = 0))
```

### Evaluating Model 1 fits - Information Criterion & Weights

**Comparing models by WAIC to estimate out-of-sample prediction evaluation**

While I am able to use Leave-one-out cross-validation with Pareto-smoothed importance sampling (PSIS-LOO) to estimate each model's out-of-sample prediction performance, there is a programmatic error when applying PSIS-LOO for computing model weights via `brms::model_weights()`; therefore, I'm using WAIC as an approximation for model weighting instead. 

*Update 12/23/18: Just noting for potential later reference - with `m1.12_spnd`, observations 105 and 306 were considered problematic/influential in the context of PSIS-LOO. Ditto for `m1.12_elecCmpgn` and `m1.12_fullHouse_noME`, the other two models containing the variable for differential in maximum campaign reported spend for D-vs.-R by district. Those two observations are for FL-18 and OH-8, respectively (each for 2012). FL-18 has a `cmpgnMaxDisbursDvsR value of roughly -$1.4mn, and OH-8 has a value of about $-2.1mn`. The OH-8 value is the largest-margin value favoring Republican spend (and has the greatest absolute value) in the data set, and the FL-18 value is third (and has an absolute value also leading any Democrat-favoring spend value).*

```{r, eval = F}
psisLOOCV_m1.12 <- loo(m1.12_int, 
                       m1.12_incmbt, m1.12_prmryUnop, m1.12_spnd, m1.12_prmryTot,
                       m1.12_logPopDens, m1.12_mdnAge, m1.12_hiEdu, m1.12_unempInc, 
                       m1.12_wht, m1.12_elecCmpgn, m1.12_demo_noME, 
                       m1.12_fullHouse_noME,
                       compare = T, reloo = T)

modWts_m1.12 <- 
  model_weights(m1.12_int, 
                m1.12_incmbt, m1.12_prmryUnop, m1.12_spnd, m1.12_prmryTot,
                m1.12_logPopDens, m1.12_mdnAge, m1.12_hiEdu, 
                m1.12_unempInc,
                m1.12_wht, m1.12_elecCmpgn, m1.12_demo_noME,
                m1.12_fullHouse_noME, 
                weights = "waic")
```

```{r, eval = F, echo = F}
# save the PSIS-LOO cross-validation IC object
saveRDS(psisLOOCV_m1.12,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/psisLOOCV_m1.12.rds")

# save the model weights object
saveRDS(modWts_m1.12,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/modWts_m1.12.rds")
```

```{r, echo = F}
psisLOOCV_m1.12 <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/psisLOOCV_m1.12.rds")

modWts_m1.12 <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/modWts_m1.12.rds")
```

Quick overview of the exhaustive between-models PSIS-LOO comparison:

```{r}
psisLOOCV_m1.12
```

```{r}
mods <- names(modWts_m1.12)

modWts_m1.12 %>%
  as_tibble() %>%
  mutate(model = mods) %>%
  rename(WAIC_weight = value) %>%
  select(model, WAIC_weight) %>%
  arrange(desc(WAIC_weight)) %>%
  mutate(WAIC_weight = formatC(WAIC_weight, format = "e", digits = 2)) %>%
  kable(align = "c", booktabs = T, digits = 3) %>% 
  kable_styling(full_width = F)
```

While no model has exactly zero weight, the "full" model has the greatest share of model weight by such a margin that I will use it alone going forward.

### Evaluating Model 1 fits - the "full" model

Let's confirm the model fit well, and then evaluate prior estimates, and see if the full model could possibly be trimmed:

```{r, echo = F, fig.align = "center", fig.width = 8.5}
# evaluating model fit
color_scheme_set(scheme = color_set8[c(1:4, 6:7)])

plot(m1.12_fullHouse_noME, 
     combo = c("dens_overlay", "trace"), N = 4, ask = F) +
  theme_ds1()
```

Model fits look good - each chain follows essentially the same density distribution for each parameter, and there is good mixing and stationarity for each chain.

Visualize posterior estimates:

```{r, echo = F, fig.align = "center", fig.width = 8.5}
post_m1.12_fullHouse_noME <- posterior_samples(m1.12_fullHouse_noME) %>%
  select(-lp__)

post_m1.12_fullHouse_noME %>%
mcmc_intervals(prob = 0.5, prob_outer = 0.9) +
  labs(title = "m1.12_fullhouse_noME",
       subtitle = "Posterior coefficient estimates",
       caption = "Line: 90% credible interval \n Bar: 50% CI") + 
  theme_ds1()

rm(m1.12_int, m1.12_incmbt, m1.12_prmryUnop, m1.12_prmryTot, m1.12_spnd, m1.12_logPopDens,
   m1.12_hiEdu, m1.12_mdnAge, m1.12_unempInc, m1.12_wht, m1.12_elecCmpgn, m1.12_demo_noME)
```
It seems very plausible that `pctWhiteAlone` and perhaps `perCapitaIncome` do not provide any marginal predictive insight into the probability of a Democratic candidate winning House election over a Republican competitor. 

### Evaluating Model 1 fits - Improving the "full" model

After first fitting a model dropping `pctWhiteAlone` (`m1.12_redHouse`), then one dropping both `pctWhiteAlone` and `perCapitaIncome` (`m1.12_redHouse2`), and finally a model dropping these two predictors and also `medianAgeYr` (`m1.12_redHouse3`), it appears that this last model is the best candidate, with lower PSIS-LOOIC values and greater allocated model weight (this time using LOO was successful):

```{r, echo = F, eval = F}
# fit the new models
m1.12_redHouse  <- update(m1.12_fullHouse_noME,
                         . ~ . - pctWhiteAlone)

m1.12_redHouse2 <- update(m1.12_fullHouse_noME,
                         . ~ . - pctWhiteAlone - perCapitaIncome)

m1.12_redHouse3 <- update(m1.12_fullHouse_noME,
                         . ~ . - pctWhiteAlone - perCapitaIncome - medianAgeYr)

# confirm good fitting behavior
plot(m1.12_redHouse2, 
     combo = c("dens_overlay", "trace"), N = 4, ask = F) +
  theme_ds1()

# assess posterior distribution estimates
post_m1.12_redHouse3 <- posterior_samples(m1.12_redHouse3) %>%
  select(-lp__)

post_m1.12_redHouse3 %>%
mcmc_intervals(prob = 0.5, prob_outer = 0.9) +
  labs(title = "m1.12_redhouse3",
       subtitle = "Posterior coefficient estimates",
       caption = "Line: 90% credible interval \n Bar: 50% CI") + 
  theme_ds1()

# compare the top candidate models
psisLOOCV_m1.12V2 <- loo(m1.12_redHouse, m1.12_redHouse2, m1.12_redHouse3, m1.12_fullHouse_noME, 
    compare = T, reloo = T)

# relative model weighting
modWts_m1.12V2 <- 
  model_weights(m1.12_redHouse, m1.12_redHouse2, m1.12_redHouse3, m1.12_fullHouse_noME, 
                weights = "loo")

saveRDS(m1.12_redHouse,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_redHouse.rds")

saveRDS(m1.12_redHouse2,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_redHouse2.rds")

saveRDS(m1.12_redHouse3,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_redHouse3.rds")

saveRDS(psisLOOCV_m1.12V2,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/psisLOOCV_m1.12V2.rds")

saveRDS(modWts_m1.12V2,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/modWts_m1.12V2.rds")
```

```{r, echo = F}
m1.12_redHouse <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_redHouse.rds")

m1.12_redHouse2 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_redHouse2.rds")

m1.12_redHouse3 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.12_redHouse3.rds")

psisLOOCV_m1.12V2 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/psisLOOCV_m1.12V2.rds")

modWts_m1.12V2 <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/modWts_m1.12V2.rds")

# visualizing stand-alone model PSIS-LOO IC
modComp1 <- 
  tibble(modName = c("m1.12_fullHouse_noME",
                     "m1.12_redHouse", "m1.12_redHouse2", "m1.12_redHouse3"),
         # for each estimates element, entry 1 is the 'Estimate', entry 2 is the 'SE'
         looIC   = c(psisLOOCV_m1.12V2$m1.12_fullHouse_noME$estimates["looic", 1], 
                     psisLOOCV_m1.12V2$m1.12_redHouse$estimates["looic", 1],
                     psisLOOCV_m1.12V2$m1.12_redHouse2$estimates["looic", 1], 
                     psisLOOCV_m1.12V2$m1.12_redHouse3$estimates["looic", 1]),
         looSE   = c(psisLOOCV_m1.12V2$m1.12_fullHouse_noME$estimates["looic", 2], 
                     psisLOOCV_m1.12V2$m1.12_redHouse$estimates["looic", 2],
                     psisLOOCV_m1.12V2$m1.12_redHouse2$estimates["looic", 2], 
                     psisLOOCV_m1.12V2$m1.12_redHouse3$estimates["looic", 2]) )

modComp1 %>%
  ggplot(aes(x = modName, y = looIC)) +
  geom_pointrange(aes(ymin = looIC - looSE, ymax = looIC + looSE, 
                      color = (modName == "m1.12_fullHouse_noME"))) +
  labs(x = "Model", y = "LOOIC", title = "Stand-alone LOOIC estimates",
       subtitle = "Lines reflect 1 standard error") +
  scale_color_manual(values = color_set8[c(2,1)]) +
  coord_flip() +
  theme_ds1() + theme(legend.position = "none")

# visualizing inter-model model PSIS-LOO IC
modComp2 <-
  # matrix of similar data as prior set, but for inter-model comparison
  psisLOOCV_m1.12V2$ic_diffs__ %>%  
  as_tibble() %>%
  mutate(modComp = rownames(psisLOOCV_m1.12V2$ic_diffs__))

modComp2 %>%
  ggplot(aes(x = modComp, y = LOOIC)) +
  geom_hline(yintercept = 0, color = "grey28", linetype = 2) +
  geom_pointrange(aes(ymin = LOOIC - SE, ymax = LOOIC + SE), 
                  color = color_set8[1], fill = color_set8[2]) +
  labs(title = "Inter-model LOOIC comparisons",
       subtitle = "Lines reflect 1 standard error",
       x = "Model comparison", y = "LOOIC") +
  coord_flip() +
  theme_ds1()

# visualizing relative model IC and weights
modWts_m1.12V2 %>%
  as_tibble() %>%
  rename(weight = value) %>%
  mutate(model  = names(modWts_m1.12V2),
         weight = weight %>% round(3)) %>%
  select(model, weight) %>%
  arrange(desc(weight)) %>%
  kable(booktabs = T) %>% kable_styling(full_width = F)
```

Since the model excluding `pctWhiteAlone`, `perCapitaIncome`, and `medianAgeYr` performs so much better than the others, I'll proceed with it for now.

```{r, echo = F, message = F, fig.align = "center", fig.width = 8.5, fig.height = 10}
set.seed(8800)
# Intercept prior
priorIntercept <- rnorm(1000, mean = 0, sd = 3) %>%
  as_tibble()
# Predictor (any) prior
priorPredictor <- rnorm(1000, mean = 0, sd = 2) %>%
  as_tibble()

# plots
post_m1.12_redHouse3 <- posterior_samples(m1.12_redHouse3) %>%
  select(-lp__)

plotX <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_Intercept)) +
  geom_density()
kernPlotDat <- ggplot_build(plotX)$'data'[[1]]
obsDensity <- kernPlotDat$density[which.min(abs(kernPlotDat$x - 0))]

plotInt <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_Intercept)) +
  geom_density(data = priorIntercept, aes(x = value), 
               fill = color_set8[1], alpha = 0.6) +
  geom_density(fill = color_set8[2], alpha = 0.4) +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = obsDensity), color = "black", linetype = 2) +
  annotate("text", label = "prior", x = -3, y = 0.25, size = 5, color = color_set8[1]) +
  annotate("text", label = "posterior", x = 4, y = 0.4, size = 5, color = color_set8[2]) + 
  labs(subtitle = "Estimates for b_Intercept", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotincumbentD <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_incumbentD)) +
  geom_density(data = priorPredictor, aes(x = value), 
               fill = color_set8[1], alpha = 0.6) +
  geom_density(fill = color_set8[2], alpha = 0.4) +
  annotate("text", label = "prior", x = -3, y = 0.2, size = 5, color = color_set8[1]) +
  annotate("text", label = "posterior", x = -1, y = 0.4, size = 5, color = color_set8[2]) + 
  labs(subtitle = "Estimates for b_incumbentD", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotincumbentR <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_incumbentR)) +
  geom_density(data = priorPredictor, aes(x = value), 
               fill = color_set8[1], alpha = 0.6) +
  geom_density(fill = color_set8[2], alpha = 0.4) +
  annotate("text", label = "prior", x = 0, y = 0.3, size = 5, color = color_set8[1]) +
  annotate("text", label = "posterior", x = 0.5, y = 0.5, size = 5, color = color_set8[2]) + 
  labs(subtitle = "Estimates for b_incumbentR", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotXD <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_primaryUnopD)) +
  geom_density()
kernPlotDat <- ggplot_build(plotXD)$'data'[[1]]
obsDensityD <- kernPlotDat$density[which.min(abs(kernPlotDat$x - 0))]

plotprimaryUnopD <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_primaryUnopD)) +
  geom_density(data = priorPredictor, aes(x = value), 
               fill = color_set8[1], alpha = 0.6) +
  geom_density(fill = color_set8[2], alpha = 0.4) +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = obsDensityD), color = "black", linetype = 2) +
  annotate("text", label = "prior", x = -3.5, y = 0.15, size = 5, color = color_set8[1]) +
  annotate("text", label = "posterior", x = -2.5, y = 0.4, size = 5, color = color_set8[2]) + 
  labs(subtitle = "Estimates for b_primaryUnopD", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotprimaryUnopR <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_primaryUnopR)) +
  geom_density(data = priorPredictor, aes(x = value), 
               fill = color_set8[1], alpha = 0.6) +
  geom_density(fill = color_set8[2], alpha = 0.4) +
  annotate("text", label = "prior", x = 2, y = 0.25, size = 5, color = color_set8[1]) +
  annotate("text", label = "posterior", x = 1.5, y = 0.4, size = 5, color = color_set8[2]) + 
  labs(subtitle = "Estimates for b_primaryUnopR", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotcmpgnMaxDisbursDvsR <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_cmpgnMaxDisbursDvsR)) +
  geom_density(data = priorPredictor, aes(x = value), 
               fill = color_set8[1], alpha = 0.6) +
  geom_density(fill = color_set8[2], alpha = 0.4) +
  annotate("text", label = "prior", x = -4, y = 0.2, size = 5, color = color_set8[1]) +
  annotate("text", label = "posterior", x = -1.5, y = 0.5, size = 5, color = color_set8[2]) + 
  labs(subtitle = "Estimates for b_cmpgnMaxDisbursDvsR", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotprimaryTotDvsR <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_primaryTotDvsR)) +
  geom_density(data = priorPredictor, aes(x = value), 
               fill = color_set8[1], alpha = 0.6) +
  geom_density(fill = color_set8[2], alpha = 0.4) +
  annotate("text", label = "prior", x = -3.5, y = 0.2, size = 5, color = color_set8[1]) +
  annotate("text", label = "posterior", x = -0.5, y = 0.5, size = 5, color = color_set8[2]) + 
  labs(subtitle = "Estimates for b_primaryTotDvsR", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotX <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_log_popDensPerSqMi)) +
  geom_density()
kernPlotDat <- ggplot_build(plotX)$'data'[[1]]
obsDensity <- kernPlotDat$density[which.min(abs(kernPlotDat$x - 0))]

plotlogPopDensPerSqMi <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_log_popDensPerSqMi)) +
  geom_density(data = priorPredictor, aes(x = value), 
               fill = color_set8[1], alpha = 0.6) +
  geom_density(fill = color_set8[2], alpha = 0.4) +
  geom_segment(aes(x = 0, xend = 0, y = 0, yend = obsDensity), color = "black", linetype = 2) +
  annotate("text", label = "prior", x = -4.5, y = 0.15, size = 5, color = color_set8[1]) +
  annotate("text", label = "posterior", x = -2, y = 0.4, size = 5, color = color_set8[2]) + 
  labs(subtitle = "Estimates for b_log_popDensPerSqMi", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotpctEduGradProAge25plus <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_pctEduGradProAge25plus)) +
  geom_density(data = priorPredictor, aes(x = value), 
               fill = color_set8[1], alpha = 0.6) +
  geom_density(fill = color_set8[2], alpha = 0.4) +
  annotate("text", label = "prior", x = -4.5, y = 0.25, size = 5, color = color_set8[1]) +
  annotate("text", label = "posterior", x = -1.5, y = 0.5, size = 5, color = color_set8[2]) + 
  labs(subtitle = "Estimates for b_pctEduGradProAge25plus", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

plotpctInLaborForceUnemp <- 
  post_m1.12_redHouse3 %>%
  ggplot(aes(x = b_pctInLaborForceUnemp)) +
  geom_density(data = priorPredictor, aes(x = value), 
               fill = color_set8[1], alpha = 0.6) +
  geom_density(fill = color_set8[2], alpha = 0.4) +
  annotate("text", label = "prior", x = -4.5, y = 0.25, size = 5, color = color_set8[1]) +
  annotate("text", label = "posterior", x = -1.5, y = 0.5, size = 5, color = color_set8[2]) + 
  labs(subtitle = "Estimates for b_pctInLaborForceUnemp", 
       x = "Estimate", y = "Density") + 
  theme_ds1()

library(cowplot)

plot_grid(plotInt, plotincumbentD, plotincumbentR, plotprimaryUnopD, plotprimaryUnopR,
          plotcmpgnMaxDisbursDvsR, plotprimaryTotDvsR, plotlogPopDensPerSqMi,
          plotpctEduGradProAge25plus, plotpctInLaborForceUnemp, ncol = 2, nrow = 5)

rm(plotInt, plotincumbentD, plotincumbentR, plotprimaryUnopD, plotprimaryUnopR,
          plotcmpgnMaxDisbursDvsR, plotprimaryTotDvsR, plotlogPopDensPerSqMi,
          plotpctEduGradProAge25plus, plotpctInLaborForceUnemp)
```

Each posterior is considerably more concentrated than the respective posterior, with relatively slim plausibility of "zero" values in each case.

```{r, echo = F, eval = F}
# THIS AND NEXT SECTION NOT USED
# relative sample sizes
parsList <- parnames(m1.12_fullHouse_noME)[grepl("b_", parnames(m1.12_fullHouse_noME))]


parsList <- c(as.character(grep("^b_", 
                                colnames(posterior_samples(m1.12_fullHouse_noME, subset = 1)), 
                                value = TRUE)) )

mcmc_neff(neff_ratio(m1.12_fullHouse_noME, pars = parsList)) +
  theme_ds1() 

mcmc_neff(neff_ratio(m1.12_unempInc, pars = popParams_m1.12_unempInc)) +
  labs(title = "unempInc") +
  theme_ds1() + theme(legend.position = "top")

# model residuals
residuals(MOD)
```

```{r, echo = F, eval = F}
# for posteriors estimated with model weighted averages
# try importing numeric vector for weight argument - get from modWts_mod1
posterior_average(MODS, 
                  weights = "kfold", 
                  seed = 8800) %>%
  as_tibble() %>%
  bind_cols(DAT)

# for posterior predictions with model weighted averages
# try importing numeric vector for weight argument - get from modWts_mod1
pp_average(MODS, 
           weights = "kfold", 
           method = "fitted", # use "predict" for new data predictions
           newdata = DAT,
           seed = 8800) %>%
  as_tibble() %>%
  bind_cols(DAT)
```

### Evaluating Model 1 fits - In-sample performance

Let's assess how well `m1.12_redHouse3` predicts the winner for in-sample data.

```{r, fig.align = "center", fig.width = 8}
# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m1.12_redHouse3 <- posterior_predict(m1.12_redHouse3)
# mean for each district (column of postPred_m1.12_redHouse3)
meanPred_m1.12_redHouse3 <- apply(postPred_m1.12_redHouse3, 2, mean)

# consider vote margin of victory
generalVoteDvsR <- 
  houseElectionDat %>%
  filter(year == 2012) %>%
  mutate(x = generalMaxD - generalMaxR) %>%
  select(x) %>%
  # standardize to use relative magnitude rather than raw counts
  mutate(x = (x - mean(x)) / sd(x))

predVsObs12 <- 
  tibble(district2        = houseElectionDat12_std$district2,
         generalWinD      = houseElectionDat12_std$generalWinD,
         meanPredWinD     = meanPred_m1.12_redHouse3,
         genlVoteDvsR_std = generalVoteDvsR$x)

nBins <- 100
cols <- c("darkred", "red",
          "lightgrey",
          "blue", "darkblue")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(72)
cuts <- cut(predVsObs12$meanPredWinD, nBins)
names(cuts) <- sapply(cuts,function(t)
                           cut.cols[which(as.character(t) == levels(cuts))]) 

predVsObs12 %>%
  ggplot(aes(x = meanPredWinD, fill = cut(meanPredWinD, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m1.12_redHouse3",
       subtitle = "Faceted by Democratic candidate elected (0 / 1)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

rm(postPred_m1.12_redHouse3, generalVoteDvsR)
```

Overall the model seems to perform pretty well on in-sample data, but there are some cases where a district was incorrectly estimated in 75% or more of the Markov Chain samples. Let's check into these cases.

```{r}
# compute proportion of cases correctly assigned to generalWinD = 0 / 1
# using <= 25% & 0 // >= 75% & 1 as cutoff for acceptable accuracy
predVsObs12 <- 
  predVsObs12 %>%
  mutate(hitOrMiss  = if_else((generalWinD == 0 & meanPredWinD <= 0.25) | 
                              (generalWinD == 1 & meanPredWinD >= 0.75), "hit", 
                              "miss"),
         missMargin = abs(generalWinD - meanPredWinD))

# data show as counts
predVsObs12 %>%
  group_by(generalWinD, hitOrMiss) %>%
  summarize(nObs    = n())

# for which districts did the model "miss" most?
predVsObs12 %>%
  filter(hitOrMiss == "miss") %>%
  arrange(desc(missMargin)) %>%
  head(20) %>%
  kable(align = "c") %>%
  kable_styling(full_width = F, bootstrap_options = c("striped"))
```  

The output above shows that the model was "fooled" by about 45 districts. My rule here is the model had a bad "miss" if the mean of a district's iterations indicate 75% or more iterations predicting a Democratic candidate winner when a Republican candidate won, OR 25% or fewer iterations predicting a Democratic candidate winner when a Democratic candidate won.

Looking at things a little more closely, each of the 20 districts with the greatest "miss margin" (discrepancy between mean predicted outcome and the actual outcome) had a Democratic-versus-Republican vote margin within a half standard deviation of the sample mean. I may very well be misreading things, but that seems to be an indication of the model not so much falling flat on its face (metaphorically speaking) as simply missing some underlying signal that would aid prediction accuracy. 

If time allows, I will dig deeper into this, and at the very least see if the model fit to all three years' data fares any better.

**Evaluating model performance**

At the 12/6/18 check-in with Dr. Frey, he suggested one way of evaluating the model's predictive performance as checking the relative proportion of candidates with a given range of mean predicted success/failure and seeing how that compares to reality.

For example, if the model has a given district with a mean proportion of about 0.7-0.8 of all simulations resulting in the Democratic candidate being elected, what proportion of the time is the model correct? How about for cases where the model "predicts" the Republican candidate will be elected in about 70-80% of simulations?

```{r}
sum(predVsObs12$meanPredWinD >= 0.7 & predVsObs12$meanPredWinD <= 0.8 & 
      predVsObs12$generalWinD == 1) / 
sum(predVsObs12$meanPredWinD >= 0.7 & predVsObs12$meanPredWinD <= 0.8)

sum(predVsObs12$meanPredWinD >= 0.50 & predVsObs12$meanPredWinD <= 0.60 & 
      predVsObs12$generalWinD == 1) / 
sum(predVsObs12$meanPredWinD >= 0.50 & predVsObs12$meanPredWinD <= 0.60)

sum(predVsObs12$meanPredWinD >= 0.40 & predVsObs12$meanPredWinD <= 0.50 & 
      predVsObs12$generalWinD == 0) / 
sum(predVsObs12$meanPredWinD >= 0.40 & predVsObs12$meanPredWinD <= 0.50)

sum(predVsObs12$meanPredWinD >= 0.2 & predVsObs12$meanPredWinD <= 0.3 & 
      predVsObs12$generalWinD == 0) / 
sum(predVsObs12$meanPredWinD >= 0.2 & predVsObs12$meanPredWinD <= 0.3)
```

The model seems to "get it right" roughly as often as one would hope, with better performance in cases with a more pronounced partisan lean. 

Where the model predicts a Democratic candidate's being elected in 70-80% of simulations, a Democrat "wins" about 85% of the time. For cases where the model predicts a Democrat will be successful in 50-60% of simulations, they are successful about 50% of the time. For model-predicted "40-50% Democratic likelihood" cases, a Republican candidate is elected in about 40% of cases. And in cases where a Democratic party candidate is elected in 20-30% of simulations, the Republican candidate is elected 75% of the time.

A histogram of the "miss margin" might be useful to see as well - though this is essentially the aggregated version of the prediction plot.

```{r, fig.align = "center", fig.width = 8.5}
predVsObs12 %>%
  ggplot(aes(x = missMargin)) +
  geom_histogram(bins = 100, color = "white", fill = color_set8[6]) +
  theme_ds1()
```

**Digging deeper into model "misses"**

Perhaps it will be useful to put up some quick plots of the model predictors, and potential predictors not currently in the model, to see where things go astray and where the model is working well.

*Note: This section was more or less set aside - nothing really jumped out to me, and I will need to carry out Dr. Frey's suggestion to consider relative proportion of the "bad-miss" cases vs. that of the "non-/slight-miss" cases for these predictors to really identify which predictors may enable better classification of the current "bad misses".*

```{r, fig.align = "center", fig.width = 8.5}
houseElectionDat12_std <- 
  houseElectionDat12_std %>%
  mutate(m1.12_redHouse3_missMargin = predVsObs12$missMargin)

houseElectionDat12_std %>%
  ggplot(aes(x = cmpgnMaxDisbursDvsR, y = m1.12_redHouse3_missMargin)) +
  geom_vline(xintercept = 0, color = "gray") +
  geom_point(aes(color = m1.12_redHouse3_missMargin)) +
  scale_color_viridis_c(direction = -1) +
  theme_ds1()
```

### Evaluating Model 1 fits - Out-of-sample performance (2014 election)

Now the rubber meets the road - let's see how the model fares in predicting the following House election.

```{r, fig.align = "center", fig.width = 8}
# estimate is the mean predicted response (generalWinD)
# for median, use robust = T
pred14From12 <- predict(m1.12_redHouse3, newdata = houseElectionDat14_std)

# evaluate with same general approach as 2012 in-sample posterior pred check

# consider vote margin of victory
generalVoteDvsR <- 
  houseElectionDat %>%
  filter(year == 2014) %>%
  mutate(x = generalMaxD - generalMaxR) %>%
  select(x) %>%
  # standardize to use relative magnitude rather than raw counts
  mutate(x = (x - mean(x)) / sd(x))

predVsObs14From12 <- 
  tibble(district2        = houseElectionDat14_std$district2,
         generalWinD      = houseElectionDat14_std$generalWinD,
         meanPredWinD     = pred14From12[,"Estimate"],
         genlVoteDvsR_std = generalVoteDvsR$x)

predVsObs14From12 %>%
  ggplot(aes(x = meanPredWinD, fill = cut(meanPredWinD, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Prediction check for m1.12_redHouse3 using 2014 data",
       subtitle = "Faceted by Democratic candidate elected (0 / 1)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Count",
       caption = "Note: model fit to 2012 election data") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

# compute proportion of cases correctly assigned to generalWinD = 0 / 1
# using <= 25% & 0 // >= 75% & 1 as cutoff for acceptable accuracy
predVsObs14From12 <- 
  predVsObs14From12 %>%
  mutate(hitOrMiss  = if_else((generalWinD == 0 & meanPredWinD <= 0.25) | 
                              (generalWinD == 1 & meanPredWinD >= 0.75), "hit", 
                              "miss"),
         missMargin = abs(generalWinD - meanPredWinD))

# data show as counts
predVsObs14From12 %>%
  group_by(generalWinD, hitOrMiss) %>%
  summarize(nObs    = n())

# for which districts did the model "miss" most?
predVsObs14From12 %>%
  filter(hitOrMiss == "miss") %>%
  arrange(desc(missMargin)) %>%
  head(20) %>%
  kable(align = "c") %>%
  kable_styling(full_width = F, bootstrap_options = c("striped"))
```
The 2012-fit reduced model seems disproportionately less accurate in predicting Republican candidate wins for 2014 data. As stated previously, I am defining a "miss" as a mean predicted win outcome giving the Republican candidate $\le$ 25% likelihood of success when a Republican is actually elected, or a mean predicted outcome giving the Democratic candidate $\le$ 25% likelihood of success when a Democrat is actually elected.

By such a metric, the model's predictions "miss" in about 11% of all cases where a Republican candidate won, versus about 5% of cases where a Democratic candidate won.

## Model 1 - Bernoulli model, using 2014 data

Now let's see how similar posterior distribution estimates are when fitting the "full" model to 2014 data, and if the 2014-fit "full" model can be similarly pared down as the 2012 model was.

```{r, eval = F}
m1.14_fullHouse_noME <- 
  brm(generalWinD ~ 1 + incumbentD + incumbentR + primaryUnopD + primaryUnopR + 
                        cmpgnMaxDisbursDvsR + primaryTotDvsR + log_popDensPerSqMi 
                      + medianAgeYr + pctEduGradProAge25plus + 
                        pctInLaborForceUnemp + perCapitaIncome + pctWhiteAlone,
      data = houseElectionDat14_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)",         class = "Intercept"),
                set_prior("normal(0,2)",         class = "b") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99, max_treedepth = 12),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.14_fullHouse_noME")
```

```{r, echo = F}
m1.14_fullHouse_noME <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.14_fullHouse_noME.rds")
```

```{r, echo = F, fig.align = "center", fig.width = 8.5}
post_m1.14_fullHouse_noME <- posterior_samples(m1.14_fullHouse_noME) %>%
  select(-lp__)

post_m1.14_fullHouse_noME %>%
mcmc_intervals(prob = 0.5, prob_outer = 0.9) +
  labs(title = "m1.14_fullhouse_noME",
       subtitle = "Posterior coefficient estimates",
       caption = "Line: 90% credible interval \n Bar: 50% CI") + 
  theme_ds1()
```

Interestingly, while the 2012-fit "full" model had `pctWhiteAlone`, `perCapitaIncome`, and `medianAgeYr` as predictors whose posterior distribution gave serious plausibility to zero-effect estimates, for the 2014-fit "full" model it is `pctEduGradProAge25plus`, `primaryUnopR`, and `primaryUnopD` that have posteriors suggesting their removal might not adversely impact the model's in-sample predictive capabilities.

Just for so, let's see how the 2014-fit model performs when predicting on in-sample data with all variables versus when those three variables are removed.

```{r, fig.align = "center", fig.width = 8}
# prediction of 2014 model in-sample, "full" model
# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m1.14_fullHouse <- posterior_predict(m1.14_fullHouse_noME)
# mean for each district (column of postPred_m1.14_redHouse)
meanPred_m1.14_fullHouse <- apply(postPred_m1.14_fullHouse, 2, mean)

# reduced version of "full" 2014-fit model
m1.14_redHouse <- 
  update(m1.14_fullHouse_noME, 
         . ~ . -primaryUnopD -primaryUnopR -pctEduGradProAge25plus)

# prediction of 2014 model in-sample, "reduced" model
# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m1.14_redHouse <- posterior_predict(m1.14_redHouse)
# mean for each district (column of postPred_m1.12_redHouse3)
meanPred_m1.14_redHouse <- apply(postPred_m1.14_redHouse, 2, mean)

# consider vote margin of victory
generalVoteDvsR <- 
  houseElectionDat %>%
  filter(year == 2014) %>%
  mutate(x = generalMaxD - generalMaxR) %>%
  select(x) %>%
  # standardize to use relative magnitude rather than raw counts
  mutate(x = (x - mean(x)) / sd(x))

predVsObs14 <- 
  tibble(district2        = houseElectionDat14_std$district2,
         generalWinD      = houseElectionDat14_std$generalWinD,
         meanPredWinDFull = meanPred_m1.14_fullHouse,
         meanPredWinDRed  = meanPred_m1.14_redHouse,
         genlVoteDvsR_std = generalVoteDvsR$x)

predVsObs14 %>%
  ggplot(aes(x = meanPredWinDFull, fill = cut(meanPredWinDFull, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m1.14_fullHouse_noME",
       subtitle = "Faceted by Democratic candidate elected (0 / 1)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

predVsObs14 %>%
  ggplot(aes(x = meanPredWinDRed, fill = cut(meanPredWinDRed, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m1.14_redHouse",
       subtitle = "Faceted by Democratic candidate elected (0 / 1)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

# comparison of the two models' predictions
predVsObs14 %>%
  ggplot() +
  geom_density(aes(x = meanPredWinDFull), fill = color_set8[1], alpha = 0.4,
               adjust = 0.2) +
  geom_density(aes(x = meanPredWinDRed), fill = color_set8[2], alpha = 0.4,
               adjust = 0.2) +
  labs(title = "Comparing posterior prediction densities, 2014 models",
       subtitle = "Full (dk blue) vs Reduced (lt blue)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Density") +
  theme_ds1()
```  

The two models are impressively similar, and yield largely accurate inferences on election outcomes, as did the 2012-fit model. Fit diagnostics (trace plots, chain density estimates, and R-hat values) were good for both the full and reduced models again.

**Out-of-sample performance - 2016 election**

As with the 2012 model, let's see how well the 2014 (reduced) model fares in predicting 2016 election outcomes.

```{r, fig.align = "center", fig.width = 8}
# estimate is the mean predicted response (generalWinD)
# for median, use robust = T
pred16From14 <- predict(m1.14_redHouse, newdata = houseElectionDat16_std)

# evaluate with same general approach as 2012 in-sample posterior pred check

# consider vote margin of victory
generalVoteDvsR <- 
  houseElectionDat %>%
  filter(year == 2016) %>%
  mutate(x = generalMaxD - generalMaxR) %>%
  select(x) %>%
  # standardize to use relative magnitude rather than raw counts
  mutate(x = (x - mean(x)) / sd(x))

predVsObs16From14 <- 
  tibble(district2        = houseElectionDat16_std$district2,
         generalWinD      = houseElectionDat16_std$generalWinD,
         meanPredWinD     = pred16From14[,"Estimate"],
         genlVoteDvsR_std = generalVoteDvsR$x)

predVsObs16From14 %>%
  ggplot(aes(x = meanPredWinD, fill = cut(meanPredWinD, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Prediction check for m1.14_redHouse using 2016 data",
       subtitle = "Faceted by Democratic candidate elected (0 / 1)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Count",
       caption = "Note: model fit to 2014 election data") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

# compute proportion of cases correctly assigned to generalWinD = 0 / 1
# using <= 25% & 0 // >= 75% & 1 as cutoff for acceptable accuracy
predVsObs16From14 <- 
  predVsObs16From14 %>%
  mutate(hitOrMiss  = if_else((generalWinD == 0 & meanPredWinD <= 0.25) | 
                              (generalWinD == 1 & meanPredWinD >= 0.75), "hit", 
                              "miss"),
         missMargin = abs(generalWinD - meanPredWinD))

# data show as counts
predVsObs16From14 %>%
  group_by(generalWinD, hitOrMiss) %>%
  summarize(nObs    = n())

# for which districts did the model "miss" most?
predVsObs16From14 %>%
  filter(hitOrMiss == "miss") %>%
  arrange(desc(missMargin)) %>%
  head(20) %>%
  kable(align = "c") %>%
  kable_styling(full_width = F, bootstrap_options = c("striped"))
```

The 2014-fit reduced model is disproportionately less accurate in predicting Democratic candidate wins for 2016 data, which is opposite of how the 2012-fit model fared in predicting 2014 outcomes. Now the model "missed" in 14% of cases where a Democratic candidate was elected, versus about 4% of cases where a Republican candidate won.

## Model 1 - Bernoulli model, using 2016 data

Now let's see how similar posterior distribution estimates are when fitting the "full" model to 2016 data, and if the 2016-fit "full" model can be similarly pared down as the 2012 and 2014 models were.

**Note: For greater comparability, posterior coefficient distribution estimates for each year's full model will be compared in the next section.**

```{r, eval = F}
m1.16_fullHouse_noME <- 
  brm(generalWinD ~ 1 + incumbentD + incumbentR + primaryUnopD + primaryUnopR + 
                        cmpgnMaxDisbursDvsR + primaryTotDvsR + log_popDensPerSqMi 
                      + medianAgeYr + pctEduGradProAge25plus + 
                        pctInLaborForceUnemp + perCapitaIncome + pctWhiteAlone,
      data = houseElectionDat16_std, family = bernoulli(),
      prior = c(set_prior("normal(0,3)",         class = "Intercept"),
                set_prior("normal(0,2)",         class = "b") ),
      iter = 7000, warmup = 3000, chains = 3,
      control = list(adapt_delta = 0.99, max_treedepth = 12),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.16_fullHouse_noME")
```

```{r, echo = F}
m1.16_fullHouse_noME <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.16_fullHouse_noME.rds")
```

```{r, echo = F, fig.align = "center", fig.width = 8.5}
post_m1.16_fullHouse_noME <- posterior_samples(m1.16_fullHouse_noME) %>%
  select(-lp__)

post_m1.16_fullHouse_noME %>%
mcmc_intervals(prob = 0.5, prob_outer = 0.9) +
  labs(title = "m1.16_fullhouse_noME",
       subtitle = "Posterior coefficient estimates",
       caption = "Line: 90% credible interval \n Bar: 50% CI") + 
  theme_ds1()
```

The 2016 "full" model once again does not directly accord with prior-year "full" models, though it seems slightly more aligned with its 2012 counterpart compared to 2014.

For the 2016 model, `medianAgeYr` seems to very plausibly not contribute any marginal predictive information when all other predictors are accounted for. `perCapitaIncome` could also potentially be removed at little cost to predictive accuracy. The intercept appears to have a posterior distribution estimate most notably similar to that of `medianAgeYr` - I believe that this implies the predictors in the model are contributing the lion's share of the marginal predictive 

Unlike the 2012-fit "full" model, now `primaryUnopD` appears to provide a healthy amount of marginal predictive utility.

As before, let's see how the 2016-fit model performs when predicting on in-sample data with all variables versus when those three variables are removed.

```{r, fig.align = "center", fig.width = 8}
# prediction of 2016 model in-sample, "full" model
# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m1.16_fullHouse <- posterior_predict(m1.16_fullHouse_noME)
# mean for each district (column of postPred_m1.16_redHouse)
meanPred_m1.16_fullHouse <- apply(postPred_m1.16_fullHouse, 2, mean)

# reduced version of "full" 2014-fit model
m1.16_redHouse <- 
  update(m1.16_fullHouse_noME, 
         . ~ . -medianAgeYr -perCapitaIncome)

# prediction of 2016 model in-sample, "reduced" model
# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m1.16_redHouse <- posterior_predict(m1.16_redHouse)
# mean for each district (column of postPred_m1.16_redHouse)
meanPred_m1.16_redHouse <- apply(postPred_m1.16_redHouse, 2, mean)

# consider vote margin of victory
generalVoteDvsR <- 
  houseElectionDat %>%
  filter(year == 2016) %>%
  mutate(x = generalMaxD - generalMaxR) %>%
  select(x) %>%
  # standardize to use relative magnitude rather than raw counts
  mutate(x = (x - mean(x)) / sd(x))

predVsObs16 <- 
  tibble(district2        = houseElectionDat16_std$district2,
         generalWinD      = houseElectionDat16_std$generalWinD,
         meanPredWinDFull = meanPred_m1.16_fullHouse,
         meanPredWinDRed  = meanPred_m1.16_redHouse,
         genlVoteDvsR_std = generalVoteDvsR$x)

predVsObs16 %>%
  ggplot(aes(x = meanPredWinDFull, fill = cut(meanPredWinDFull, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m1.16_fullHouse_noME",
       subtitle = "Faceted by Democratic candidate elected (0 / 1)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

predVsObs16 %>%
  ggplot(aes(x = meanPredWinDRed, fill = cut(meanPredWinDRed, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m1.16_redHouse",
       subtitle = "Faceted by Democratic candidate elected (0 / 1)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

# comparison of the two models' predictions
predVsObs16 %>%
  ggplot() +
  geom_density(aes(x = meanPredWinDFull), fill = color_set8[5], alpha = 0.4,
               adjust = 0.2) +
  geom_density(aes(x = meanPredWinDRed), fill = color_set8[6], alpha = 0.4,
               adjust = 0.2) +
  labs(title = "Comparing posterior prediction densities, 2016 models",
       subtitle = "Full (tan) vs Reduced (yellow)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Density") +
  theme_ds1()
```  

As with the 2014 models, the 2016 "full" and reduced models are very strongly similar, as would easily be expected.


# Appendix 2 - Bernoulli outcome models - good use of partial pooling, or island of misfit toy models?

**Note: After going through the full analysis of this section, I've concluded that the analysis is pretty much bunk, but I'll keep in Appendix 2 for reference.**

The following models are fit treating the outcome variable (0 / 1 for `generalWinD`) as a Bernoulli random variable - in other words, each district-level election has a single trial with some unknown probability of "success" (here, the election of a Democratic party candidate), to be estimated from the Bayesian regression.

After going back and forth on this (in consultation with chapter 12 of *Statistical Rethinking*, which discusses multilevel models), I'm proceeding with the Bernoulli-outcome models as the "good" models for deeper analysis and consideration in this project, rather than the next section's Binomial-outcome models (which use the same data, just aggregated at the district level for the three elections).

I believe this section's models are the better application of multilevel modeling for one distinct reason. Before getting to that, I believe that using `district2` (the variable identifying unique districts, e.g. "PA-1" being Pennsylvania's 1st district) as the "cluster-identifier" is correct in either the Bernoulli or the Binomial case, and either case's varying-effects model will be able to partially pool varying intercept / varying slopes inferences among districts identified as having similar underlying behavior (association between predictor and outcome values).

What makes the Bernoulli case, explored in this section, a potentially *better* application of partial pooling is that, because each district-level election is its own observation (rather than 1/3 of an aggregated district-level observation as with the Binomial case in the next section), the model is being given three "opportunities" to apply Bayesian updating in estimating posteriors for each individual district, rather than a single one.

There is of the concern that the varying-intercepts model is being overfit in this case, but I will go forward with the analysis in this section nonetheless.

## Fitting fixed-effects and varying-effects Bernoulli models (data from all three elections)

Having considered each individual election's data and models, let's fit a pair of models with one having fixed and the other having varying intercepts and slopes.

The *fixed-effects* model attempts to estimate a common intercept and common slope coefficients using observations across all cases - this is a "complete pooling" model where individual district-level elections are treated as all sharing a common underlying distribution for each parameter.

The *verying-effects* model attempts to estimate parameter values in a setting where there is some population-wide underlying set of values (as in the fixed-effects case), but where individual districts vary by differing degrees from these population-wide values, and estimation from each individual observation contributes to adjusted estimates for all other observations. This is the "partial pooling" model.

After evaluating the model fits and in-sample predictive performance, we can move on to sensitivity analysis (how do changes on the priors affect posterior distribution estimates?) and counterfactual analysis (how do changes in a given predictor's values relate to changes in outcome prediction?).

### Fitting the models

```{r, eval = F}
m1.allFull_fIfS <- 
  brm(generalWinD ~ 1 + incumbentD + incumbentR + primaryUnopD + primaryUnopR + 
                        cmpgnMaxDisbursDvsR + primaryTotDvsR + log_popDensPerSqMi 
                      + medianAgeYr + pctEduGradProAge25plus + 
                        pctInLaborForceUnemp + perCapitaIncome + pctWhiteAlone, 
      data = houseElectionDat_std, family = bernoulli(), 
      prior = c(set_prior("normal(0, 3)", class = "Intercept"),
                set_prior("normal(0, 2)", class = "b")), 
      iter = 7000, warmup = 3000, chains = 3, 
      control = list(adapt_delta = 0.99),
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.allFull_fIfS")

m1.allFull_vIvS <- 
  brm(generalWinD ~ 1 + incumbentD + incumbentR + primaryUnopD + primaryUnopR + 
                        cmpgnMaxDisbursDvsR + primaryTotDvsR + log_popDensPerSqMi 
                      + medianAgeYr + pctEduGradProAge25plus + 
                        pctInLaborForceUnemp + perCapitaIncome + pctWhiteAlone + 
                   (1 + incumbentD + incumbentR + primaryUnopD + primaryUnopR + 
                        cmpgnMaxDisbursDvsR + primaryTotDvsR + log_popDensPerSqMi 
                      + medianAgeYr + pctEduGradProAge25plus + 
                        pctInLaborForceUnemp + perCapitaIncome + pctWhiteAlone |
                      district2), 
      data = houseElectionDat_std, family = bernoulli(), 
      prior = c(set_prior("normal(0, 3)", class = "Intercept"),
                set_prior("normal(0, 2)", class = "b"), 
                set_prior("cauchy(0, 2)", class = "sd"),
                set_prior("lkj(2)", class = "cor")), 
      iter = 7000, warmup = 3000, chains = 3, 
      control = list(adapt_delta = 0.99),
      thin = 2,
      file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.allFull_vIvS")
```

```{r, echo = F}
m1.allFull_fIfS <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.allFull_fIfS.rds")

m1.allFull_vIvS <- readRDS(file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.allFull_vIvS.rds")
```

### Relative weights for fixed- vs. varying-effects models

In order to check how the two "full" models are estimate to perform in out-of-sample inference, let's compare model weights using WAIC.

```{r, eval = F}
(m1.allFull_modWts <- model_weights(m1.allFull_fIfS, m1.allFull_vIvS, 
                                   weights = "waic") )
```

```{r, echo = F, eval = F}
saveRDS(m1.allFull_modWts,
        file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.allFull_modWts.rds")
```

```{r, echo = F}
readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.allFull_modWts.rds")
```

By a very wide margin, the varying-effects model is deemed the superior model for minimizing pointwise out-of-sample deviance - in other words, the varying-effects model is estimated to be less "wrong" for a given prediction on out-of-sample data. While I'll be pursuing the most in-depth analysis on the varying-effects model, let's continue to consider both models for now.

### In-depth fit diagnostics

Just to confirm the models have fit well, I checked all density plots and trace plots for the various population-level and group-level effects - the fits are good (good stationarity around a concentrated set of estimated values, and good mixing of MCMC chains across the range of values).

Here are the model summaries (note that the R-hat values are all roughly 1.00, indicating good convergence):

```{r}
summary(m1.allFull_fIfS)
```

Fixed-effect model effective sample sizes are very large (the maximum possible is 12,000), and as noted the R-hat convergence diagnostic is good, suggesting the posterior estimates are fairly stable.

As might be expected, the mean estimates and 95% credible intervals for parameter posterior distributions vary, with many predictor posteriors overlapping both negative and positive values (and therefore also containing 0) - suggesting the association with the outcome variable `generalWinD` is uncertain and very plausibly small to nonexistent. Other parameters, such as `incumbentD` and `incumbentR`, are very plausibly nonzero and indicate relatively sizable association with a given probability of a Democratic candidate being elected.

```{r}
summary(m1.allFull_vIvS)
```

While the varying-effects model also has good convergence diagnostics, there are at least two major differences compared to the fixed-effects model:

1) There are many more parameters being estimated - this is because the "partial pooling" model is considering the divergence of each district-level value from the corresponding population-wide value for each parameter's posterior.

2) Effective sample sizes are much smaller - this this is because I set the argument `thin = 2` when fitting the model - this divides the number of post-warmup MCMC iterations to half of the "full" number, and setting "thin" > 1 reduces the computation time and file size of the model object. I did this because the complexity of the model (lots of observations and parameter estimations) resulted in a very large file (446MB, versus 891MB for the "default / thin = 1" equivalent).

In other words, the "true" model actually has approximately twice the effective sample size for each parameter, but our posterior estimates shown above are accurate to the "full" version of the model - I generated the "full" version of the model to confirm this, and just saved the "thinned" version. Estimates for the "full" version of the model had mean estimates and 95% credible interval bounds which differed from the "thinned" equivalent by less than 0.1 in all cases and for each parameter - I suspect this is simply due to my not having set a "seed" value for the MCMC random number generator.

### Visualizing parameter posterior estimates

**Comparing population-level effect estimates**

First, let's compare the population-level parameter posterior estimates between the fixed- and varying-effects models.

```{r, echo = F, fig.align = "center", fig.width = 8}
post_b_1.f <- 
  posterior_summary(m1.allFull_fIfS, pars = "b_", probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(modFX = "fixed",
         par   = rownames(posterior_summary(m1.allFull_fIfS, pars = "b_")))

post_b_1.v <- 
  posterior_summary(m1.allFull_vIvS, pars = "b_", probs = c(0.05, 0.95)) %>%
  as_tibble() %>%
  mutate(modFX = "varying",
         par   = rownames(posterior_summary(m1.allFull_vIvS, pars = "b_")))

# using this next part to set plot par order
# based on varying-effects model mean estimates
parOrder <- 
  post_b_1.v %>%
  mutate(par = str_remove(par, "b_") ) %>%
  arrange(Estimate) %>%
  select(par)

post_b_1 <- 
  post_b_1.f %>%
  bind_rows(post_b_1.v) %>%
  # tidy parameter labels by removing common "b_"
  mutate(par = str_remove(par, "b_"),
         par = factor(par,
                      levels = parOrder$par) )

post_b_1 %>%
  ggplot(aes(color = modFX, group = modFX)) +
  geom_hline(yintercept = 0, color = "gray") +
  geom_pointrange(aes(x = par, y = Estimate, ymin = Q5, ymax = Q95),
                  position = position_dodge(width = 0.5)) +
  labs(title = "Comparison of m1.allFull models' population-level effects",
       subtitle = "Fixed vs. Varying effects",
       color = NULL,
       x = "Population-level parameter",
       y = "Posterior estimate",
       caption = "Mean point estimate and 90% credible intervals") +
  coord_flip() +
  scale_color_manual(values = color_set8[c(2, 4)]) +
  theme_ds1() + theme(legend.position = "top")
```

The varying-effects model has wider 90% credible intervals (indicating greater uncertainty in the posterior estimates) for each population-level parameter, and it does not have point estimates shrinking towards zero as I had expected. Among parameters with fixed-effects estimates farther from zero, the corresponding varying-effects estimates are even farther from zero - though the two 90% credible intervals largely overlap. 

The two notable exceptions to this last note concerns `cmpgnMaxDisbursDvsR` (difference in maximum campaign-reported spending) and `primaryTotDvsR` (difference in total number of primary votes) - here, the varying-effects model is gives much more plausibility to a more positive effect.

The following plot shows the posterior slope estimates for the varying-effects model, which will be used to consider trimming the model.

```{r, echo = F, fig.align = "center", fig.width = 8.5}
post_m1.allFull_vIvS_b <- posterior_samples(m1.allFull_vIvS) %>%
  select(parnames(m1.allFull_vIvS)[grepl("b_", parnames(m1.allFull_vIvS))])

post_m1.allFull_vIvS_b %>%
  mcmc_intervals(prob = 0.5, prob_outer = 0.9) +
  labs(title = "m1.allFull_vIvS_b",
       subtitle = "Posterior coefficient estimates for Intercept and Slopes",
       caption = "Line: 90% credible interval \n Bar: 50% CI") + 
  theme_ds1()
```

The plot above suggests there are several parameters with 50% credible intervals overlapping zero (thus highlighting the plausibility such a parameter has a coefficient of zero, and therefore no marginal predictive value when the other predictors are accounted for). The most immediately plausible of these is the intercept, which I consider an essential component of the regression model. `pctEduGradProAge25plus` is the next likeliest candidate.

For the sake of convenience, I'll evaluate the predictive merit of removing `pctEduGradProAge25plus` by comparing the WAIC weight of the "full" model versus that of the "reduced" model.

```{r, eval = F}
m1.allRed_vIvS <- 
  update(m1.allFull_vIvS, . ~ . -pctEduGradProAge25plus)

m1.all_modWts <- model_weights(m1.allFull_vIvS, m1.allRed_vIvS, weights = "waic")
```

```{r, echo = F, eval = F}
saveRDS(m1.allRed_vIvS,
       file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.allRed_vIvS.rds")

saveRDS(m1.all_modWts,
       file = "C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.all_modWts.rds")
```

```{r, echo = F}
m1.allRed_vIvS <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.allRed_vIvS.rds")

m1.all_modWts <- 
  readRDS("C:/Users/Duane/Documents/Academic/Villanova/4. Fall 18/ProjectData/ModFits/m1.all_modWts.rds")
```

```{r}
m1.all_modWts
```

Since the "full" and "reduced" models are allocated almost equal shares of the relative model weight, and the reduced model has only one fewer variable than the "full" model, I'll just stick with the "full" model for now.

**Reviewing group-level effects**

```{r, echo = F, fig.align = "center", fig.width = 8.5}
post_m1.allFull_vIvS_sd <- posterior_samples(m1.allFull_vIvS) %>%
  select(parnames(m1.allFull_vIvS)[grepl("sd_", parnames(m1.allFull_vIvS))])

post_m1.allFull_vIvS_sd %>%
  mcmc_intervals(prob = 0.5, prob_outer = 0.9) +
  labs(title = "m1.allFull_vIvS_sd",
       subtitle = "Posterior estimates for slope SDs",
       caption = "Line: 90% credible interval \n Bar: 50% CI") + 
  theme_ds1()
```

I'm less sure of how to interpret parameters associated with varying intercepts/slopes, but I believe the above indicates that there is relatively little variability in slope coefficient values at the per-district level - that is, the estimated slope coefficient for a given district's `b_Intercept` or `b_perCapitaIncome` value, say, is not likely to change very much from one year (in the 2012/'14/'16 data set) to another. It appears that this is not necessarily the case for the predictors `incumbentD` and `cmpgnMaxDisbursDvsR`, however.

The next plot's a bit of an eyesore, but it seems useful to consider estimated correlations between the slopes of the various predictors.

```{r, echo = F, fig.align = "center", fig.width = 8.5, fig.height = 8}
post_m1.allFull_vIvS_cor <- posterior_samples(m1.allFull_vIvS) %>%
  select(parnames(m1.allFull_vIvS)[grepl("cor_", parnames(m1.allFull_vIvS))])

colnames(post_m1.allFull_vIvS_cor) <- str_remove(colnames(post_m1.allFull_vIvS_cor),
                                                 "cor_district2__")

post_m1.allFull_vIvS_cor %>%
  mcmc_intervals(prob = 0.5, prob_outer = 0.9) +
  labs(title = "m1.allFull_vIvS_cor",
       subtitle = "Posterior estimates for district-level parameter cor's",
       caption = "Line: 90% credible interval \n Bar: 50% CI") + 
  theme_ds1() +
  theme(axis.text.y = element_text(size = 9))
```

It seems strongly plausible that the correlation between slopes for essentially every predictor variable pairing (including with the intercept) at the district level is 0. The estimated correlation between `incumbentD` and `incumbentR` has a point (mean) estimate markedly more different from zero than the others - though this particular interaction isn't very meaningful since only a very small number of 2012 districts had both D and R incumbents due to post-2010 redistricting. I will consider (eventually) including interactions as additional predictors in the model.

These very-plausibly-zero correlation estimates may be a result of the `brms` package's default to using non-centered parametrization, but I'm not sure.

### Evaluating all-years Model 1 fit

Let's assess how well `m1.allFull_vIvS` predicts the winner for in-sample data.

```{r, fig.align = "center", fig.width = 8}
# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m1.allFull_vIvS <- posterior_predict(m1.allFull_vIvS)
# mean for each per-year district (column of postPred_m1.allFull_vIvS)
meanPred_m1.allFull_vIvS <- apply(postPred_m1.allFull_vIvS, 2, mean)

predVsObs.1v <- 
  tibble(district2        = houseElectionDat_std$district2,
         year             = houseElectionDat_std$year,
         generalWinD      = houseElectionDat_std$generalWinD,
         meanPredWinD     = meanPred_m1.allFull_vIvS )

nBins <- 100
cols <- c("darkred", "red",
          "lightgrey",
          "blue", "darkblue")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(74)
cuts <- cut(predVsObs.1v$meanPredWinD, nBins)
names(cuts) <- sapply(cuts,function(t)
                           cut.cols[which(as.character(t) == levels(cuts))]) 

predVsObs.1v %>%
  ggplot(aes(x = meanPredWinD, fill = cut(meanPredWinD, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m1.allFull_vIvS",
       subtitle = "Faceted by Democratic candidate elected (0 / 1)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()
```

The plot above, to my relatively inexperienced model-fitting eyes, seems excellent. The model doesn't appear to have nearly as many "bad" misses where the mean of model-predicted outcomes infer the incorrect party's success 75% or more of the time (relative to the single-year models - see Appendix 1). At the same time, the model seems to assign a fairly high mean predicted-outcome likelihood in a vast majority of cases - something on the order of 85%+ predicted-outcome mean proportions correctly inferring either the Republican or Democratic candidate's winning.

For the sake of analysis, let's create a quick "hit or miss" table similar to what was constructed for the individual year-fit models, but now defining a "miss" as a mean proportion of the election outcome assigning 40% or more of simulations to the incorrect party's success.

```{r}
# compute proportion of cases correctly assigned to generalWinD = 0 / 1
# using <= 40% & 0 // >= 60% & 1 as cutoff for acceptable accuracy
predVsObs.1v <- 
  predVsObs.1v %>%
  mutate(hitOrMiss  = if_else((generalWinD == 0 & meanPredWinD < 0.40) | 
                              (generalWinD == 1 & meanPredWinD > 0.60), "hit", 
                              "miss"),
         missMargin = abs(generalWinD - meanPredWinD))

# data shown as counts
predVsObs.1v %>%
  group_by(generalWinD, hitOrMiss, year) %>%
  summarize(nObs    = n())

# for which districts did the model "miss" most?
predVsObs.1v %>%
  filter(hitOrMiss == "miss") %>%
  arrange(desc(missMargin)) %>%
  head(20) %>%
  kable(align = "c") %>%
  kable_styling(full_width = F, bootstrap_options = c("striped"))
```

Impressive. Across 1305 district-level elections (435 Congressional districts x 3 years of elections data), the model-predicted outcome ascribes a greater than 40% likelihood of the incorrect party's success in 14 cases, with nearly half coming in 2012, the first year of 2010 Census-based redistricting. Interestingly, 11 of the 14 "misses" are for districts where Democratic candidates ultimately won - perhaps suggesting the model is in some way incorporating the fact that Republican candidates succeeded in a majority of districts each of the three years.

### Considering whether the varying-effects model is overfit

I'm inclined to think the varying-intercepts model is appropriate for this analysis, but there is a definite possibility it is overfit to the data.

Let's compare the **full** fixed-and varying-effects models once more, this time considering the predictions of the fixed-effects model and then the model density.

```{r, fig.align = "center", fig.width = 8}
# prediction of each district (columns) for each post-warmup iteration (rows)
postPred_m1.allFull_fIfS <- posterior_predict(m1.allFull_fIfS)
# mean for each per-year district (column of postPred_m1.allFull_fIfS)
meanPred_m1.allFull_fIfS <- apply(postPred_m1.allFull_fIfS, 2, mean)

predVsObs.1f <- 
  tibble(district2        = houseElectionDat_std$district2,
         year             = houseElectionDat_std$year,
         generalWinD      = houseElectionDat_std$generalWinD,
         meanPredWinD     = meanPred_m1.allFull_vIvS )

nBins <- 100
cols <- c("darkred", "red",
          "lightgrey",
          "blue", "darkblue")
colGradient <- colorRampPalette(cols)
cut.cols <- colGradient(74)
cuts <- cut(predVsObs.1f$meanPredWinD, nBins)
names(cuts) <- sapply(cuts,function(t)
                           cut.cols[which(as.character(t) == levels(cuts))]) 

predVsObs.1f %>%
  ggplot(aes(x = meanPredWinD, fill = cut(meanPredWinD, nBins))) +
  geom_histogram(bins = nBins) +
  facet_wrap(. ~ generalWinD) +
  labs(title = "Posterior prediction check for m1.allFull_fIfS",
       subtitle = "Faceted by Democratic candidate elected (0 / 1)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Count") +
  scale_fill_manual(values = cut.cols,
                    guide  = F) +
  theme_ds1()

# comparison of fixed- and varying-effects model predictions
ggplot() +
  geom_density(data = predVsObs.1f,
               aes(x = meanPredWinD), fill = color_set8[1], alpha = 0.4,
               adjust = 0.2) +
  geom_density(data = predVsObs.1v,
               aes(x = meanPredWinD), fill = color_set8[2], alpha = 0.4,
               adjust = 0.2) +
  labs(title = "Comparing posterior prediction densities, 2016 models",
       subtitle = "Fixed (dk blue) vs Varying effects (lt blue)",
       x = "Mean proportion of model predictions, D candidate elected",
       y = "Density") +
  theme_ds1()
```

Since the density plot comparison shows an exact overlap between the two models, it seems as if the varying-effects model doesn't contribute any additional inferential utility over the fixed-effects model. 

I've gone through chapter 12 of *Statistical Rethinking* (which introduces and discusses partial pooling/multilevel models), and as best I can tell this is an appropriate application of the mechanism - so perhaps either model is actually just doing a good job of identifying that there are "Democratic"- and "Republican-friendly" districts.

Based on the definition of what constitutes a "miss" (described above), let's compared how the two models fare.

```{r}
# compute proportion of cases correctly assigned to generalWinD = 0 / 1
# using <= 40% & 0 // >= 60% & 1 as cutoff for acceptable accuracy
# now for the fixed-effects model
predVsObs.1f <- 
  predVsObs.1f %>%
  mutate(hitOrMiss.f  = if_else((generalWinD == 0 & meanPredWinD < 0.40) | 
                              (generalWinD == 1 & meanPredWinD > 0.60), "hit", 
                              "miss"),
         missMargin.f = abs(generalWinD - meanPredWinD))

# adjust variable names for the varying-effects equivalent
predVsObs.1v <- 
  predVsObs.1v %>%
  rename(hitOrMiss.v  = hitOrMiss,
         missMargin.v = missMargin)

# compare miss data shown as counts
predVsObs.1v %>%
  bind_cols(predVsObs.1f) %>%
  group_by(generalWinD, year) %>%
  summarize(hit.f  = sum(hitOrMiss.f == "hit"),
            miss.f = sum(hitOrMiss.f == "miss"),
            hit.v  = sum(hitOrMiss.v == "hit"),
            miss.v = sum(hitOrMiss.v == "miss") ) %>%
  kable(align = "c") %>%
  kable_styling(full_width = F, bootstrap_options = c("striped"))
```

The two models appear to provide the same level of in-sample predictive capability, and I can't come up with any better explanation than the fact that the two models are essentially different parametrizations of the same underlying model - in other words, there is no materially significant difference that makes the varying-effects model superior.

*As a result, I'm relegating this section to Appendix 2 of this report and moving forward with the Binomial-outcome model section.*

